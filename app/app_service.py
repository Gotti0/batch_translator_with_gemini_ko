# app_service.py
from pathlib import Path
# typing ëª¨ë“ˆì—ì„œ Tupleì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from typing import Dict, Any, Optional, List, Callable, Union, Tuple
import os
import json
import csv
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed, wait # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬
import time
from tqdm import tqdm # tqdm ì„í¬íŠ¸ í™•ì¸
import sys # sys ì„í¬íŠ¸ í™•ì¸ (tqdm_file_stream=sys.stdout ì— ì‚¬ìš©ë  ìˆ˜ ìˆìŒ)

try:
    from infrastructure.logger_config import setup_logger
except ImportError:
    from infrastructure.logger_config import setup_logger

try:
    # file_handlerì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ë“¤ì„ import í•©ë‹ˆë‹¤.
    from infrastructure.file_handler import (
        read_text_file, write_text_file,
        save_chunk_with_index_to_file, get_metadata_file_path, delete_file,
        load_chunks_from_file,
        create_new_metadata, save_metadata, load_metadata,
        update_metadata_for_chunk_completion, update_metadata_for_chunk_failure, # ì¶”ê°€
        _hash_config_for_metadata,
        save_merged_chunks_to_file
    )
    from ..core.config.config_manager import ConfigManager
    from infrastructure.gemini_client import GeminiClient, GeminiAllApiKeysExhaustedException, GeminiInvalidRequestException
    from domain.translation_service import TranslationService
    from domain.glossary_service import SimpleGlossaryService
    from ..utils.chunk_service import ChunkService
    from ..core.exceptions import BtgServiceException, BtgConfigException, BtgFileHandlerException, BtgApiClientException, BtgTranslationException, BtgBusinessLogicException
    from ..core.dtos import TranslationJobProgressDTO, GlossaryExtractionProgressDTO
    from ..utils.post_processing_service import PostProcessingService
except ImportError:
    # Fallback imports
    from infrastructure.file_handler import (
        read_text_file, write_text_file,
        save_chunk_with_index_to_file, get_metadata_file_path, delete_file,
        load_chunks_from_file,
        create_new_metadata, save_metadata, load_metadata,
        update_metadata_for_chunk_completion, update_metadata_for_chunk_failure, # ì¶”ê°€
        _hash_config_for_metadata,
        save_merged_chunks_to_file
    )
    from core.config.config_manager import ConfigManager
    from infrastructure.gemini_client import GeminiClient, GeminiAllApiKeysExhaustedException, GeminiInvalidRequestException
    from domain.translation_service import TranslationService
    from domain.glossary_service import SimpleGlossaryService
    from utils.chunk_service import ChunkService
    from core.exceptions import BtgServiceException, BtgConfigException, BtgFileHandlerException, BtgApiClientException, BtgTranslationException, BtgBusinessLogicException
    from core.dtos import TranslationJobProgressDTO, GlossaryExtractionProgressDTO
    from utils.post_processing_service import PostProcessingService

logger = setup_logger(__name__)

class AppService:
    """
    ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì£¼ìš” ìœ ìŠ¤ì¼€ì´ìŠ¤ë¥¼ ì¡°ì •í•˜ëŠ” ì„œë¹„ìŠ¤ ê³„ì¸µì…ë‹ˆë‹¤.
    í”„ë ˆì  í…Œì´ì…˜ ê³„ì¸µê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§/ì¸í”„ë¼ ê³„ì¸µ ê°„ì˜ ì¸í„°í˜ì´ìŠ¤ ì—­í• ì„ í•©ë‹ˆë‹¤.
    """

    def __init__(self, config_file_path: Optional[Union[str, Path]] = None):
        self.config_manager = ConfigManager(config_file_path)
        self.config: Dict[str, Any] = {}
        self.gemini_client: Optional[GeminiClient] = None
        self.translation_service: Optional[TranslationService] = None
        self.glossary_service: Optional[SimpleGlossaryService] = None # Renamed from pronoun_service
        self.chunk_service = ChunkService()

        self.is_translation_running = False
        self.stop_requested = False
        self._translation_lock = threading.Lock()
        self._progress_lock = threading.Lock()
        self.processed_chunks_count = 0
        self.successful_chunks_count = 0
        self.failed_chunks_count = 0
        self.post_processing_service = PostProcessingService()

        self.load_app_config()

    def load_app_config(self, runtime_overrides: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        logger.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ë¡œë“œ ì¤‘...")
        if runtime_overrides:
            logger.debug(f"ë¡œë“œ ì‹œ ì ìš©í•  ëŸ°íƒ€ì„ ì˜¤ë²„ë¼ì´ë“œ ê°’: {list(runtime_overrides.keys())}")

        try:
            # 1. íŒŒì¼ ë° ê¸°ë³¸ê°’ìœ¼ë¡œë¶€í„° ê¸°ë³¸ ì„¤ì • ë¡œë“œ
            config_from_manager = self.config_manager.load_config()
            self.config = config_from_manager # íŒŒì¼/ê¸°ë³¸ê°’ìœ¼ë¡œ ì‹œì‘

            # 2. ì œê³µëœ runtime_overridesê°€ ìˆë‹¤ë©´, self.configì— ë®ì–´ì“°ê¸°
            if runtime_overrides:
                self.config.update(runtime_overrides)
                logger.info(f"ëŸ°íƒ€ì„ ì˜¤ë²„ë¼ì´ë“œ ê°’ë“¤ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ìµœì¢… ì„¤ì •ì— ë°˜ì˜ë¨: {list(runtime_overrides.keys())}")
            logger.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ë¡œë“œ ì™„ë£Œ.")

            auth_credentials_for_gemini_client: Optional[Union[str, List[str], Dict[str, Any]]] = None
            use_vertex = self.config.get("use_vertex_ai", False)
            gcp_project_from_config = self.config.get("gcp_project")
            gcp_location = self.config.get("gcp_location")
            sa_file_path_str = self.config.get("service_account_file_path")

            logger.debug(f"[AppService.load_app_config] Vertex AI ì‚¬ìš© ì—¬ë¶€ (use_vertex): {use_vertex}")
            logger.debug(f"[AppService.load_app_config] ì„¤ì • íŒŒì¼ ë‚´ GCP í”„ë¡œì íŠ¸ (gcp_project_from_config): '{gcp_project_from_config}'")
            logger.debug(f"[AppService.load_app_config] ì„¤ì • íŒŒì¼ ë‚´ GCP ìœ„ì¹˜ (gcp_location): '{gcp_location}'")
            logger.debug(f"[AppService.load_app_config] ì„œë¹„ìŠ¤ ê³„ì • íŒŒì¼ ê²½ë¡œ (sa_file_path_str): '{sa_file_path_str}'")

            if use_vertex:
                logger.info("Vertex AI ì‚¬ìš© ëª¨ë“œë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
                # Vertex AI ëª¨ë“œì—ì„œëŠ” auth_credentials_for_gemini_clientê°€ SA JSON ë¬¸ìì—´, SA Dict, ë˜ëŠ” None (ADCìš©)ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                if sa_file_path_str:
                    sa_file_path = Path(sa_file_path_str)
                    if sa_file_path.is_file():
                        try:
                            auth_credentials_for_gemini_client = read_text_file(sa_file_path) # file_handler is now in infrastructure.file_system
                            logger.info(f"Vertex AI ì„œë¹„ìŠ¤ ê³„ì • íŒŒì¼ ('{sa_file_path}')ì—ì„œ ì¸ì¦ ì •ë³´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                        except Exception as e:
                            logger.error(f"Vertex AI ì„œë¹„ìŠ¤ ê³„ì • íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({sa_file_path}): {e}")
                            auth_credentials_for_gemini_client = None
                    else:
                        logger.warning(f"Vertex AI ì„œë¹„ìŠ¤ ê³„ì • íŒŒì¼ ê²½ë¡œê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤: {sa_file_path_str}")
                        # sa_file_path_strì´ ì œê³µë˜ì—ˆì§€ë§Œ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°, auth_credentialsë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
                        auth_conf_val = self.config.get("auth_credentials")
                        if isinstance(auth_conf_val, (str, dict)) and auth_conf_val: # SA JSON ë¬¸ìì—´ ë˜ëŠ” SA dict
                            auth_credentials_for_gemini_client = auth_conf_val
                            logger.info("ì„œë¹„ìŠ¤ ê³„ì • íŒŒì¼ ê²½ë¡œê°€ ìœ íš¨í•˜ì§€ ì•Šì•„ 'auth_credentials' ê°’ì„ ì§ì ‘ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        else:
                            auth_credentials_for_gemini_client = None # ADCë¥¼ ê¸°ëŒ€í•˜ê±°ë‚˜, ì˜¤ë¥˜ë¡œ ê°„ì£¼
                            logger.info("ì„œë¹„ìŠ¤ ê³„ì • íŒŒì¼ ê²½ë¡œê°€ ìœ íš¨í•˜ì§€ ì•Šê³  'auth_credentials'ë„ ì—†ì–´ ADCë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤.")
                elif self.config.get("auth_credentials"):
                    auth_conf_val = self.config.get("auth_credentials")
                    if isinstance(auth_conf_val, (str, dict)) and auth_conf_val: # SA JSON ë¬¸ìì—´ ë˜ëŠ” SA dict
                        auth_credentials_for_gemini_client = auth_conf_val
                        logger.info("Vertex AI: ì„œë¹„ìŠ¤ ê³„ì • íŒŒì¼ ê²½ë¡œê°€ ì—†ì–´, 'auth_credentials' ê°’ì„ ì§ì ‘ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    else:
                        auth_credentials_for_gemini_client = None # ADCë¥¼ ê¸°ëŒ€
                        logger.info("Vertex AI: 'auth_credentials'ê°€ ìœ íš¨í•˜ì§€ ì•Šì•„ ADCë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤.")
                else: # sa_file_path_strë„ ì—†ê³ , auth_credentialsë„ ì—†ëŠ” ê²½ìš° ADC ê¸°ëŒ€
                    auth_credentials_for_gemini_client = None
                    logger.info("Vertex AI: ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•„ ADC(Application Default Credentials)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            else:
                logger.info("Gemini Developer API ì‚¬ìš© ëª¨ë“œì…ë‹ˆë‹¤.")
                auth_credentials_for_gemini_client = None # ê¸°ë³¸ê°’ Noneìœ¼ë¡œ ì‹œì‘

                api_keys_list_val = self.config.get("api_keys", [])
                if isinstance(api_keys_list_val, list):
                    valid_api_keys = [key for key in api_keys_list_val if isinstance(key, str) and key.strip()]
                    if valid_api_keys:
                        auth_credentials_for_gemini_client = valid_api_keys
                        logger.info(f"{len(valid_api_keys)}ê°œì˜ API í‚¤ ëª©ë¡ ('api_keys')ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                
                if auth_credentials_for_gemini_client is None:
                    api_key_val = self.config.get("api_key")
                    if isinstance(api_key_val, str) and api_key_val.strip():
                        auth_credentials_for_gemini_client = api_key_val # GeminiClientëŠ” strì„ ë‹¨ì¼ API í‚¤ë¡œ ì²˜ë¦¬
                        logger.info("ë‹¨ì¼ API í‚¤ ('api_key')ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

                if auth_credentials_for_gemini_client is None:
                    auth_credentials_conf_val = self.config.get("auth_credentials")
                    if isinstance(auth_credentials_conf_val, str) and auth_credentials_conf_val.strip():
                        auth_credentials_for_gemini_client = auth_credentials_conf_val # ë‹¨ì¼ API í‚¤ ë˜ëŠ” SA JSON ë¬¸ìì—´
                        logger.info("auth_credentials ê°’ì„ ë‹¨ì¼ ì¸ì¦ ë¬¸ìì—´(API í‚¤ ë˜ëŠ” SA JSON)ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    elif isinstance(auth_credentials_conf_val, list): # auth_credentialsê°€ í‚¤ ëª©ë¡ì¼ ê²½ìš°
                        valid_keys_from_auth_cred = [k for k in auth_credentials_conf_val if isinstance(k, str) and k.strip()]
                        if valid_keys_from_auth_cred:
                            auth_credentials_for_gemini_client = valid_keys_from_auth_cred
                            logger.info(f"auth_credentialsì—ì„œ {len(valid_keys_from_auth_cred)}ê°œì˜ API í‚¤ ëª©ë¡ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    elif isinstance(auth_credentials_conf_val, dict): # SA ì •ë³´ (dict)
                        auth_credentials_for_gemini_client = auth_credentials_conf_val
                        logger.info("auth_credentials ê°’ì„ ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´(dict)ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

                if auth_credentials_for_gemini_client is None:
                    logger.warning("Gemini Developer API ëª¨ë“œì´ì§€ë§Œ ì‚¬ìš©í•  API í‚¤ê°€ ì„¤ì •ì— ì—†ìŠµë‹ˆë‹¤.")

            should_initialize_client = False
            if auth_credentials_for_gemini_client:
                if isinstance(auth_credentials_for_gemini_client, str) and auth_credentials_for_gemini_client.strip():
                    should_initialize_client = True
                elif isinstance(auth_credentials_for_gemini_client, list) and auth_credentials_for_gemini_client:
                    should_initialize_client = True
                elif isinstance(auth_credentials_for_gemini_client, dict):
                    should_initialize_client = True
            elif use_vertex and not auth_credentials_for_gemini_client and \
                 (gcp_project_from_config or os.environ.get("GOOGLE_CLOUD_PROJECT")): # ADC ì‚¬ìš© ê¸°ëŒ€
                should_initialize_client = True
                logger.info("Vertex AI ì‚¬ìš© ë° í”„ë¡œì íŠ¸ ID ì¡´ì¬ (ì„¤ì • ë˜ëŠ” í™˜ê²½ë³€ìˆ˜)ë¡œ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¡°ê±´ ì¶©ì¡± (ì¸ì¦ì •ë³´ëŠ” ADC ê¸°ëŒ€).")


            logger.debug(f"[AppService.load_app_config] GeminiClient ì´ˆê¸°í™” ì „: should_initialize_client={should_initialize_client}")
            logger.debug(f"[AppService.load_app_config] auth_credentials_for_gemini_client íƒ€ì…: {type(auth_credentials_for_gemini_client)}")
            if isinstance(auth_credentials_for_gemini_client, str) and len(auth_credentials_for_gemini_client) > 200:
                 logger.debug(f"[AppService.load_app_config] auth_credentials_for_gemini_client (ì¼ë¶€): {auth_credentials_for_gemini_client[:100]}...{auth_credentials_for_gemini_client[-100:]}")
            elif isinstance(auth_credentials_for_gemini_client, dict):
                 logger.debug(f"[AppService.load_app_config] auth_credentials_for_gemini_client (í‚¤ ëª©ë¡): {list(auth_credentials_for_gemini_client.keys())}")
            else:
                 logger.debug(f"[AppService.load_app_config] auth_credentials_for_gemini_client: {auth_credentials_for_gemini_client}")

            if should_initialize_client:
                try:
                    project_to_pass_to_client = gcp_project_from_config if gcp_project_from_config and gcp_project_from_config.strip() else None
                    rpm_value = self.config.get("requests_per_minute")
                    logger.info(f"GeminiClient ì´ˆê¸°í™” ì‹œë„: project='{project_to_pass_to_client}', location='{gcp_location}', RPM='{rpm_value}'")
                    self.gemini_client = GeminiClient(
                        auth_credentials=auth_credentials_for_gemini_client,
                        project=project_to_pass_to_client,
                        location=gcp_location,
                        requests_per_minute=rpm_value
                    )
                except GeminiInvalidRequestException as e_inv:
                    logger.error(f"GeminiClient ì´ˆê¸°í™” ì‹¤íŒ¨ (ì˜ëª»ëœ ìš”ì²­/ì¸ì¦): {e_inv}")
                    self.gemini_client = None
                except Exception as e_client:
                    logger.error(f"GeminiClient ì´ˆê¸°í™” ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e_client}", exc_info=True)
                    self.gemini_client = None
            else:
                logger.warning("API í‚¤ ë˜ëŠ” Vertex AI ì„¤ì •ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”ë¥¼ ì‹œë„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                self.gemini_client = None

            if self.gemini_client:
                self.translation_service = TranslationService(self.gemini_client, self.config)
                self.glossary_service = SimpleGlossaryService(self.gemini_client, self.config) # Changed to SimpleGlossaryService
                logger.info("TranslationService ë° SimpleGlossaryServiceê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.") # Message updated
            else:
                self.translation_service = None
                self.glossary_service = None # Renamed
                logger.warning("Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ë²ˆì—­ ë° ê³ ìœ ëª…ì‚¬ ì„œë¹„ìŠ¤ê°€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

            return self.config
        except FileNotFoundError as e:
            logger.error(f"ì„¤ì • íŒŒì¼ ì°¾ê¸° ì‹¤íŒ¨: {e}")
            self.config = self.config_manager.get_default_config()
            logger.warning("ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤. Gemini í´ë¼ì´ì–¸íŠ¸ëŠ” ì´ˆê¸°í™”ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            self.gemini_client = None
            self.translation_service = None # Keep
            self.glossary_service = None # Renamed
            return self.config
        except Exception as e:
            logger.error(f"ì„¤ì • ë¡œë“œ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            raise BtgConfigException(f"ì„¤ì • ë¡œë“œ ì˜¤ë¥˜: {e}", original_exception=e) from e

    def save_app_config(self, config_data: Dict[str, Any]) -> bool:
        logger.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ì €ì¥ ì¤‘...")
        try:
            success = self.config_manager.save_config(config_data)
            if success:
                logger.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ì €ì¥ ì™„ë£Œ.")
                # ì €ì¥ í›„ì—ëŠ” íŒŒì¼ì—ì„œ ìµœì‹  ì„¤ì •ì„ ë¡œë“œí•˜ë¯€ë¡œ runtime_overrides ì—†ì´ í˜¸ì¶œ
                # config_dataê°€ ìµœì‹  ìƒíƒœì´ë¯€ë¡œ, ì´ë¥¼ self.configì— ë°˜ì˜í•˜ê³  í´ë¼ì´ì–¸íŠ¸ë¥¼ ì¬ì„¤ì •í•  ìˆ˜ë„ ìˆì§€ë§Œ,
                # load_app_config()ë¥¼ í˜¸ì¶œí•˜ì—¬ ì¼ê´€ëœ ë¡œì§ì„ ë”°ë¥´ëŠ” ê²ƒì´ ë” ê°„ë‹¨í•©ë‹ˆë‹¤.
                self.load_app_config() # runtime_overrides=None (ê¸°ë³¸ê°’)
            return success
        except Exception as e:
            logger.error(f"ì„¤ì • ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise BtgConfigException(f"ì„¤ì • ì €ì¥ ì˜¤ë¥˜: {e}", original_exception=e) from e

    def get_available_models(self) -> List[Dict[str, Any]]:
        if not self.gemini_client:
            logger.error("ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            raise BtgServiceException("Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ ë˜ëŠ” Vertex AI ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        logger.info("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì„œë¹„ìŠ¤ í˜¸ì¶œë¨.")
        try:
            all_models = self.gemini_client.list_models()
            # ëª¨ë¸ í•„í„°ë§ ë¡œì§ ì œê±°ë¨
            logger.info(f"ì´ {len(all_models)}ê°œì˜ ëª¨ë¸ì„ APIë¡œë¶€í„° ì§ì ‘ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return all_models
            
        except BtgApiClientException as e:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ API ì˜¤ë¥˜: {e}")
            raise
        except Exception as e:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True) # type: ignore
            raise BtgServiceException(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}", original_exception=e) from e

    def extract_glossary(
        self,
        input_file_path: Union[str, Path],
        progress_callback: Optional[Callable[[GlossaryExtractionProgressDTO], None]] = None, # DTO Changed
        novel_language_code: Optional[str] = None, # ëª…ì‹œì  ì–¸ì–´ ì½”ë“œ ì „ë‹¬
        seed_glossary_path: Optional[Union[str, Path]] = None, # CLIì—ì„œ ì „ë‹¬ëœ ì‹œë“œ ìš©ì–´ì§‘ ê²½ë¡œ
        user_override_glossary_extraction_prompt: Optional[str] = None # ì‚¬ìš©ì ì¬ì •ì˜ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
        # tqdm_file_stream is not typically used by lorebook extraction directly in AppService,
        # but can be passed down if SimpleGlossaryService supports it (currently it doesn't directly)
        # For CLI, tqdm is handled in the CLI module itself.
    ) -> Path:
        if not self.glossary_service: # Changed from pronoun_service
            logger.error("ìš©ì–´ì§‘ ì¶”ì¶œ ì„œë¹„ìŠ¤ ì‹¤íŒ¨: ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.") # Message updated
            raise BtgServiceException("ìš©ì–´ì§‘ ì¶”ì¶œ ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.") # Message updated
        
        logger.info(f"ìš©ì–´ì§‘ ì¶”ì¶œ ì„œë¹„ìŠ¤ ì‹œì‘: {input_file_path}, ì‹œë“œ íŒŒì¼: {seed_glossary_path}")  
        try:
            file_content = read_text_file(input_file_path)
            if not file_content:
                logger.warning(f"ì…ë ¥ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {input_file_path}")
                # For lorebook, an empty input means an empty lorebook, unless a seed is provided.
                # SimpleGlossaryService.extract_and_save_lorebook handles empty content.
        
            # ë¡œì–´ë¶ ì¶”ì¶œ ì‹œ ì‚¬ìš©í•  ì–¸ì–´ ì½”ë“œ ê²°ì •
            # 1. ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬ëœ novel_language_code
            # 2. ì„¤ì • íŒŒì¼ì˜ novel_language (í†µí•©ë¨)
            # 3. None (SimpleGlossaryServiceì—ì„œ ìì²´ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê±°ë‚˜ ì–¸ì–´ íŠ¹ì • ê¸°ëŠ¥ ë¹„í™œì„±í™”)
            lang_code_for_extraction = novel_language_code or self.config.get("novel_language") # í†µí•©ëœ ì„¤ì • ì‚¬ìš©

            # ì‚¬ìš©í•  í”„ë¡¬í”„íŠ¸ ê²°ì •: ë©”ì„œë“œ ì¸ìë¡œ ì „ë‹¬ëœ ê²ƒì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©, ì—†ìœ¼ë©´ ì„¤ì • íŒŒì¼ ê°’ ì‚¬ìš©
            prompt_to_use = user_override_glossary_extraction_prompt \
                if user_override_glossary_extraction_prompt is not None \
                else self.config.get("user_override_glossary_extraction_prompt")

            # lang_code_for_extractionì€ SimpleGlossaryService.extract_and_save_glossaryì—ì„œ ì§ì ‘ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ.
            result_path = self.glossary_service.extract_and_save_glossary( # type: ignore
                novel_text_content=file_content,
                input_file_path_for_naming=input_file_path,
                progress_callback=progress_callback,
                seed_glossary_path=seed_glossary_path, # ì‹œë“œ ìš©ì–´ì§‘ ê²½ë¡œ ì „ë‹¬
                user_override_glossary_extraction_prompt=prompt_to_use # ê²°ì •ëœ í”„ë¡¬í”„íŠ¸ ì „ë‹¬
            )
            logger.info(f"ìš©ì–´ì§‘ ì¶”ì¶œ ì™„ë£Œ. ê²°ê³¼ íŒŒì¼: {result_path}") # Message updated
        
            return result_path
        except FileNotFoundError as e:
            logger.error(f"ìš©ì–´ì§‘ ì¶”ì¶œì„ ìœ„í•œ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file_path}") # Message updated
            if progress_callback:
                progress_callback(GlossaryExtractionProgressDTO(0,0,f"ì˜¤ë¥˜: ì…ë ¥ íŒŒì¼ ì—†ìŒ - {e.filename}",0)) # DTO Changed
            raise BtgFileHandlerException(f"ì…ë ¥ íŒŒì¼ ì—†ìŒ: {input_file_path}", original_exception=e) from e
        except (BtgBusinessLogicException, BtgApiClientException) as e: # BtgPronounException replaced with BtgBusinessLogicException
            logger.error(f"ìš©ì–´ì§‘ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}") # Message updated
            if progress_callback:
                progress_callback(GlossaryExtractionProgressDTO(0,0,f"ì˜¤ë¥˜: {e}",0)) # DTO Changed
            raise
        except Exception as e: 
            logger.error(f"ìš©ì–´ì§‘ ì¶”ì¶œ ì„œë¹„ìŠ¤ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)  # Message updated
            if progress_callback:
                progress_callback(GlossaryExtractionProgressDTO(0,0,f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}",0)) # DTO Changed
            raise BtgServiceException(f"ìš©ì–´ì§‘ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}", original_exception=e) from e # Message updated


    def _translate_and_save_chunk(self, chunk_index: int, chunk_text: str,
                            current_run_output_file: Path,
                            total_chunks: int,
                            input_file_path_for_metadata: Path,
                            progress_callback: Optional[Callable[[TranslationJobProgressDTO], None]] = None) -> bool:
        current_chunk_info_msg = f"ì²­í¬ {chunk_index + 1}/{total_chunks}"
        
        # ì²­í¬ ë¶„ì„ ë° ìƒì„¸ ì •ë³´ ë¡œê¹…
        chunk_lines = chunk_text.count('\n') + 1
        chunk_words = len(chunk_text.split())
        chunk_chars = len(chunk_text)
        chunk_preview = chunk_text[:100].replace('\n', ' ') + '...' if len(chunk_text) > 100 else chunk_text
        
        logger.info(f"{current_chunk_info_msg} ì²˜ë¦¬ ì‹œì‘")
        logger.info(f"  ğŸ“ ì²­í¬ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {chunk_preview}")
        logger.debug(f"  ğŸ“Š ì²­í¬ í†µê³„: ê¸€ì ìˆ˜={chunk_chars}, ë‹¨ì–´ ìˆ˜={chunk_words}, ì¤„ ìˆ˜={chunk_lines}")
        
        start_time = time.time()
        last_error = None
        success = False

        

        try:
            if self.stop_requested:
                logger.info(f"{current_chunk_info_msg} â¸ï¸ ì²˜ë¦¬ ì¤‘ì§€ë¨ (ì‚¬ìš©ì ìš”ì²­)")
                return False

            if not self.translation_service:
                raise BtgServiceException("TranslationServiceê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            # ë²ˆì—­ ì„¤ì • ì •ë³´ ë¡œê¹…
            use_content_safety_retry = self.config.get("use_content_safety_retry", True)
            max_split_attempts = self.config.get("max_content_safety_split_attempts", 3)
            min_chunk_size = self.config.get("min_content_safety_chunk_size", 100)
            model_name = self.config.get("model_name", "gemini-2.0-flash")
            system_instruction = self.config.get("system_instruction", "") # AppService ë ˆë²¨ì—ì„œëŠ” ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ. TranslationServiceê°€ configì—ì„œ ì½ìŒ.
            
            logger.debug(f"  âš™ï¸ ë²ˆì—­ ì„¤ì •: ëª¨ë¸={model_name}, ì•ˆì „ì¬ì‹œë„={use_content_safety_retry}")
            if use_content_safety_retry:
                logger.debug(f"  ğŸ”„ ê²€ì—´ ì¬ì‹œë„ ì„¤ì •: ìµœëŒ€ì‹œë„={max_split_attempts}, ìµœì†Œí¬ê¸°={min_chunk_size}")
                logger.info(f"  ğŸ”„ {current_chunk_info_msg} ë²ˆì—­ API í˜¸ì¶œ ì‹œì‘...")
                translation_start_time = time.time()
                
            if use_content_safety_retry:
                logger.debug(f"  ğŸ›¡ï¸ ì½˜í…ì¸  ì•ˆì „ ì¬ì‹œë„ ëª¨ë“œë¡œ ë²ˆì—­ ì‹œì‘")
                translated_chunk = self.translation_service.translate_text_with_content_safety_retry(
                    chunk_text, max_split_attempts, min_chunk_size
                )
            else:
                logger.debug(f"  ğŸ“ ì¼ë°˜ ë²ˆì—­ ëª¨ë“œë¡œ ë²ˆì—­ ì‹œì‘")
                translated_chunk = self.translation_service.translate_text(chunk_text)
            
            translation_time = time.time() - translation_start_time
            translated_length = len(translated_chunk) if translated_chunk else 0
            
            logger.info(f"  âœ… {current_chunk_info_msg} ë²ˆì—­ ì™„ë£Œ (ì†Œìš”: {translation_time:.2f}ì´ˆ)")
            logger.debug(f"    ë²ˆì—­ ê²°ê³¼ ê¸¸ì´: {translated_length} ê¸€ì")
            logger.debug(f"    ë²ˆì—­ ì†ë„: {chunk_chars/translation_time:.1f} ê¸€ì/ì´ˆ" if translation_time > 0 else "    ë²ˆì—­ ì†ë„: ì¦‰ì‹œ ì™„ë£Œ")# ìƒˆë¡œìš´ ê²€ì—´ ì¬ì‹œë„ ë¡œì§ ì‚¬ìš©
            if self.stop_requested:
                logger.warning(f"  âš ï¸ {current_chunk_info_msg}ì˜ ë²ˆì—­ ê²°ê³¼ë¥¼ ë°›ì•˜ì§€ë§Œ, ì‹œìŠ¤í…œì´ ì¤‘ì§€ë˜ì–´ ê²°ê³¼ë¥¼ ì €ì¥í•˜ì§€ ì•Šê³  íê¸°í•©ë‹ˆë‹¤.")
                return False

            # íŒŒì¼ ì €ì¥ ê³¼ì • ë¡œê¹…
            logger.debug(f"  ğŸ’¾ {current_chunk_info_msg} ê²°ê³¼ ì €ì¥ ì‹œì‘...")
            save_start_time = time.time()
            
            save_chunk_with_index_to_file(current_run_output_file, chunk_index, translated_chunk)
            
            save_time = time.time() - save_start_time
            logger.debug(f"  ğŸ’¾ íŒŒì¼ ì €ì¥ ì™„ë£Œ (ì†Œìš”: {save_time:.3f}ì´ˆ)")
            
            success = True
            
            total_processing_time = time.time() - start_time
            logger.info(f"  ğŸ¯ {current_chunk_info_msg} ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ (ì´ ì†Œìš”: {total_processing_time:.2f}ì´ˆ)")

        except BtgTranslationException as e_trans:
            if self.stop_requested:
                logger.warning(f"  âš ï¸ {current_chunk_info_msg}ì—ì„œ ë²ˆì—­ ì˜ˆì™¸ê°€ ë°œìƒí–ˆìœ¼ë‚˜, ì‹œìŠ¤í…œì´ ì¤‘ì§€ë˜ì–´ ì˜¤ë¥˜ ê¸°ë¡ì„ ìƒëµí•©ë‹ˆë‹¤.")
                return False
            processing_time = time.time() - start_time
            logger.error(f"  âŒ {current_chunk_info_msg} ë²ˆì—­ ì‹¤íŒ¨ (ì†Œìš”: {processing_time:.2f}ì´ˆ)")
            logger.error(f"    ì˜¤ë¥˜ ìœ í˜•: ë²ˆì—­ ì„œë¹„ìŠ¤ ì˜¤ë¥˜")
            logger.error(f"    ì˜¤ë¥˜ ë‚´ìš©: {e_trans}")
            
            if "ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œ" in str(e_trans):
                logger.warning(f"    ğŸ›¡ï¸ ì½˜í…ì¸  ê²€ì—´ë¡œ ì¸í•œ ì‹¤íŒ¨")
            
            save_chunk_with_index_to_file(current_run_output_file, chunk_index, f"[ë²ˆì—­ ì‹¤íŒ¨: {e_trans}]\n\n--- ì›ë¬¸ ë‚´ìš© ---\n{chunk_text}")
            last_error = str(e_trans)
            success = False

        except BtgApiClientException as e_api:
            if self.stop_requested:
                logger.warning(f"  âš ï¸ {current_chunk_info_msg}ì—ì„œ API ì˜ˆì™¸ê°€ ë°œìƒí–ˆìœ¼ë‚˜, ì‹œìŠ¤í…œì´ ì¤‘ì§€ë˜ì–´ ì˜¤ë¥˜ ê¸°ë¡ì„ ìƒëµí•©ë‹ˆë‹¤.")
                return False
            processing_time = time.time() - start_time
            logger.error(f"  âŒ {current_chunk_info_msg} API ì˜¤ë¥˜ë¡œ ë²ˆì—­ ì‹¤íŒ¨ (ì†Œìš”: {processing_time:.2f}ì´ˆ)")
            logger.error(f"    ì˜¤ë¥˜ ìœ í˜•: API í´ë¼ì´ì–¸íŠ¸ ì˜¤ë¥˜")
            logger.error(f"    ì˜¤ë¥˜ ë‚´ìš©: {e_api}")
            
            # API ì˜¤ë¥˜ ìœ í˜•ë³„ ë¶„ë¥˜
            if "ì‚¬ìš©ëŸ‰ ì œí•œ" in str(e_api) or "429" in str(e_api):
                logger.warning(f"    âš ï¸ API ì‚¬ìš©ëŸ‰ ì œí•œ ì˜¤ë¥˜")
            elif "í‚¤" in str(e_api).lower() or "ì¸ì¦" in str(e_api):
                logger.warning(f"    ğŸ”‘ API ì¸ì¦ ê´€ë ¨ ì˜¤ë¥˜")
            
            save_chunk_with_index_to_file(current_run_output_file, chunk_index, f"[API ì˜¤ë¥˜ë¡œ ë²ˆì—­ ì‹¤íŒ¨: {e_api}]\n\n--- ì›ë¬¸ ë‚´ìš© ---\n{chunk_text}")
            last_error = str(e_api)
            success = False

        except Exception as e_gen:
            if self.stop_requested:
                logger.warning(f"  âš ï¸ {current_chunk_info_msg}ì—ì„œ ì¼ë°˜ ì˜ˆì™¸ê°€ ë°œìƒí–ˆìœ¼ë‚˜, ì‹œìŠ¤í…œì´ ì¤‘ì§€ë˜ì–´ ì˜¤ë¥˜ ê¸°ë¡ì„ ìƒëµí•©ë‹ˆë‹¤.")
                return False
            processing_time = time.time() - start_time
            logger.error(f"  âŒ {current_chunk_info_msg} ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ (ì†Œìš”: {processing_time:.2f}ì´ˆ)", exc_info=True)
            logger.error(f"    ì˜¤ë¥˜ ìœ í˜•: {type(e_gen).__name__}")
            logger.error(f"    ì˜¤ë¥˜ ë‚´ìš©: {e_gen}")
            
            save_chunk_with_index_to_file(current_run_output_file, chunk_index, f"[ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ë¡œ ë²ˆì—­ ì‹¤íŒ¨: {e_gen}]\n\n--- ì›ë¬¸ ë‚´ìš© ---\n{chunk_text}")
            last_error = str(e_gen)
            success = False
                    

        
        finally:
            total_time = time.time() - start_time
            with self._progress_lock:
                # ì‹œìŠ¤í…œì´ ì¤‘ì§€ ìƒíƒœê°€ ì•„ë‹ˆë¼ë©´, ì§„í–‰ ìƒí™©ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
                # ì´ ê²€ì‚¬ë¥¼ _progress_lock ì•ˆì—ì„œ ìˆ˜í–‰í•˜ì—¬, í”Œë˜ê·¸ í™•ì¸ê³¼ ì¹´ìš´í„° ì—…ë°ì´íŠ¸ ì‚¬ì´ì˜
                # ê²½ìŸ ì¡°ê±´ì„ ì™„ë²½í•˜ê²Œ ë°©ì§€í•©ë‹ˆë‹¤. ì´ê²ƒì´ ë°ì´í„° ì •í•©ì„±ì„ ë³´ì¥í•˜ëŠ” ìµœì¢… ë°©ì–´ì„ ì…ë‹ˆë‹¤.
                if not self.stop_requested:
                    # 1ë‹¨ê³„: ë¨¼ì € processed_chunks_count ì¦ê°€
                    self.processed_chunks_count += 1
                    # 2ë‹¨ê³„: ê²°ê³¼ì— ë”°ë¼ ì„±ê³µ/ì‹¤íŒ¨ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
                    if success:
                        self.successful_chunks_count += 1
                        # âœ… ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸: translated_chunksì— ì™„ë£Œëœ ì²­í¬ ê¸°ë¡
                        try:
                            metadata_updated = update_metadata_for_chunk_completion(input_file_path_for_metadata, chunk_index)
                            if metadata_updated:
                                logger.debug(f"  ğŸ’¾ {current_chunk_info_msg} ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                            else:
                                logger.warning(f"  âš ï¸ {current_chunk_info_msg} ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
                        except Exception as meta_e:
                            logger.error(f"  âŒ {current_chunk_info_msg} ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {meta_e}")
                    else: # 'success'ê°€ Falseì¸ ê²½ìš°, ì‹¤íŒ¨ ì¹´ìš´í„°ë¥¼ ì¦ê°€ì‹œí‚µë‹ˆë‹¤.
                        self.failed_chunks_count += 1
                        #  ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ ê¸°ë¡
                        if last_error:
                            try:
                                update_metadata_for_chunk_failure(input_file_path_for_metadata, chunk_index, last_error)
                                logger.debug(f"  ğŸ’¾ {current_chunk_info_msg} ì‹¤íŒ¨ ì •ë³´ ë©”íƒ€ë°ì´í„°ì— ê¸°ë¡ ì™„ë£Œ")
                            except Exception as meta_fail_e:
                                logger.error(f"  âŒ {current_chunk_info_msg} ì‹¤íŒ¨ ì •ë³´ ë©”íƒ€ë°ì´í„° ê¸°ë¡ ì¤‘ ì˜¤ë¥˜: {meta_fail_e}")
                    
                    # 3ë‹¨ê³„: ëª¨ë“  ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ í›„ ì§„í–‰ë¥  ê³„ì‚°
                    progress_percentage = (self.processed_chunks_count / total_chunks) * 100
                    logger.info(f"  ğŸ“ˆ ì „ì²´ ì§„í–‰ë¥ : {progress_percentage:.1f}% ({self.processed_chunks_count}/{total_chunks})")
                    
                    # ì„±ê³µë¥  ê³„ì‚°
                    if self.processed_chunks_count > 0:
                        success_rate = (self.successful_chunks_count / self.processed_chunks_count) * 100
                        logger.info(f"  ğŸ“Š ì„±ê³µë¥ : {success_rate:.1f}% (ì„±ê³µ: {self.successful_chunks_count}, ì‹¤íŒ¨: {self.failed_chunks_count})")

                    # ì˜ˆìƒ ì™„ë£Œ ì‹œê°„ ê³„ì‚° (ì„ íƒì‚¬í•­)
                    if total_time > 0 and self.processed_chunks_count > 0:
                        avg_time_per_chunk = total_time / 1  # í˜„ì¬ ì²­í¬ ê¸°ì¤€
                        remaining_chunks = total_chunks - self.processed_chunks_count
                        estimated_remaining_time = remaining_chunks * avg_time_per_chunk
                        logger.debug(f"  â±ï¸ ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {estimated_remaining_time:.1f}ì´ˆ (í‰ê·  {avg_time_per_chunk:.2f}ì´ˆ/ì²­í¬)")


                    if progress_callback:
                        if success:
                            status_msg_for_dto = f"âœ… ì²­í¬ {chunk_index + 1}/{total_chunks} ì™„ë£Œ ({total_time:.1f}ì´ˆ)"
                        else:
                            status_msg_for_dto = f"âŒ ì²­í¬ {chunk_index + 1}/{total_chunks} ì‹¤íŒ¨ ({total_time:.1f}ì´ˆ)"
                            if last_error:
                                status_msg_for_dto += f" - {last_error[:50]}..."

                        progress_dto = TranslationJobProgressDTO(
                            total_chunks=total_chunks,
                            processed_chunks=self.processed_chunks_count,
                            successful_chunks=self.successful_chunks_count,
                            failed_chunks=self.failed_chunks_count,
                            current_status_message=status_msg_for_dto,
                            current_chunk_processing=chunk_index + 1,
                            last_error_message=last_error
                        )
                        progress_callback(progress_dto)
                else:
                    # stop_requestedê°€ Trueì´ë©´, ì•„ë¬´ ì‘ì—…ë„ ìˆ˜í–‰í•˜ì§€ ì•Šê³  ì¡°ìš©íˆ ì¢…ë£Œí•©ë‹ˆë‹¤.
                    # ì´ ì¢€ë¹„ ìŠ¤ë ˆë“œì˜ ê²°ê³¼ëŠ” ë²„ë ¤ì§€ë©°, ì–´ë–¤ ê³µìœ  ìƒíƒœë„ ì˜¤ì—¼ì‹œí‚¤ì§€ ì•ŠìŠµë‹ˆë‹¤.
                    logger.warning(f"  âš ï¸ {current_chunk_info_msg}ì˜ ìµœì¢… ì²˜ë¦¬(ì§„í–‰ë¥ , ë©”íƒ€ë°ì´í„°)ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤ (ì‹œìŠ¤í…œ ì¤‘ì§€ë¨).")
            
            logger.debug(f"  {current_chunk_info_msg} ì²˜ë¦¬ ì™„ë£Œ ë°˜í™˜: {success}")
            return success



    def start_translation(
        self,
        input_file_path: Union[str, Path],
        output_file_path: Union[str, Path],
        progress_callback: Optional[Callable[[TranslationJobProgressDTO], None]] = None,
        status_callback: Optional[Callable[[str], None]] = None,
        tqdm_file_stream: Optional[Any] = None,
        blocking: bool = False, # blocking ë§¤ê°œë³€ìˆ˜ ì¶”ê°€
        retranslate_failed_only: bool = False # ì‹¤íŒ¨ ì²­í¬ ì¬ë²ˆì—­ ëª¨ë“œ
    ) -> None:
        # === ìš©ì–´ì§‘ ë™ì  ë¡œë”© ë¡œì§ ì¶”ê°€ ===
        try:
            input_p = Path(input_file_path)
            # ì„¤ì •ì—ì„œ ìš©ì–´ì§‘ íŒŒì¼ ì ‘ë¯¸ì‚¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
            glossary_suffix = self.config.get("glossary_output_json_filename_suffix", "_simple_glossary.json")
            # í˜„ì¬ ì…ë ¥ íŒŒì¼ì— í•´ë‹¹í•˜ëŠ” ìš©ì–´ì§‘ íŒŒì¼ ê²½ë¡œë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.
            assumed_glossary_path = input_p.parent / f"{input_p.stem}{glossary_suffix}"

            # configì— ì„¤ì •ëœ ê²½ë¡œì™€ ì¶”ì •ëœ ê²½ë¡œë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
            # 1. ì¶”ì •ëœ ê²½ë¡œì— íŒŒì¼ì´ ì¡´ì¬í•˜ë©´, í•´ë‹¹ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •ì„ ë®ì–´ì”ë‹ˆë‹¤.
            # 2. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´, configì— ëª…ì‹œëœ ê²½ë¡œ(ì‚¬ìš©ìê°€ ìˆ˜ë™ìœ¼ë¡œ ì§€ì •í•œ ê²½ë¡œ)ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
            
            glossary_to_use = None
            if assumed_glossary_path.exists():
                glossary_to_use = str(assumed_glossary_path)
                logger.info(f"'{input_p.name}'ì— ëŒ€í•œ ìš©ì–´ì§‘ '{assumed_glossary_path.name}'ì„(ë¥¼) ìë™ìœ¼ë¡œ ë°œê²¬í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            else:
                # ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •ëœ ê²½ë¡œê°€ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
                manual_path = self.config.get("glossary_json_path")
                if manual_path and Path(manual_path).exists():
                    glossary_to_use = manual_path
                    logger.info(f"ìë™ìœ¼ë¡œ ë°œê²¬ëœ ìš©ì–´ì§‘ì´ ì—†ì–´, ì„¤ì •ëœ ê²½ë¡œ '{manual_path}'ì˜ ìš©ì–´ì§‘ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                else:
                    logger.info(f"'{input_p.name}'ì— ëŒ€í•œ ìš©ì–´ì§‘ì„ ì°¾ì„ ìˆ˜ ì—†ì–´, ìš©ì–´ì§‘ ì—†ì´ ë²ˆì—­ì„ ì§„í–‰í•©ë‹ˆë‹¤.")

            # ì°¾ì€ ìš©ì–´ì§‘ ê²½ë¡œë¥¼ ëŸ°íƒ€ì„ ì„¤ì •ì— ë°˜ì˜í•˜ì—¬ TranslationServiceê°€ ë¡œë“œí•˜ë„ë¡ í•©ë‹ˆë‹¤.
            if self.translation_service:
                # TranslationServiceê°€ ìƒˆë¡œìš´ ì„¤ì •ì„ ê¸°ë°˜ìœ¼ë¡œ ìš©ì–´ì§‘ì„ ë‹¤ì‹œ ë¡œë“œí•˜ë„ë¡ í•©ë‹ˆë‹¤.
                self.config['glossary_json_path'] = glossary_to_use
                self.translation_service.config = self.config
                self.translation_service._load_glossary_data() # TranslationService ë‚´ë¶€ì˜ ìš©ì–´ì§‘ ë°ì´í„°ë¥¼ ê°±ì‹ í•©ë‹ˆë‹¤.

        except Exception as e:
            logger.error(f"ìš©ì–´ì§‘ ë™ì  ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            # ìš©ì–´ì§‘ ë¡œë”©ì— ì‹¤íŒ¨í•´ë„ ë²ˆì—­ì€ ê³„ì† ì§„í–‰í•˜ë„ë¡ í•©ë‹ˆë‹¤.
        # === ìš©ì–´ì§‘ ë™ì  ë¡œë”© ë¡œì§ ë ===
        
        # ìŠ¤ë ˆë“œ ìƒì„± ë° ì‹œì‘ ë¶€ë¶„ ìˆ˜ì •
        thread = threading.Thread(
            target=self._translation_task, # ì‹¤ì œ ë²ˆì—­ ë¡œì§ì„ ë³„ë„ ë©”ì„œë“œë¡œ ë¶„ë¦¬
            args=(input_file_path, output_file_path, progress_callback, status_callback, tqdm_file_stream, retranslate_failed_only),
            daemon=not blocking # blocking ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ ë°ëª¬ ìŠ¤ë ˆë“œë¡œ ì„¤ì •
        )
        thread.start()

        if blocking:
            thread.join() # blocking ëª¨ë“œì¼ ê²½ìš° ìŠ¤ë ˆë“œê°€ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°


    def _translation_task( # start_translationì˜ ìŠ¤ë ˆë“œ ì‹¤í–‰ ë¡œì§ì„ ì´ ë©”ì„œë“œë¡œ ì´ë™
        self,
        input_file_path: Union[str, Path],
        output_file_path: Union[str, Path],
        progress_callback: Optional[Callable[[TranslationJobProgressDTO], None]] = None,
        status_callback: Optional[Callable[[str], None]] = None,
        tqdm_file_stream: Optional[Any] = None,
        retranslate_failed_only: bool = False
    ):
        if not self.translation_service or not self.chunk_service:
            logger.error("ë²ˆì—­ ì„œë¹„ìŠ¤ ì‹¤íŒ¨: ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            if status_callback: status_callback("ì˜¤ë¥˜: ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨")
            raise BtgServiceException("ë²ˆì—­ ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")

        with self._translation_lock:
            if self.is_translation_running:
                logger.warning("ë²ˆì—­ ì‘ì—…ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
                if status_callback: status_callback("ê²½ê³ : ë²ˆì—­ ì‘ì—… ì´ë¯¸ ì‹¤í–‰ ì¤‘")
                return
            self.is_translation_running = True
            self.stop_requested = False
            self.processed_chunks_count = 0
            self.successful_chunks_count = 0
            self.failed_chunks_count = 0

        logger.info(f"ë²ˆì—­ ì„œë¹„ìŠ¤ ì‹œì‘: ì…ë ¥={input_file_path}, ìµœì¢… ì¶œë ¥={output_file_path}")
        if status_callback: status_callback("ë²ˆì—­ ì‹œì‘ë¨...")

        # TranslationServiceì— ì¤‘ë‹¨ í™•ì¸ ì½œë°± ì„¤ì •
        if self.translation_service:
            self.translation_service.set_stop_check_callback(lambda: self.stop_requested)
            logger.debug("TranslationServiceì— ì¤‘ë‹¨ í™•ì¸ ì½œë°±ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")

        input_file_path_obj = Path(input_file_path)
        final_output_file_path_obj = Path(output_file_path)
        metadata_file_path = get_metadata_file_path(input_file_path_obj)
        loaded_metadata: Dict[str, Any] = {}
        resume_translation = False
        total_chunks = 0 

        try:
            with self._progress_lock: 
                if metadata_file_path.exists():
                    try:
                        loaded_metadata = load_metadata(metadata_file_path)
                        if loaded_metadata: logger.info(f"ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì„±ê³µ: {metadata_file_path}")
                        else: logger.warning(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ '{metadata_file_path}'ì´ ë¹„ì–´ìˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
                    except json.JSONDecodeError as json_err:
                        logger.error(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ '{metadata_file_path}' ì†ìƒ (JSONDecodeError): {json_err}. ìƒˆë¡œ ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                        delete_file(metadata_file_path) 
                    except Exception as e: 
                        logger.error(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ '{metadata_file_path}' ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}. ìƒˆë¡œ ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤.", exc_info=True)
                        delete_file(metadata_file_path) 
                else:
                    logger.info(f"ê¸°ì¡´ ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {metadata_file_path}. ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")

            current_config_hash = _hash_config_for_metadata(self.config)
            previous_config_hash = loaded_metadata.get("config_hash")

            if previous_config_hash and previous_config_hash == current_config_hash:
                resume_translation = True
                logger.info("ì„¤ì • í•´ì‹œê°€ ì¼ì¹˜í•˜ì—¬ ì´ì–´í•˜ê¸° ëª¨ë“œë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
            elif previous_config_hash: 
                logger.warning("í˜„ì¬ ì„¤ì •ì´ ì´ì „ ë©”íƒ€ë°ì´í„°ì˜ ì„¤ì •ê³¼ ë‹¤ë¦…ë‹ˆë‹¤. ìƒˆë¡œ ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                loaded_metadata = {} 
                resume_translation = False
            
            file_content = read_text_file(input_file_path_obj)
            if not file_content:
                logger.warning(f"ì…ë ¥ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {input_file_path_obj}")
                if status_callback: status_callback("ì™„ë£Œ: ì…ë ¥ íŒŒì¼ ë¹„ì–´ìˆìŒ")
                with self._progress_lock:
                    if not loaded_metadata.get("config_hash"): 
                         loaded_metadata = create_new_metadata(input_file_path_obj, 0, self.config)
                    loaded_metadata["status"] = "completed"; loaded_metadata["total_chunks"] = 0
                    loaded_metadata["last_updated"] = time.time()
                    save_metadata(metadata_file_path, loaded_metadata)
                if progress_callback: progress_callback(TranslationJobProgressDTO(0,0,0,0,"ì…ë ¥ íŒŒì¼ ë¹„ì–´ìˆìŒ"))
                with self._translation_lock: self.is_translation_running = False
                return

            all_chunks: List[str] = self.chunk_service.create_chunks_from_file_content(file_content, self.config.get("chunk_size", 6000))
            total_chunks = len(all_chunks) 
            logger.info(f"ì´ {total_chunks}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë¨.")

            current_run_output_file_path = final_output_file_path_obj.with_suffix(final_output_file_path_obj.suffix + '.current_run.tmp')
            with self._progress_lock: 
                delete_file(current_run_output_file_path) 
                current_run_output_file_path.touch()

                if not resume_translation or not loaded_metadata.get("config_hash"): 
                    logger.info("ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ë®ì–´ì”ë‹ˆë‹¤ (ìƒˆë¡œ ì‹œì‘ ë˜ëŠ” ì„¤ì • ë³€ê²½).")
                    loaded_metadata = create_new_metadata(input_file_path_obj, total_chunks, self.config)
                    logger.info(f"ìƒˆë¡œ ë²ˆì—­ì„ ì‹œì‘í•˜ë¯€ë¡œ ìµœì¢… ì¶œë ¥ íŒŒì¼ '{final_output_file_path_obj}'ì„ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                    delete_file(final_output_file_path_obj) 
                    final_output_file_path_obj.touch() 
                else: 
                    if loaded_metadata.get("total_chunks") != total_chunks:
                        logger.warning(f"ì…ë ¥ íŒŒì¼ì˜ ì²­í¬ ìˆ˜ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤ ({loaded_metadata.get('total_chunks')} -> {total_chunks}). ë©”íƒ€ë°ì´í„°ë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                        loaded_metadata = create_new_metadata(input_file_path_obj, total_chunks, self.config)
                        resume_translation = False 
                        logger.info(f"ì²­í¬ ìˆ˜ ë³€ê²½ìœ¼ë¡œ ì¸í•´ ìƒˆë¡œ ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤. ìµœì¢… ì¶œë ¥ íŒŒì¼ '{final_output_file_path_obj}'ì„ ë‹¤ì‹œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
                        delete_file(final_output_file_path_obj); final_output_file_path_obj.touch()
                    else:
                        logger.info(f"ì´ì–´í•˜ê¸° ëª¨ë“œ: ë©”íƒ€ë°ì´í„° ìƒíƒœë¥¼ 'in_progress'ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.")
                    loaded_metadata["status"] = "in_progress" 
                
                loaded_metadata["last_updated"] = time.time()
                save_metadata(metadata_file_path, loaded_metadata)

            chunks_to_process_with_indices: List[Tuple[int, str]] = []
            if retranslate_failed_only:
                if "failed_chunks" in loaded_metadata and loaded_metadata["failed_chunks"]:
                    failed_indices = {int(k) for k in loaded_metadata["failed_chunks"].keys()}
                    for i, chunk_text in enumerate(all_chunks):
                        if i in failed_indices:
                            chunks_to_process_with_indices.append((i, chunk_text))
                    logger.info(f"ì‹¤íŒ¨ ì²­í¬ ì¬ë²ˆì—­ ëª¨ë“œ: {len(chunks_to_process_with_indices)}ê°œ ëŒ€ìƒ.")
                else:
                    logger.info("ì‹¤íŒ¨í•œ ì²­í¬ê°€ ì—†ì–´ ì¬ë²ˆì—­ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            elif resume_translation and "translated_chunks" in loaded_metadata:
                with self._progress_lock:
                    previously_translated_indices = {int(k) for k in loaded_metadata.get("translated_chunks", {}).keys()}
                    self.successful_chunks_count = len(previously_translated_indices)
                    self.processed_chunks_count = self.successful_chunks_count 
                    self.failed_chunks_count = 0 
                for i, chunk_text in enumerate(all_chunks):
                    if i not in previously_translated_indices:
                        chunks_to_process_with_indices.append((i, chunk_text))
                logger.info(f"ì´ì–´í•˜ê¸°: {self.successful_chunks_count}ê°œ ì´ë¯¸ ì™„ë£Œ, {len(chunks_to_process_with_indices)}ê°œ ì¶”ê°€ ë²ˆì—­ ëŒ€ìƒ.")
            else: 
                chunks_to_process_with_indices = list(enumerate(all_chunks))
                logger.info(f"ìƒˆë¡œ ë²ˆì—­: {len(chunks_to_process_with_indices)}ê°œ ë²ˆì—­ ëŒ€ìƒ.")
                
            if not chunks_to_process_with_indices and total_chunks > 0 : 
                logger.info("ë²ˆì—­í•  ìƒˆë¡œìš´ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤ (ëª¨ë“  ì²­í¬ê°€ ì´ë¯¸ ë²ˆì—­ë¨).")
                if status_callback: status_callback("ì™„ë£Œ: ëª¨ë“  ì²­í¬ ì´ë¯¸ ë²ˆì—­ë¨")
                with self._progress_lock:
                    loaded_metadata["status"] = "completed"
                    loaded_metadata["last_updated"] = time.time()
                    save_metadata(metadata_file_path, loaded_metadata)
                if progress_callback:
                    progress_callback(TranslationJobProgressDTO(
                        total_chunks, self.processed_chunks_count, self.successful_chunks_count,
                        self.failed_chunks_count, "ëª¨ë“  ì²­í¬ ì´ë¯¸ ë²ˆì—­ë¨"
                    ))
                with self._translation_lock: self.is_translation_running = False
                delete_file(current_run_output_file_path) 
                return

            initial_status_msg = "ë²ˆì—­ ì¤€ë¹„ ì¤‘..."
            if resume_translation: initial_status_msg = f"ì´ì–´í•˜ê¸° ì¤€ë¹„ (ë‚¨ì€ ì²­í¬: {len(chunks_to_process_with_indices)})"
            if progress_callback:
                with self._progress_lock:
                    progress_callback(TranslationJobProgressDTO(
                        total_chunks, self.processed_chunks_count, self.successful_chunks_count,
                        self.failed_chunks_count, initial_status_msg,
                        (chunks_to_process_with_indices[0][0] + 1 if chunks_to_process_with_indices else None) 
                    ))
            
            max_workers = self.config.get("max_workers", os.cpu_count() or 1)
            if not isinstance(max_workers, int) or max_workers <= 0:
                logger.warning(f"ì˜ëª»ëœ max_workers ê°’ ({max_workers}), ê¸°ë³¸ê°’ (CPU ì½”ì–´ ìˆ˜ ë˜ëŠ” 1)ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
                max_workers = os.cpu_count() or 1
            
            logger.info(f"ìµœëŒ€ {max_workers} ìŠ¤ë ˆë“œë¡œ ë³‘ë ¬ ë²ˆì—­ (ëŒ€ìƒ: {len(chunks_to_process_with_indices)} ì²­í¬)...")

            pbar = None
            if tqdm_file_stream and chunks_to_process_with_indices: 
                pbar = tqdm(total=len(chunks_to_process_with_indices), 
                            desc="ì²­í¬ ë²ˆì—­", 
                            unit="ì²­í¬", 
                            file=tqdm_file_stream, 
                            initial=0, 
                            leave=False)


            self.executor = ThreadPoolExecutor(max_workers=max_workers)
            try:
                future_to_chunk_index: Dict[Any, int] = {}
                for i, chunk_text in chunks_to_process_with_indices:
                    if self.stop_requested:
                        logger.info("ìƒˆ ì‘ì—… ì œì¶œ ì¤‘ë‹¨ë¨ (ì‚¬ìš©ì ìš”ì²­).")
                        break
                    future = self.executor.submit(self._translate_and_save_chunk, i, chunk_text,
                                            current_run_output_file_path,
                                            total_chunks, 
                                            input_file_path_obj, progress_callback)
                    future_to_chunk_index[future] = i

                if self.stop_requested and not future_to_chunk_index:
                    logger.info("ë²ˆì—­ ì‹œì‘ ì „ ì¤‘ì§€ ìš”ì²­ë¨.")
                    if pbar: pbar.close()

                # as_completedì—ì„œ ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦¬ë˜, stop_requestedê°€ Trueê°€ ë˜ë©´ ë£¨í”„ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.
                for future in as_completed(future_to_chunk_index.keys()):
                    if self.stop_requested:
                        # ì‹¤í–‰ ì¤‘ì¸ futureë“¤ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.
                        for f in future_to_chunk_index.keys():
                            if not f.done():
                                f.cancel()
                        logger.info("ì§„í–‰ ì¤‘ì¸ ì‘ì—…ë“¤ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                        break

                    chunk_idx_completed = future_to_chunk_index[future]
                    try:
                        future.result()
                    except Exception as e_future:
                        logger.error(f"ë³‘ë ¬ ì‘ì—… (ì²­í¬ {chunk_idx_completed + 1}) ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ (as_completed): {e_future}", exc_info=True)
                    finally:
                        if pbar: pbar.update(1)
            finally:
                if pbar: pbar.close()
                self.executor.shutdown(wait=False) # ëª¨ë“  ìŠ¤ë ˆë“œê°€ ì¦‰ì‹œ ì¢…ë£Œë˜ë„ë¡ ë³´ì¥
                logger.info("ThreadPoolExecutorê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê²°ê³¼ ë³‘í•©ì„ ì‹œì‘í•©ë‹ˆë‹¤.") 


            logger.info("ëª¨ë“  ëŒ€ìƒ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ. ê²°ê³¼ ë³‘í•© ë° ìµœì¢… ì €ì¥ ì‹œì‘...")
            newly_translated_chunks: Dict[int, str] = {}
            previously_translated_chunks_from_main_output: Dict[int, str] = {}

            try:
                newly_translated_chunks = load_chunks_from_file(current_run_output_file_path)
            except Exception as e:
                logger.error(f"ì„ì‹œ ë²ˆì—­ íŒŒì¼ '{current_run_output_file_path}' ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}. ë³‘í•©ì´ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", exc_info=True)

            final_merged_chunks: Dict[int, str] = {}
            if resume_translation and final_output_file_path_obj.exists(): 
                # âœ… ê°œì„ : ë¨¼ì € ì²­í¬ ì¸ë±ìŠ¤ê°€ ìˆëŠ” íŒŒì¼(.chunked.txt)ì—ì„œ ë¡œë“œ ì‹œë„
                chunked_file_path = final_output_file_path_obj.with_suffix('.chunked.txt')
                
                if chunked_file_path.exists():
                    logger.info(f"ì´ì „ ë²ˆì—­ ê²°ê³¼ë¥¼ ì²­í¬ íŒŒì¼ '{chunked_file_path}'ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤.")
                    try:
                        previously_translated_chunks_from_main_output = load_chunks_from_file(chunked_file_path)
                        final_merged_chunks.update(previously_translated_chunks_from_main_output)
                        logger.info(f"{len(previously_translated_chunks_from_main_output)}ê°œì˜ ì´ì „ ì²­í¬ ë¡œë“œë¨ (ì²­í¬ íŒŒì¼ì—ì„œ).")
                    except Exception as e:
                        logger.error(f"ì²­í¬ íŒŒì¼ '{chunked_file_path}' ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}. ë©”ì¸ íŒŒì¼ì—ì„œ ì‹œë„í•©ë‹ˆë‹¤.", exc_info=True)
                        # ì²­í¬ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë©”ì¸ íŒŒì¼ì—ì„œ ì‹œë„
                        try:
                            logger.info(f"ë©”ì¸ ë²ˆì—­ íŒŒì¼ '{final_output_file_path_obj}'ì—ì„œ ì²­í¬ ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
                            previously_translated_chunks_from_main_output = load_chunks_from_file(final_output_file_path_obj)
                            final_merged_chunks.update(previously_translated_chunks_from_main_output)
                            logger.info(f"{len(previously_translated_chunks_from_main_output)}ê°œì˜ ì´ì „ ì²­í¬ ë¡œë“œë¨ (ë©”ì¸ íŒŒì¼ì—ì„œ).")
                        except Exception as e2:
                            logger.error(f"ë©”ì¸ íŒŒì¼ '{final_output_file_path_obj}' ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e2}. ì´ì „ ë‚´ìš©ì€ ë³‘í•©ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.", exc_info=True)
                else:                    # ì²­í¬ íŒŒì¼ì´ ì—†ìœ¼ë©´ ë©”ì¸ íŒŒì¼ì—ì„œ ì‹œë„
                    logger.info(f"ì²­í¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë©”ì¸ ë²ˆì—­ íŒŒì¼ '{final_output_file_path_obj}'ì—ì„œ ì²­í¬ ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
                    try:
                        previously_translated_chunks_from_main_output = load_chunks_from_file(final_output_file_path_obj)
                        final_merged_chunks.update(previously_translated_chunks_from_main_output)
                        logger.info(f"{len(previously_translated_chunks_from_main_output)}ê°œì˜ ì´ì „ ì²­í¬ ë¡œë“œë¨ (ë©”ì¸ íŒŒì¼ì—ì„œ).")
                    except Exception as e:
                        logger.error(f"ë©”ì¸ íŒŒì¼ '{final_output_file_path_obj}' ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}. ì´ì „ ë‚´ìš©ì€ ë³‘í•©ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.", exc_info=True)

            final_merged_chunks.update(newly_translated_chunks)
            logger.info(f"{len(newly_translated_chunks)}ê°œì˜ ìƒˆ ì²­í¬ ì¶”ê°€/ë®ì–´ì“°ê¸°ë¨. ì´ {len(final_merged_chunks)} ì²­í¬ ë³‘í•© ì¤€ë¹„ ì™„ë£Œ.")
            
            try:
                save_merged_chunks_to_file(final_output_file_path_obj, final_merged_chunks)
                logger.info(f"ìµœì¢… ë²ˆì—­ ê²°ê³¼ê°€ '{final_output_file_path_obj}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                  # âœ… ê°œì„ : í›„ì²˜ë¦¬ ì‹¤í–‰ (ì„¤ì •ì—ì„œ í™œì„±í™”ëœ ê²½ìš°)
                if self.config.get("enable_post_processing", True):
                    logger.info("ë²ˆì—­ ì™„ë£Œ í›„ í›„ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
                    try:
                        # 1. ë¨¼ì € ì²­í¬ ì¸ë±ìŠ¤ê°€ í¬í•¨ëœ ë°±ì—… íŒŒì¼ ì €ì¥ (í›„ì²˜ë¦¬ ì „ ì›ë³¸ ë³´ì¡´)
                        chunked_backup_path = final_output_file_path_obj.with_suffix('.chunked.txt')
                        save_merged_chunks_to_file(chunked_backup_path, final_merged_chunks)
                        logger.info(f"ì´ì–´í•˜ê¸°ìš© ì²­í¬ ë°±ì—… íŒŒì¼ ì €ì¥ ì™„ë£Œ: {chunked_backup_path}")
                        
                        # 2. ì²­í¬ ë‹¨ìœ„ í›„ì²˜ë¦¬ (í—¤ë” ì œê±°, HTML ì •ë¦¬ ë“±)
                        processed_chunks = self.post_processing_service.post_process_merged_chunks(final_merged_chunks)
                        
                        # 3. í›„ì²˜ë¦¬ëœ ë‚´ìš©ì„ ì„ì‹œë¡œ ì €ì¥ (ì²­í¬ ì¸ë±ìŠ¤ëŠ” ì—¬ì „íˆ í¬í•¨)
                        save_merged_chunks_to_file(final_output_file_path_obj, processed_chunks)
                        logger.info("ì²­í¬ ë‹¨ìœ„ í›„ì²˜ë¦¬ ì™„ë£Œ ë° ì„ì‹œ ì €ì¥ë¨.")
                        
                        # 4. ìµœì¢…ì ìœ¼ë¡œ ì²­í¬ ì¸ë±ìŠ¤ ë§ˆì»¤ ì œê±° (ì‚¬ìš©ìê°€ ë³´ëŠ” ìµœì¢… íŒŒì¼)
                        if self.post_processing_service.remove_chunk_indexes_from_final_file(final_output_file_path_obj):
                            logger.info("ìµœì¢… ì¶œë ¥ íŒŒì¼ì—ì„œ ì²­í¬ ì¸ë±ìŠ¤ ë§ˆì»¤ ì œê±° ì™„ë£Œ.")
                        else:
                            logger.warning("ì²­í¬ ì¸ë±ìŠ¤ ë§ˆì»¤ ì œê±°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                            
                        # 5. í›„ì²˜ë¦¬ëœ ë‚´ìš©ë„ ë°±ì—… íŒŒì¼ì— ì—…ë°ì´íŠ¸ (ì²­í¬ ì¸ë±ìŠ¤ í¬í•¨)
                        save_merged_chunks_to_file(chunked_backup_path, processed_chunks)
                        logger.info(f"í›„ì²˜ë¦¬ëœ ì²­í¬ ë°±ì—… íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {chunked_backup_path}")
                        
                    except Exception as post_proc_e:
                        logger.error(f"í›„ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {post_proc_e}. í›„ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.", exc_info=True)
                        # í›„ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œì—ë„ ì²­í¬ ë°±ì—… íŒŒì¼ì€ ë³´ì¥
                        try:
                            chunked_backup_path = final_output_file_path_obj.with_suffix('.chunked.txt')
                            save_merged_chunks_to_file(chunked_backup_path, final_merged_chunks)
                            logger.info(f"í›„ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì²­í¬ ë°±ì—… íŒŒì¼ ì €ì¥: {chunked_backup_path}")
                        except Exception as backup_fallback_e:
                            logger.error(f"ì›ë³¸ ì²­í¬ ë°±ì—… íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {backup_fallback_e}", exc_info=True)
                else:
                    logger.info("í›„ì²˜ë¦¬ê°€ ì„¤ì •ì—ì„œ ë¹„í™œì„±í™”ë˜ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                    # í›„ì²˜ë¦¬ê°€ ë¹„í™œì„±í™”ëœ ê²½ìš°ì—ë„ ì²­í¬ ë°±ì—… íŒŒì¼ì€ ì €ì¥ (ì´ì–´í•˜ê¸°ìš©)
                    try:
                        chunked_backup_path = final_output_file_path_obj.with_suffix('.chunked.txt')
                        save_merged_chunks_to_file(chunked_backup_path, final_merged_chunks)
                        logger.info(f"ì²­í¬ ì¸ë±ìŠ¤ í¬í•¨ ë°±ì—… íŒŒì¼ ì €ì¥: {chunked_backup_path} (í›„ì²˜ë¦¬ ë¹„í™œì„±í™”ë¨)")
                    except Exception as backup_e:
                        logger.error(f"ì²­í¬ ë°±ì—… íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {backup_e}", exc_info=True)
                        
            except Exception as e:
                logger.error(f"ìµœì¢… ë²ˆì—­ ê²°ê³¼ íŒŒì¼ '{final_output_file_path_obj}' ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
                raise BtgFileHandlerException(f"ìµœì¢… ì¶œë ¥ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}", original_exception=e)

            delete_file(current_run_output_file_path)
            logger.info(f"ì„ì‹œ íŒŒì¼ '{current_run_output_file_path}' ì‚­ì œë¨.")

            final_status_msg = "ë²ˆì—­ ì™„ë£Œ."
            with self._progress_lock:
                # í•­ìƒ ìµœì‹  ë©”íƒ€ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë¡œë“œí•˜ì—¬ ë®ì–´ì“°ê¸° ë¬¸ì œë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
                try:
                    # ì‘ì—… ì‹œì‘ ì‹œì ì˜ ë©”íƒ€ë°ì´í„°ê°€ ì•„ë‹Œ, íŒŒì¼ì— ê¸°ë¡ëœ ìµœì‹  ìƒíƒœë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
                    loaded_metadata = load_metadata(metadata_file_path)
                    if not loaded_metadata:
                        logger.warning("ìµœì¢… ë©”íƒ€ë°ì´í„° ì €ì¥ ë‹¨ê³„ì—ì„œ ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                        # ì´ ê²½ìš°, ì •ë³´ê°€ ì¼ë¶€ ìœ ì‹¤ë  ìˆ˜ ìˆì§€ë§Œ ìµœì†Œí•œì˜ êµ¬ì¡°ëŠ” ë³´ì¡´í•©ë‹ˆë‹¤.
                        loaded_metadata = create_new_metadata(input_file_path_obj, total_chunks, self.config)
                    else:
                        logger.info("ìµœì¢… ì €ì¥ì„ ìœ„í•´ ìµœì‹  ë©”íƒ€ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë‹¤ì‹œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    logger.error(f"ìµœì¢… ë©”íƒ€ë°ì´í„° ì €ì¥ ì „, ìµœì‹  ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}. ì¼ë¶€ ì •ë³´ê°€ ìœ ì‹¤ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    # loaded_metadataëŠ” ì´ì „ ìƒíƒœë¥¼ ìœ ì§€í•˜ì§€ë§Œ, ì˜¤ë¥˜ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.

                if self.stop_requested:
                    final_status_msg = "ë²ˆì—­ ì¤‘ë‹¨ë¨."
                    loaded_metadata["status"] = "stopped"
                elif self.failed_chunks_count > 0:
                    final_status_msg = f"ë²ˆì—­ ì™„ë£Œ (ì‹¤íŒ¨ {self.failed_chunks_count}ê°œ í¬í•¨)."
                    loaded_metadata["status"] = "completed_with_errors"
                elif self.successful_chunks_count == total_chunks: 
                    final_status_msg = "ë²ˆì—­ ì™„ë£Œ (ëª¨ë“  ì²­í¬ ì„±ê³µ)."
                    loaded_metadata["status"] = "completed"
                else: 
                    if self.processed_chunks_count == total_chunks: 
                        final_status_msg = f"ë²ˆì—­ ì™„ë£Œ (ì„±ê³µ: {self.successful_chunks_count}/{total_chunks}, ì‹¤íŒ¨: {self.failed_chunks_count})."
                        loaded_metadata["status"] = "completed_with_errors" if self.failed_chunks_count > 0 else "completed_with_pending" 
                    else: 
                        final_status_msg = f"ë²ˆì—­ ì²˜ë¦¬ë¨ (ì²˜ë¦¬ ì‹œë„: {self.processed_chunks_count}/{total_chunks}, ì„±ê³µ: {self.successful_chunks_count})."
                        loaded_metadata["status"] = "unknown_incomplete" 
                logger.info(f"ìµœì¢… ìƒíƒœ ë©”ì‹œì§€: {final_status_msg}")

                loaded_metadata["last_updated"] = time.time()
                save_metadata(metadata_file_path, loaded_metadata)

            if status_callback: status_callback(final_status_msg)
            if progress_callback:
                with self._progress_lock:
                    progress_callback(TranslationJobProgressDTO(
                        total_chunks, self.processed_chunks_count,
                        self.successful_chunks_count, self.failed_chunks_count,
                        final_status_msg
                    ))

        except Exception as e:
            logger.error(f"ë²ˆì—­ ì„œë¹„ìŠ¤ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            if status_callback: status_callback(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
            tc_for_error_dto = total_chunks if 'total_chunks' in locals() and total_chunks > 0 else 0
            pc_for_error_dto = self.processed_chunks_count if hasattr(self, 'processed_chunks_count') else 0
            sc_for_error_dto = self.successful_chunks_count if hasattr(self, 'successful_chunks_count') else 0
            fc_for_error_dto = self.failed_chunks_count if hasattr(self, 'failed_chunks_count') else 0

            if progress_callback: progress_callback(TranslationJobProgressDTO(
                tc_for_error_dto, pc_for_error_dto, sc_for_error_dto, fc_for_error_dto, f"ì˜¤ë¥˜ ë°œìƒ: {e}"
            ))
            
            with self._progress_lock:
                error_metadata = {}
                try:
                    error_metadata = load_metadata(metadata_file_path) 
                except Exception as load_err:
                    logger.error(f"ì˜¤ë¥˜ ë°œìƒ í›„ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {load_err}")

                if not error_metadata.get("config_hash"): 
                    error_metadata = create_new_metadata(input_file_path_obj, tc_for_error_dto, self.config)

                error_metadata["status"] = "error"
                error_metadata["last_updated"] = time.time()
                if 'total_chunks' in locals(): error_metadata["total_chunks"] = total_chunks 
                save_metadata(metadata_file_path, error_metadata)

            if 'current_run_output_file_path' in locals() and current_run_output_file_path.exists():
                delete_file(current_run_output_file_path)
                logger.info(f"ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ì„ì‹œ íŒŒì¼ '{current_run_output_file_path}' ì‚­ì œ ì‹œë„ë¨.")
            raise BtgServiceException(f"ë²ˆì—­ ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}", original_exception=e) from e
        finally:
            # ì¤‘ì§€ ìš”ì²­ì´ ìˆì—ˆë˜ ê²½ìš°, is_translation_runningì€ ì´ë¯¸ Falseë¡œ ì„¤ì •ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            if not self.stop_requested:
                with self._translation_lock:
                    self.is_translation_running = False

    def stop_translation(self):
        if not self.is_translation_running:
            logger.info("ë²ˆì—­ ì‘ì—…ì´ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹ˆë¯€ë¡œ ì¤‘ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        logger.info("ë²ˆì—­ ì¤‘ì§€ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤...")
        self.stop_requested = True

        # ThreadPoolExecutorë¥¼ ì§ì ‘ ì¢…ë£Œí•˜ì—¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë“  ìŠ¤ë ˆë“œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
        if hasattr(self, 'executor') and self.executor:
            # shutdown(wait=False)ëŠ” ëŒ€ê¸° ì¤‘ì¸ futureë¥¼ ì·¨ì†Œí•˜ê³ , ì‹¤í–‰ ì¤‘ì¸ ìŠ¤ë ˆë“œê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤.
            self.executor.shutdown(wait=False)
            logger.info("ThreadPoolExecutorê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

        # is_translation_running í”Œë˜ê·¸ë¥¼ Falseë¡œ ì„¤ì •í•˜ì—¬ ìƒˆë¡œìš´ ì‘ì—…ì´ ì‹œì‘ë˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.
        with self._translation_lock:
            self.is_translation_running = False
        
        logger.info("ë²ˆì—­ ì‘ì—…ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def request_stop_translation(self):
        if self.is_translation_running:
            logger.info("ë²ˆì—­ ì¤‘ì§€ ìš”ì²­ ìˆ˜ì‹ ë¨.")
            self.stop_translation()
        else:
            logger.info("ì‹¤í–‰ ì¤‘ì¸ ë²ˆì—­ ì‘ì—…ì´ ì—†ì–´ ì¤‘ì§€ ìš”ì²­ì„ ë¬´ì‹œí•©ë‹ˆë‹¤.")


if __name__ == '__main__':
    import logging
    from logging import DEBUG # type: ignore

    logger.setLevel(DEBUG)

    test_output_dir = Path("test_app_service_output")
    test_output_dir.mkdir(exist_ok=True)

    temp_config_path = test_output_dir / "temp_config.json"
    sample_app_config_data = {
        "api_key": os.environ.get("GOOGLE_API_KEY", "YOUR_DEFAULT_API_KEY_FOR_TEST"),
        "service_account_file_path": None, 
        "use_vertex_ai": False, 
        "gcp_project": None, 
        "gcp_location": None, 
        "model_name": "gemini-2.0-flash",
        "temperature": 0.7,
        "top_p": 0.9,
        "prompts": "Translate to Korean: {{slot}}",
        "chunk_size": 50, 
        "glossary_json_path": str(test_output_dir / "sample_glossary.json"), # Changed from pronouns_csv
        "requests_per_minute": 60,
        # "max_glossary_entries": 5, # ìš©ì–´ì§‘ìœ¼ë¡œ ëŒ€ì²´ë˜ë©´ì„œ ì´ ì„¤ì •ì€ ì§ì ‘ ì‚¬ìš©ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
        "glossary_sampling_ratio": 100.0, # Changed from pronoun_sample_ratio
        "max_workers": 2 
    }

    with open(temp_config_path, "w", encoding="utf-8") as f:
        json.dump(sample_app_config_data, f, indent=4)

    sample_glossary_file = test_output_dir / "sample_glossary.json" # Changed from sample_pronoun_file
    sample_glossary_content = [
        {"keyword": "BTG", "translated_keyword": "ë¹„í‹°ì§€", "source_language": "en", "target_language": "ko", "occurrence_count": 10},
        {"keyword": "Gemini", "translated_keyword": "ì œë¯¸ë‹ˆ", "source_language": "en", "target_language": "ko", "occurrence_count": 5}
    ]
    with open(sample_glossary_file, "w", encoding="utf-8") as f: # Changed to JSON
        json.dump(sample_glossary_content, f, indent=4, ensure_ascii=False)

    temp_input_file = test_output_dir / "sample_input.txt"
    temp_input_content = (
        "Hello BTG.\nThis is a test for the Gemini API.\n"
        "We are testing the application service layer.\n"
        "Another line for chunking. And one more for Gemini.\n"
        "This is the fifth line.\nAnd the sixth line is here.\n"
        "Seventh line for more data.\nEighth line, almost done."
    )
    with open(temp_input_file, "w", encoding="utf-8") as f:
        f.write(temp_input_content)

    temp_output_file = test_output_dir / "sample_output.txt"
    
    temp_metadata_file = get_metadata_file_path(temp_input_file)
    if temp_metadata_file.exists():
        delete_file(temp_metadata_file)
    if temp_output_file.exists():
        delete_file(temp_output_file)


    app_service: Optional[AppService] = None
    try:
        app_service = AppService(config_file_path=temp_config_path)
        logger.info("AppService ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì„±ê³µ.")
    except Exception as e:
        logger.error(f"AppService ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
        exit()

    if app_service and app_service.gemini_client:
        print("\n--- ëª¨ë¸ ëª©ë¡ ì¡°íšŒ í…ŒìŠ¤íŠ¸ ---")
        try:
            models = app_service.get_available_models()
            if models:
                logger.info(f"ì¡°íšŒëœ ëª¨ë¸ ìˆ˜: {len(models)}")
                for m in models[:2]: 
                    logger.info(f"  - {m.get('display_name', m.get('name'))}")
            else:
                logger.info("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        except BtgApiClientException as e:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        except Exception as e_models:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e_models}", exc_info=True)

    else:
        logger.warning("Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

    if app_service and app_service.glossary_service: # Changed from pronoun_service
        print("\n--- ìš©ì–´ì§‘ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ---") # Changed
        try:
            def _glossary_progress_dto_cb(dto: GlossaryExtractionProgressDTO): # Changed DTO and function name
                logger.debug(f"ìš©ì–´ì§‘ ì§„í–‰ DTO: {dto.processed_segments}/{dto.total_segments} - {dto.current_status_message} (ì¶”ì¶œ í•­ëª©: {dto.extracted_entries_count})") # Changed fields
        
            result_path = app_service.extract_glossary( # Changed method
                temp_input_file,
                progress_callback=_glossary_progress_dto_cb, # Changed callback
                seed_glossary_path=sample_glossary_file # Optionally provide a seed path for testing
            )
            logger.info(f"ìš©ì–´ì§‘ ì¶”ì¶œ ì™„ë£Œ, ê²°ê³¼ íŒŒì¼: {result_path}") # Changed
        except Exception as e:
            logger.error(f"ìš©ì–´ì§‘ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True) # Changed
    else:
        logger.warning("Glossary ì„œë¹„ìŠ¤ê°€ ì—†ì–´ ìš©ì–´ì§‘ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.") # Changed

    if app_service and app_service.translation_service and app_service.gemini_client: # Ensure client exists for translation
        print("\n--- ë²ˆì—­ í…ŒìŠ¤íŠ¸ (ë³‘ë ¬ ì²˜ë¦¬) ---")
        try:
            test_tqdm_stream = sys.stdout 

            def _trans_progress_dto(dto: TranslationJobProgressDTO):
                logger.debug(f"ë²ˆì—­ ì§„í–‰ DTO: {dto.current_chunk_processing or '-'}/{dto.total_chunks}, ì„±ê³µ: {dto.successful_chunks}, ì‹¤íŒ¨: {dto.failed_chunks} - {dto.current_status_message}")
                pass

            def _trans_status(status_msg):
                logger.info(f"ë²ˆì—­ ìƒíƒœ: {status_msg}")


            app_service.start_translation(
                temp_input_file,
                temp_output_file,
                _trans_progress_dto,
                _trans_status,
                tqdm_file_stream=test_tqdm_stream 
            )

            start_time = time.time()
            while app_service.is_translation_running and (time.time() - start_time) < 120: 
                time.sleep(0.5)

            if app_service.is_translation_running:
                logger.warning("ë²ˆì—­ ì‘ì—…ì´ ì‹œê°„ ë‚´ì— ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (í…ŒìŠ¤íŠ¸). ì¤‘ì§€ ìš”ì²­...")
                app_service.request_stop_translation()
                time.sleep(2) 

            if temp_output_file.exists():
                logger.info(f"ë²ˆì—­ ì™„ë£Œ, ê²°ê³¼ íŒŒì¼: {temp_output_file}")
            else:
                logger.error("ë²ˆì—­ ê²°ê³¼ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"ë²ˆì—­ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True)
    else:
        logger.warning("Translation ì„œë¹„ìŠ¤ê°€ ì—†ì–´ ë²ˆì—­ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

    logger.info("AppService í…ŒìŠ¤íŠ¸ ì™„ë£Œ.")
