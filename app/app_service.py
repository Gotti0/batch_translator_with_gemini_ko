# app_service.py
from pathlib import Path
# typing ëª¨ë“ˆì—ì„œ Tupleì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from typing import Dict, Any, Optional, List, Callable, Union, Tuple
import os
import json
import csv
import logging
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed, wait # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬
import time
from tqdm import tqdm # tqdm ì„í¬íŠ¸ í™•ì¸
import sys # sys ì„í¬íŠ¸ í™•ì¸ (tqdm_file_stream=sys.stdout ì— ì‚¬ìš©ë  ìˆ˜ ìˆìŒ)

try:
    from infrastructure.logger_config import setup_logger
except ImportError:
    from infrastructure.logger_config import setup_logger

try:
    # file_handlerì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ë“¤ì„ import í•©ë‹ˆë‹¤.
    from infrastructure.file_handler import (
        read_text_file, write_text_file,
        save_chunk_with_index_to_file, get_metadata_file_path, delete_file,
        load_chunks_from_file,
        create_new_metadata, save_metadata, load_metadata,
        update_metadata_for_chunk_completion, update_metadata_for_chunk_failure, # ì¶”ê°€
        _hash_config_for_metadata,
        save_merged_chunks_to_file
    )
    from ..core.config.config_manager import ConfigManager
    from infrastructure.gemini_client import GeminiClient, GeminiAllApiKeysExhaustedException, GeminiInvalidRequestException
    from domain.translation_service import TranslationService
    from domain.glossary_service import SimpleGlossaryService
    from ..utils.chunk_service import ChunkService
    from ..core.exceptions import BtgServiceException, BtgConfigException, BtgFileHandlerException, BtgApiClientException, BtgTranslationException, BtgBusinessLogicException
    from ..core.dtos import TranslationJobProgressDTO, GlossaryExtractionProgressDTO
    from ..utils.post_processing_service import PostProcessingService
    from ..utils.quality_check_service import QualityCheckService
except ImportError:
    # Fallback imports
    from infrastructure.file_handler import (
        read_text_file, write_text_file,
        save_chunk_with_index_to_file, get_metadata_file_path, delete_file,
        load_chunks_from_file,
        create_new_metadata, save_metadata, load_metadata,
        update_metadata_for_chunk_completion, update_metadata_for_chunk_failure, # ì¶”ê°€
        _hash_config_for_metadata,
        save_merged_chunks_to_file
    )
    from core.config.config_manager import ConfigManager
    from infrastructure.gemini_client import GeminiClient, GeminiAllApiKeysExhaustedException, GeminiInvalidRequestException
    from domain.translation_service import TranslationService
    from domain.glossary_service import SimpleGlossaryService
    from utils.chunk_service import ChunkService
    from core.exceptions import BtgServiceException, BtgConfigException, BtgFileHandlerException, BtgApiClientException, BtgTranslationException, BtgBusinessLogicException
    from core.dtos import TranslationJobProgressDTO, GlossaryExtractionProgressDTO
    from utils.post_processing_service import PostProcessingService
    from utils.quality_check_service import QualityCheckService

logger = setup_logger(__name__)

class AppService:
    """
    ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì£¼ìš” ìœ ìŠ¤ì¼€ì´ìŠ¤ë¥¼ ì¡°ì •í•˜ëŠ” ì„œë¹„ìŠ¤ ê³„ì¸µì…ë‹ˆë‹¤.
    í”„ë ˆì  í…Œì´ì…˜ ê³„ì¸µê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§/ì¸í”„ë¼ ê³„ì¸µ ê°„ì˜ ì¸í„°í˜ì´ìŠ¤ ì—­í• ì„ í•©ë‹ˆë‹¤.
    """

    def __init__(self, config_file_path: Optional[Union[str, Path]] = None):
        self.config_manager = ConfigManager(config_file_path)
        self.config: Dict[str, Any] = {}
        self.gemini_client: Optional[GeminiClient] = None
        self.translation_service: Optional[TranslationService] = None
        self.glossary_service: Optional[SimpleGlossaryService] = None # Renamed from pronoun_service
        self.chunk_service = ChunkService()

        self.is_translation_running = False
        self.stop_requested = False
        self._translation_lock = threading.Lock()
        self._progress_lock = threading.Lock()
        # íŒŒì¼ ì“°ê¸° ë™ê¸°í™” (ìŠ¤ë ˆë“œ ì„¸ì´í”„)
        self._file_write_lock = threading.Lock()
        self.processed_chunks_count = 0
        self.successful_chunks_count = 0
        self.failed_chunks_count = 0
        self.post_processing_service = PostProcessingService()
        self.quality_check_service = QualityCheckService()

        self.load_app_config()

    def load_app_config(self, runtime_overrides: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        logger.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ë¡œë“œ ì¤‘...")

        try:
            # 1. íŒŒì¼ ë° ê¸°ë³¸ê°’ìœ¼ë¡œë¶€í„° ê¸°ë³¸ ì„¤ì • ë¡œë“œ
            config_from_manager = self.config_manager.load_config()
            self.config = config_from_manager # íŒŒì¼/ê¸°ë³¸ê°’ìœ¼ë¡œ ì‹œì‘

            # 2. ì œê³µëœ runtime_overridesê°€ ìˆë‹¤ë©´, self.configì— ë®ì–´ì“°ê¸°
            if runtime_overrides:
                self.config.update(runtime_overrides)
                logger.info(f"ëŸ°íƒ€ì„ ì˜¤ë²„ë¼ì´ë“œ ì ìš©: {list(runtime_overrides.keys())}")
            logger.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ë¡œë“œ ì™„ë£Œ.")

            auth_credentials_for_gemini_client: Optional[Union[str, List[str], Dict[str, Any]]] = None
            use_vertex = self.config.get("use_vertex_ai", False)
            gcp_project_from_config = self.config.get("gcp_project")
            gcp_location = self.config.get("gcp_location")
            sa_file_path_str = self.config.get("service_account_file_path")

            # ì„¤ì • ìš”ì•½ ë¡œê¹… (ì¡°ê±´ë¶€)
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"ì„¤ì • ìš”ì•½: vertex={use_vertex}, project={gcp_project_from_config}, location={gcp_location}")

            if use_vertex:
                logger.info("Vertex AI ì‚¬ìš© ëª¨ë“œë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
                # Vertex AI ëª¨ë“œì—ì„œëŠ” auth_credentials_for_gemini_clientê°€ SA JSON ë¬¸ìì—´, SA Dict, ë˜ëŠ” None (ADCìš©)ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                if sa_file_path_str:
                    sa_file_path = Path(sa_file_path_str)
                    if sa_file_path.is_file():
                        try:
                            auth_credentials_for_gemini_client = read_text_file(sa_file_path)
                            logger.info(f"Vertex AI SA íŒŒì¼ì—ì„œ ì¸ì¦ ì •ë³´ ë¡œë“œë¨: {sa_file_path.name}")
                        except Exception as e:
                            logger.error(f"Vertex AI SA íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
                            auth_credentials_for_gemini_client = None
                    else:
                        logger.warning(f"Vertex AI SA íŒŒì¼ ê²½ë¡œ ë¬´íš¨: {sa_file_path_str}")
                        auth_conf_val = self.config.get("auth_credentials")
                        if isinstance(auth_conf_val, (str, dict)) and auth_conf_val:
                            auth_credentials_for_gemini_client = auth_conf_val
                            logger.info("auth_credentials ê°’ì„ ëŒ€ì²´ ì‚¬ìš©")
                        else:
                            auth_credentials_for_gemini_client = None
                            logger.info("ADC ì‚¬ìš© ì˜ˆì •")
                elif self.config.get("auth_credentials"):
                    auth_conf_val = self.config.get("auth_credentials")
                    if isinstance(auth_conf_val, (str, dict)) and auth_conf_val:
                        auth_credentials_for_gemini_client = auth_conf_val
                        logger.info("Vertex AI: auth_credentials ê°’ ì‚¬ìš©")
                    else:
                        auth_credentials_for_gemini_client = None
                        logger.info("Vertex AI: ADC ì‚¬ìš© ì˜ˆì •")
                else:
                    auth_credentials_for_gemini_client = None
                    logger.info("Vertex AI: ADC ì‚¬ìš©")
            else:
                logger.info("Gemini Developer API ëª¨ë“œ")
                auth_credentials_for_gemini_client = None

                api_keys_list_val = self.config.get("api_keys", [])
                if isinstance(api_keys_list_val, list):
                    valid_api_keys = [key for key in api_keys_list_val if isinstance(key, str) and key.strip()]
                    if valid_api_keys:
                        auth_credentials_for_gemini_client = valid_api_keys
                        logger.info(f"API í‚¤ {len(valid_api_keys)}ê°œ ì‚¬ìš©")
                
                if auth_credentials_for_gemini_client is None:
                    api_key_val = self.config.get("api_key")
                    if isinstance(api_key_val, str) and api_key_val.strip():
                        auth_credentials_for_gemini_client = api_key_val
                        logger.info("ë‹¨ì¼ API í‚¤ ì‚¬ìš©")

                if auth_credentials_for_gemini_client is None:
                    auth_credentials_conf_val = self.config.get("auth_credentials")
                    if isinstance(auth_credentials_conf_val, str) and auth_credentials_conf_val.strip():
                        auth_credentials_for_gemini_client = auth_credentials_conf_val
                        logger.info("auth_credentials ë¬¸ìì—´ ì‚¬ìš©")
                    elif isinstance(auth_credentials_conf_val, list):
                        valid_keys_from_auth_cred = [k for k in auth_credentials_conf_val if isinstance(k, str) and k.strip()]
                        if valid_keys_from_auth_cred:
                            auth_credentials_for_gemini_client = valid_keys_from_auth_cred
                            logger.info(f"auth_credentialsì—ì„œ API í‚¤ {len(valid_keys_from_auth_cred)}ê°œ ì‚¬ìš©")
                    elif isinstance(auth_credentials_conf_val, dict):
                        auth_credentials_for_gemini_client = auth_credentials_conf_val
                        logger.info("auth_credentials SA dict ì‚¬ìš©")

                if auth_credentials_for_gemini_client is None:
                    logger.warning("API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")

            should_initialize_client = False
            if auth_credentials_for_gemini_client:
                if isinstance(auth_credentials_for_gemini_client, str) and auth_credentials_for_gemini_client.strip():
                    should_initialize_client = True
                elif isinstance(auth_credentials_for_gemini_client, list) and auth_credentials_for_gemini_client:
                    should_initialize_client = True
                elif isinstance(auth_credentials_for_gemini_client, dict):
                    should_initialize_client = True
            elif use_vertex and not auth_credentials_for_gemini_client and \
                 (gcp_project_from_config or os.environ.get("GOOGLE_CLOUD_PROJECT")):
                should_initialize_client = True
                logger.info("Vertex AI ADC ëª¨ë“œë¡œ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜ˆì •")

            if should_initialize_client:
                try:
                    project_to_pass_to_client = gcp_project_from_config if gcp_project_from_config and gcp_project_from_config.strip() else None
                    rpm_value = self.config.get("requests_per_minute")
                    logger.info(f"GeminiClient ì´ˆê¸°í™”: project={project_to_pass_to_client}, RPM={rpm_value}")
                    self.gemini_client = GeminiClient(
                        auth_credentials=auth_credentials_for_gemini_client,
                        project=project_to_pass_to_client,
                        location=gcp_location,
                        requests_per_minute=rpm_value
                    )
                except GeminiInvalidRequestException as e_inv:
                    logger.error(f"GeminiClient ì´ˆê¸°í™” ì‹¤íŒ¨: {e_inv}")
                    self.gemini_client = None
                except Exception as e_client:
                    logger.error(f"GeminiClient ì´ˆê¸°í™” ì˜¤ë¥˜: {e_client}", exc_info=True)
                    self.gemini_client = None
            else:
                logger.warning("API í‚¤ ë˜ëŠ” Vertex AI ì„¤ì •ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”ë¥¼ ì‹œë„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                self.gemini_client = None

            if self.gemini_client:
                self.translation_service = TranslationService(self.gemini_client, self.config)
                self.glossary_service = SimpleGlossaryService(self.gemini_client, self.config) # Changed to SimpleGlossaryService
                logger.info("TranslationService ë° SimpleGlossaryServiceê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.") # Message updated
            else:
                self.translation_service = None
                self.glossary_service = None # Renamed
                logger.warning("Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ë²ˆì—­ ë° ê³ ìœ ëª…ì‚¬ ì„œë¹„ìŠ¤ê°€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

            return self.config
        except FileNotFoundError as e:
            logger.error(f"ì„¤ì • íŒŒì¼ ì°¾ê¸° ì‹¤íŒ¨: {e}")
            self.config = self.config_manager.get_default_config()
            logger.warning("ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤. Gemini í´ë¼ì´ì–¸íŠ¸ëŠ” ì´ˆê¸°í™”ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            self.gemini_client = None
            self.translation_service = None # Keep
            self.glossary_service = None # Renamed
            return self.config
        except Exception as e:
            logger.error(f"ì„¤ì • ë¡œë“œ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            raise BtgConfigException(f"ì„¤ì • ë¡œë“œ ì˜¤ë¥˜: {e}", original_exception=e) from e

    def save_app_config(self, config_data: Dict[str, Any]) -> bool:
        logger.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ì €ì¥ ì¤‘...")
        try:
            success = self.config_manager.save_config(config_data)
            if success:
                logger.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ì €ì¥ ì™„ë£Œ.")
                # ì €ì¥ í›„ì—ëŠ” íŒŒì¼ì—ì„œ ìµœì‹  ì„¤ì •ì„ ë¡œë“œí•˜ë¯€ë¡œ runtime_overrides ì—†ì´ í˜¸ì¶œ
                # config_dataê°€ ìµœì‹  ìƒíƒœì´ë¯€ë¡œ, ì´ë¥¼ self.configì— ë°˜ì˜í•˜ê³  í´ë¼ì´ì–¸íŠ¸ë¥¼ ì¬ì„¤ì •í•  ìˆ˜ë„ ìˆì§€ë§Œ,
                # load_app_config()ë¥¼ í˜¸ì¶œí•˜ì—¬ ì¼ê´€ëœ ë¡œì§ì„ ë”°ë¥´ëŠ” ê²ƒì´ ë” ê°„ë‹¨í•©ë‹ˆë‹¤.
                self.load_app_config() # runtime_overrides=None (ê¸°ë³¸ê°’)
            return success
        except Exception as e:
            logger.error(f"ì„¤ì • ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise BtgConfigException(f"ì„¤ì • ì €ì¥ ì˜¤ë¥˜: {e}", original_exception=e) from e

    def get_available_models(self) -> List[Dict[str, Any]]:
        if not self.gemini_client:
            logger.error("ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            raise BtgServiceException("Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ ë˜ëŠ” Vertex AI ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        logger.info("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì„œë¹„ìŠ¤ í˜¸ì¶œë¨.")
        try:
            all_models = self.gemini_client.list_models()
            # ëª¨ë¸ í•„í„°ë§ ë¡œì§ ì œê±°ë¨
            logger.info(f"ì´ {len(all_models)}ê°œì˜ ëª¨ë¸ì„ APIë¡œë¶€í„° ì§ì ‘ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return all_models
            
        except BtgApiClientException as e:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ API ì˜¤ë¥˜: {e}")
            raise
        except Exception as e:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True) # type: ignore
            raise BtgServiceException(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}", original_exception=e) from e

    def extract_glossary(
        self,
        input_file_path: Union[str, Path],
        progress_callback: Optional[Callable[[GlossaryExtractionProgressDTO], None]] = None, # DTO Changed
        novel_language_code: Optional[str] = None, # ëª…ì‹œì  ì–¸ì–´ ì½”ë“œ ì „ë‹¬
        seed_glossary_path: Optional[Union[str, Path]] = None, # CLIì—ì„œ ì „ë‹¬ëœ ì‹œë“œ ìš©ì–´ì§‘ ê²½ë¡œ
        user_override_glossary_extraction_prompt: Optional[str] = None, # ì‚¬ìš©ì ì¬ì •ì˜ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
        stop_check: Optional[Callable[[], bool]] = None
    ) -> Path:
        if not self.glossary_service: # Changed from pronoun_service
            logger.error("ìš©ì–´ì§‘ ì¶”ì¶œ ì„œë¹„ìŠ¤ ì‹¤íŒ¨: ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.") # Message updated
            raise BtgServiceException("ìš©ì–´ì§‘ ì¶”ì¶œ ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.") # Message updated
        
        logger.info(f"ìš©ì–´ì§‘ ì¶”ì¶œ ì„œë¹„ìŠ¤ ì‹œì‘: {input_file_path}, ì‹œë“œ íŒŒì¼: {seed_glossary_path}")  
        try:
            file_content = read_text_file(input_file_path)
            if not file_content:
                logger.warning(f"ì…ë ¥ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {input_file_path}")
                # For lorebook, an empty input means an empty lorebook, unless a seed is provided.
                # SimpleGlossaryService.extract_and_save_lorebook handles empty content.
        
            # ë¡œì–´ë¶ ì¶”ì¶œ ì‹œ ì‚¬ìš©í•  ì–¸ì–´ ì½”ë“œ ê²°ì •
            # 1. ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬ëœ novel_language_code
            # 2. ì„¤ì • íŒŒì¼ì˜ novel_language (í†µí•©ë¨)
            # 3. None (SimpleGlossaryServiceì—ì„œ ìì²´ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê±°ë‚˜ ì–¸ì–´ íŠ¹ì • ê¸°ëŠ¥ ë¹„í™œì„±í™”)
            lang_code_for_extraction = novel_language_code or self.config.get("novel_language") # í†µí•©ëœ ì„¤ì • ì‚¬ìš©

            # ì‚¬ìš©í•  í”„ë¡¬í”„íŠ¸ ê²°ì •: ë©”ì„œë“œ ì¸ìë¡œ ì „ë‹¬ëœ ê²ƒì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©, ì—†ìœ¼ë©´ ì„¤ì • íŒŒì¼ ê°’ ì‚¬ìš©
            prompt_to_use = user_override_glossary_extraction_prompt \
                if user_override_glossary_extraction_prompt is not None \
                else self.config.get("user_override_glossary_extraction_prompt")

            # lang_code_for_extractionì€ SimpleGlossaryService.extract_and_save_glossaryì—ì„œ ì§ì ‘ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ.
            result_path = self.glossary_service.extract_and_save_glossary( # type: ignore
                novel_text_content=file_content,
                input_file_path_for_naming=input_file_path,
                progress_callback=progress_callback,
                seed_glossary_path=seed_glossary_path, # ì‹œë“œ ìš©ì–´ì§‘ ê²½ë¡œ ì „ë‹¬
                user_override_glossary_extraction_prompt=prompt_to_use, # ê²°ì •ëœ í”„ë¡¬í”„íŠ¸ ì „ë‹¬
                stop_check=stop_check
            )
            logger.info(f"ìš©ì–´ì§‘ ì¶”ì¶œ ì™„ë£Œ. ê²°ê³¼ íŒŒì¼: {result_path}") # Message updated
        
            return result_path
        except FileNotFoundError as e:
            logger.error(f"ìš©ì–´ì§‘ ì¶”ì¶œì„ ìœ„í•œ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file_path}") # Message updated
            if progress_callback:
                progress_callback(GlossaryExtractionProgressDTO(0,0,f"ì˜¤ë¥˜: ì…ë ¥ íŒŒì¼ ì—†ìŒ - {e.filename}",0)) # DTO Changed
            raise BtgFileHandlerException(f"ì…ë ¥ íŒŒì¼ ì—†ìŒ: {input_file_path}", original_exception=e) from e
        except (BtgBusinessLogicException, BtgApiClientException) as e: # BtgPronounException replaced with BtgBusinessLogicException
            logger.error(f"ìš©ì–´ì§‘ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}") # Message updated
            if progress_callback:
                progress_callback(GlossaryExtractionProgressDTO(0,0,f"ì˜¤ë¥˜: {e}",0)) # DTO Changed
            raise
        except Exception as e: 
            logger.error(f"ìš©ì–´ì§‘ ì¶”ì¶œ ì„œë¹„ìŠ¤ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)  # Message updated
            if progress_callback:
                progress_callback(GlossaryExtractionProgressDTO(0,0,f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}",0)) # DTO Changed
            raise BtgServiceException(f"ìš©ì–´ì§‘ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}", original_exception=e) from e # Message updated


    def _translate_and_save_chunk(self, chunk_index: int, chunk_text: str,
                            chunked_output_file: Path,
                            total_chunks: int,
                            input_file_path_for_metadata: Path,
                            progress_callback: Optional[Callable[[TranslationJobProgressDTO], None]] = None) -> bool:
        current_chunk_info_msg = f"ì²­í¬ {chunk_index}/{total_chunks}"
        
        # ì²­í¬ ë¶„ì„ (ë¡œê¹… ìµœì í™”: í†µê³„ëŠ” DEBUG ë ˆë²¨ì—ì„œë§Œ ìƒì„¸ ì¶œë ¥)
        chunk_chars = len(chunk_text)
        start_time = time.time()
        
        # í†µí•© ë¡œê·¸: ì‹œì‘ ì •ë³´ì™€ ê¸°ë³¸ í†µê³„ë¥¼ í•œ ì¤„ë¡œ
        logger.info(f"{current_chunk_info_msg} ì²˜ë¦¬ ì‹œì‘ (ê¸¸ì´: {chunk_chars}ì)")
        
        # ìƒì„¸ ì •ë³´ëŠ” DEBUG ë ˆë²¨ì—ì„œë§Œ ì¶œë ¥
        if logger.isEnabledFor(logging.DEBUG):
            chunk_lines = chunk_text.count('\n') + 1
            chunk_words = len(chunk_text.split())
            chunk_preview = chunk_text[:100].replace('\n', ' ') + '...' if len(chunk_text) > 100 else chunk_text
            logger.debug(f"  ğŸ“ ë¯¸ë¦¬ë³´ê¸°: {chunk_preview}")
            logger.debug(f"  ğŸ“Š í†µê³„: ê¸€ì={chunk_chars}, ë‹¨ì–´={chunk_words}, ì¤„={chunk_lines}")
        last_error = None
        success = False

        

        try:
            if self.stop_requested:
                logger.info(f"{current_chunk_info_msg} â¸ï¸ ì²˜ë¦¬ ì¤‘ì§€ë¨ (ì‚¬ìš©ì ìš”ì²­)")
                return False

            if not self.translation_service:
                raise BtgServiceException("TranslationServiceê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            # ë²ˆì—­ ì„¤ì • ë¡œë“œ
            use_content_safety_retry = self.config.get("use_content_safety_retry", True)
            max_split_attempts = self.config.get("max_content_safety_split_attempts", 3)
            min_chunk_size = self.config.get("min_content_safety_chunk_size", 100)
            model_name = self.config.get("model_name", "gemini-2.0-flash")
            
            # ë²ˆì—­ ì„¤ì • ìƒì„¸ëŠ” DEBUGì—ì„œë§Œ ì¶œë ¥
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"  âš™ï¸ ì„¤ì •: ëª¨ë¸={model_name}, ì•ˆì „ì¬ì‹œë„={use_content_safety_retry}, ìµœëŒ€ì‹œë„={max_split_attempts}")
            
            translation_start_time = time.time()
                
            if use_content_safety_retry:
                translated_chunk = self.translation_service.translate_text_with_content_safety_retry(
                    chunk_text, max_split_attempts, min_chunk_size
                )
            else:
                translated_chunk = self.translation_service.translate_text(chunk_text)
            
            translation_time = time.time() - translation_start_time
            translated_length = len(translated_chunk) if translated_chunk else 0
            
            if self.stop_requested:
                logger.warning(f"  âš ï¸ {current_chunk_info_msg} ë²ˆì—­ ì™„ë£Œë˜ì—ˆìœ¼ë‚˜ ì¤‘ì§€ë¨ - ê²°ê³¼ íê¸°")
                return False

            # ìŠ¤ë ˆë“œ ì„¸ì´í”„í•˜ê²Œ ì²­í¬ íŒŒì¼ì— ì§ì ‘ ê¸°ë¡
            with self._file_write_lock:
                save_chunk_with_index_to_file(chunked_output_file, chunk_index, translated_chunk)
            
            # ë²ˆì—­ ì„±ëŠ¥ ìƒì„¸ëŠ” DEBUGì—ì„œë§Œ
            if logger.isEnabledFor(logging.DEBUG):
                speed = chunk_chars / translation_time if translation_time > 0 else 0
                logger.debug(f"  âœ… ë²ˆì—­ì™„ë£Œ: {translated_length}ì, {translation_time:.2f}ì´ˆ, {speed:.0f}ì/ì´ˆ")
            
            success = True
            
            ratio = len(translated_chunk) / len(chunk_text) if len(chunk_text) > 0 else 0.0
            total_processing_time = time.time() - start_time
            logger.info(f"  ğŸ¯ {current_chunk_info_msg} ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ (ì´ ì†Œìš”: {total_processing_time:.2f}ì´ˆ, ê¸¸ì´ë¹„ìœ¨: {ratio:.2f})")

        except BtgTranslationException as e_trans:
            if self.stop_requested:
                logger.warning(f"  âš ï¸ {current_chunk_info_msg} ì¤‘ì§€ë¨ - ë²ˆì—­ ì˜ˆì™¸ ë¬´ì‹œ")
                return False
            processing_time = time.time() - start_time
            error_type = "ì½˜í…ì¸  ê²€ì—´" if "ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œ" in str(e_trans) else "ë²ˆì—­ ì„œë¹„ìŠ¤"
            logger.error(f"  âŒ {current_chunk_info_msg} ì‹¤íŒ¨: {error_type} - {e_trans} ({processing_time:.2f}ì´ˆ)")
            
            with self._file_write_lock:
                save_chunk_with_index_to_file(chunked_output_file, chunk_index, f"[ë²ˆì—­ ì‹¤íŒ¨: {e_trans}]\n\n--- ì›ë¬¸ ë‚´ìš© ---\n{chunk_text}")
            last_error = str(e_trans)
            success = False

        except BtgApiClientException as e_api:
            if self.stop_requested:
                logger.warning(f"  âš ï¸ {current_chunk_info_msg} ì¤‘ì§€ë¨ - API ì˜ˆì™¸ ë¬´ì‹œ")
                return False
            processing_time = time.time() - start_time
            # API ì˜¤ë¥˜ ìœ í˜• íŒë³„
            error_detail = ""
            if "ì‚¬ìš©ëŸ‰ ì œí•œ" in str(e_api) or "429" in str(e_api):
                error_detail = " [ì‚¬ìš©ëŸ‰ ì œí•œ]"
            elif "í‚¤" in str(e_api).lower() or "ì¸ì¦" in str(e_api):
                error_detail = " [ì¸ì¦ ì˜¤ë¥˜]"
            logger.error(f"  âŒ {current_chunk_info_msg} API ì˜¤ë¥˜{error_detail}: {e_api} ({processing_time:.2f}ì´ˆ)")
            
            with self._file_write_lock:
                save_chunk_with_index_to_file(chunked_output_file, chunk_index, f"[API ì˜¤ë¥˜ë¡œ ë²ˆì—­ ì‹¤íŒ¨: {e_api}]\n\n--- ì›ë¬¸ ë‚´ìš© ---\n{chunk_text}")
            last_error = str(e_api)
            success = False

        except Exception as e_gen:
            if self.stop_requested:
                logger.warning(f"  âš ï¸ {current_chunk_info_msg} ì¤‘ì§€ë¨ - ì˜ˆì™¸ ë¬´ì‹œ")
                return False
            processing_time = time.time() - start_time
            logger.error(f"  âŒ {current_chunk_info_msg} ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {type(e_gen).__name__} - {e_gen} ({processing_time:.2f}ì´ˆ)", exc_info=True)
            
            with self._file_write_lock:
                save_chunk_with_index_to_file(chunked_output_file, chunk_index, f"[ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ë¡œ ë²ˆì—­ ì‹¤íŒ¨: {e_gen}]\n\n--- ì›ë¬¸ ë‚´ìš© ---\n{chunk_text}")
            last_error = str(e_gen)
            success = False
                    

        
        finally:
            total_time = time.time() - start_time
            with self._progress_lock:
                # ì‹œìŠ¤í…œì´ ì¤‘ì§€ ìƒíƒœê°€ ì•„ë‹ˆë¼ë©´, ì§„í–‰ ìƒí™©ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
                # ì´ ê²€ì‚¬ë¥¼ _progress_lock ì•ˆì—ì„œ ìˆ˜í–‰í•˜ì—¬, í”Œë˜ê·¸ í™•ì¸ê³¼ ì¹´ìš´í„° ì—…ë°ì´íŠ¸ ì‚¬ì´ì˜
                # ê²½ìŸ ì¡°ê±´ì„ ì™„ë²½í•˜ê²Œ ë°©ì§€í•©ë‹ˆë‹¤. ì´ê²ƒì´ ë°ì´í„° ì •í•©ì„±ì„ ë³´ì¥í•˜ëŠ” ìµœì¢… ë°©ì–´ì„ ì…ë‹ˆë‹¤.
                if not self.stop_requested:
                    # 1ë‹¨ê³„: ë¨¼ì € processed_chunks_count ì¦ê°€
                    self.processed_chunks_count += 1
                    # 2ë‹¨ê³„: ê²°ê³¼ì— ë”°ë¼ ì„±ê³µ/ì‹¤íŒ¨ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
                    if success:
                        self.successful_chunks_count += 1
                        # âœ… ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸: translated_chunksì— ì™„ë£Œëœ ì²­í¬ ê¸°ë¡
                        try:
                            metadata_updated = update_metadata_for_chunk_completion(
                                input_file_path_for_metadata, 
                                chunk_index,
                                source_length=len(chunk_text),
                                translated_length=len(translated_chunk)
                            )
                            if metadata_updated:
                                logger.debug(f"  ğŸ’¾ {current_chunk_info_msg} ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                            else:
                                logger.warning(f"  âš ï¸ {current_chunk_info_msg} ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
                        except Exception as meta_e:
                            logger.error(f"  âŒ {current_chunk_info_msg} ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {meta_e}")
                    else: # 'success'ê°€ Falseì¸ ê²½ìš°, ì‹¤íŒ¨ ì¹´ìš´í„°ë¥¼ ì¦ê°€ì‹œí‚µë‹ˆë‹¤.
                        self.failed_chunks_count += 1
                        #  ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ ê¸°ë¡
                        if last_error:
                            try:
                                update_metadata_for_chunk_failure(input_file_path_for_metadata, chunk_index, last_error)
                                logger.debug(f"  ğŸ’¾ {current_chunk_info_msg} ì‹¤íŒ¨ ì •ë³´ ë©”íƒ€ë°ì´í„°ì— ê¸°ë¡ ì™„ë£Œ")
                            except Exception as meta_fail_e:
                                logger.error(f"  âŒ {current_chunk_info_msg} ì‹¤íŒ¨ ì •ë³´ ë©”íƒ€ë°ì´í„° ê¸°ë¡ ì¤‘ ì˜¤ë¥˜: {meta_fail_e}")
                    
                    # 3ë‹¨ê³„: ì§„í–‰ë¥  ê³„ì‚° ë° í†µí•© ë¡œê¹… (2ê°œ ë¡œê·¸ â†’ 1ê°œ)
                    progress_percentage = (self.processed_chunks_count / total_chunks) * 100
                    success_rate = (self.successful_chunks_count / self.processed_chunks_count) * 100 if self.processed_chunks_count > 0 else 0
                    
                    # ë§¤ 10% ë˜ëŠ” ë§ˆì§€ë§‰ ì²­í¬ì—ì„œë§Œ ìƒì„¸ ë¡œê·¸ ì¶œë ¥ (ë¡œê·¸ ë¹ˆë„ ìµœì í™”)
                    should_log_progress = (self.processed_chunks_count % max(1, total_chunks // 10) == 0) or (self.processed_chunks_count == total_chunks)
                    if should_log_progress:
                        logger.info(f"  ğŸ“ˆ ì§„í–‰ë¥ : {progress_percentage:.0f}% ({self.processed_chunks_count}/{total_chunks}) | ì„±ê³µë¥ : {success_rate:.0f}% (âœ…{self.successful_chunks_count} âŒ{self.failed_chunks_count})")


                    if progress_callback:
                        if success:
                            status_msg_for_dto = f"âœ… ì²­í¬ {chunk_index + 1}/{total_chunks} ì™„ë£Œ ({total_time:.1f}ì´ˆ)"
                        else:
                            status_msg_for_dto = f"âŒ ì²­í¬ {chunk_index + 1}/{total_chunks} ì‹¤íŒ¨ ({total_time:.1f}ì´ˆ)"
                            if last_error:
                                status_msg_for_dto += f" - {last_error[:50]}..."

                        progress_dto = TranslationJobProgressDTO(
                            total_chunks=total_chunks,
                            processed_chunks=self.processed_chunks_count,
                            successful_chunks=self.successful_chunks_count,
                            failed_chunks=self.failed_chunks_count,
                            current_status_message=status_msg_for_dto,
                            current_chunk_processing=chunk_index + 1,
                            last_error_message=last_error
                        )
                        progress_callback(progress_dto)
                else:
                    # stop_requestedê°€ Trueì´ë©´, ì•„ë¬´ ì‘ì—…ë„ ìˆ˜í–‰í•˜ì§€ ì•Šê³  ì¡°ìš©íˆ ì¢…ë£Œí•©ë‹ˆë‹¤.
                    # ì´ ì¢€ë¹„ ìŠ¤ë ˆë“œì˜ ê²°ê³¼ëŠ” ë²„ë ¤ì§€ë©°, ì–´ë–¤ ê³µìœ  ìƒíƒœë„ ì˜¤ì—¼ì‹œí‚¤ì§€ ì•ŠìŠµë‹ˆë‹¤.
                    logger.warning(f"  âš ï¸ {current_chunk_info_msg}ì˜ ìµœì¢… ì²˜ë¦¬(ì§„í–‰ë¥ , ë©”íƒ€ë°ì´í„°)ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤ (ì‹œìŠ¤í…œ ì¤‘ì§€ë¨).")
            
            logger.debug(f"  {current_chunk_info_msg} ì²˜ë¦¬ ì™„ë£Œ ë°˜í™˜: {success}")
            return success



    def start_translation(
        self,
        input_file_path: Union[str, Path],
        output_file_path: Union[str, Path],
        progress_callback: Optional[Callable[[TranslationJobProgressDTO], None]] = None,
        status_callback: Optional[Callable[[str], None]] = None,
        tqdm_file_stream: Optional[Any] = None,
        blocking: bool = False, # blocking ë§¤ê°œë³€ìˆ˜ ì¶”ê°€
        retranslate_failed_only: bool = False # ì‹¤íŒ¨ ì²­í¬ ì¬ë²ˆì—­ ëª¨ë“œ
    ) -> None:
        # === ìš©ì–´ì§‘ ë™ì  ë¡œë”© ë¡œì§ ì¶”ê°€ ===
        try:
            input_p = Path(input_file_path)
            # ì„¤ì •ì—ì„œ ìš©ì–´ì§‘ íŒŒì¼ ì ‘ë¯¸ì‚¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
            glossary_suffix = self.config.get("glossary_output_json_filename_suffix", "_simple_glossary.json")
            # í˜„ì¬ ì…ë ¥ íŒŒì¼ì— í•´ë‹¹í•˜ëŠ” ìš©ì–´ì§‘ íŒŒì¼ ê²½ë¡œë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.
            assumed_glossary_path = input_p.parent / f"{input_p.stem}{glossary_suffix}"

            # configì— ì„¤ì •ëœ ê²½ë¡œì™€ ì¶”ì •ëœ ê²½ë¡œë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
            # 1. ì¶”ì •ëœ ê²½ë¡œì— íŒŒì¼ì´ ì¡´ì¬í•˜ë©´, í•´ë‹¹ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •ì„ ë®ì–´ì”ë‹ˆë‹¤.
            # 2. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´, configì— ëª…ì‹œëœ ê²½ë¡œ(ì‚¬ìš©ìê°€ ìˆ˜ë™ìœ¼ë¡œ ì§€ì •í•œ ê²½ë¡œ)ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
            
            glossary_to_use = None
            if assumed_glossary_path.exists():
                glossary_to_use = str(assumed_glossary_path)
                logger.info(f"'{input_p.name}'ì— ëŒ€í•œ ìš©ì–´ì§‘ '{assumed_glossary_path.name}'ì„(ë¥¼) ìë™ìœ¼ë¡œ ë°œê²¬í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            else:
                # ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •ëœ ê²½ë¡œê°€ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
                manual_path = self.config.get("glossary_json_path")
                if manual_path and Path(manual_path).exists():
                    glossary_to_use = manual_path
                    logger.info(f"ìë™ìœ¼ë¡œ ë°œê²¬ëœ ìš©ì–´ì§‘ì´ ì—†ì–´, ì„¤ì •ëœ ê²½ë¡œ '{manual_path}'ì˜ ìš©ì–´ì§‘ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                else:
                    logger.info(f"'{input_p.name}'ì— ëŒ€í•œ ìš©ì–´ì§‘ì„ ì°¾ì„ ìˆ˜ ì—†ì–´, ìš©ì–´ì§‘ ì—†ì´ ë²ˆì—­ì„ ì§„í–‰í•©ë‹ˆë‹¤.")

            # ì°¾ì€ ìš©ì–´ì§‘ ê²½ë¡œë¥¼ ëŸ°íƒ€ì„ ì„¤ì •ì— ë°˜ì˜í•˜ì—¬ TranslationServiceê°€ ë¡œë“œí•˜ë„ë¡ í•©ë‹ˆë‹¤.
            if self.translation_service:
                # TranslationServiceê°€ ìƒˆë¡œìš´ ì„¤ì •ì„ ê¸°ë°˜ìœ¼ë¡œ ìš©ì–´ì§‘ì„ ë‹¤ì‹œ ë¡œë“œí•˜ë„ë¡ í•©ë‹ˆë‹¤.
                self.config['glossary_json_path'] = glossary_to_use
                self.translation_service.config = self.config
                self.translation_service._load_glossary_data() # TranslationService ë‚´ë¶€ì˜ ìš©ì–´ì§‘ ë°ì´í„°ë¥¼ ê°±ì‹ í•©ë‹ˆë‹¤.

        except Exception as e:
            logger.error(f"ìš©ì–´ì§‘ ë™ì  ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            # ìš©ì–´ì§‘ ë¡œë”©ì— ì‹¤íŒ¨í•´ë„ ë²ˆì—­ì€ ê³„ì† ì§„í–‰í•˜ë„ë¡ í•©ë‹ˆë‹¤.
        # === ìš©ì–´ì§‘ ë™ì  ë¡œë”© ë¡œì§ ë ===
        
        # ìŠ¤ë ˆë“œ ìƒì„± ë° ì‹œì‘ ë¶€ë¶„ ìˆ˜ì •
        thread = threading.Thread(
            target=self._translation_task, # ì‹¤ì œ ë²ˆì—­ ë¡œì§ì„ ë³„ë„ ë©”ì„œë“œë¡œ ë¶„ë¦¬
            args=(input_file_path, output_file_path, progress_callback, status_callback, tqdm_file_stream, retranslate_failed_only),
            daemon=not blocking # blocking ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ ë°ëª¬ ìŠ¤ë ˆë“œë¡œ ì„¤ì •
        )
        thread.start()

        if blocking:
            thread.join() # blocking ëª¨ë“œì¼ ê²½ìš° ìŠ¤ë ˆë“œê°€ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°


    def _translation_task( # start_translationì˜ ìŠ¤ë ˆë“œ ì‹¤í–‰ ë¡œì§ì„ ì´ ë©”ì„œë“œë¡œ ì´ë™
        self,
        input_file_path: Union[str, Path],
        output_file_path: Union[str, Path],
        progress_callback: Optional[Callable[[TranslationJobProgressDTO], None]] = None,
        status_callback: Optional[Callable[[str], None]] = None,
        tqdm_file_stream: Optional[Any] = None,
        retranslate_failed_only: bool = False
    ):
        if not self.translation_service or not self.chunk_service:
            logger.error("ë²ˆì—­ ì„œë¹„ìŠ¤ ì‹¤íŒ¨: ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            if status_callback: status_callback("ì˜¤ë¥˜: ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨")
            raise BtgServiceException("ë²ˆì—­ ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")

        with self._translation_lock:
            if self.is_translation_running:
                logger.warning("ë²ˆì—­ ì‘ì—…ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
                if status_callback: status_callback("ê²½ê³ : ë²ˆì—­ ì‘ì—… ì´ë¯¸ ì‹¤í–‰ ì¤‘")
                return
            self.is_translation_running = True
            self.stop_requested = False
            self.processed_chunks_count = 0
            self.successful_chunks_count = 0
            self.failed_chunks_count = 0

        logger.info(f"ë²ˆì—­ ì„œë¹„ìŠ¤ ì‹œì‘: ì…ë ¥={input_file_path}, ìµœì¢… ì¶œë ¥={output_file_path}")
        if status_callback: status_callback("ë²ˆì—­ ì‹œì‘ë¨...")

        # TranslationServiceì— ì¤‘ë‹¨ í™•ì¸ ì½œë°± ì„¤ì •
        if self.translation_service:
            self.translation_service.set_stop_check_callback(lambda: self.stop_requested)
            logger.debug("TranslationServiceì— ì¤‘ë‹¨ í™•ì¸ ì½œë°±ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")

        input_file_path_obj = Path(input_file_path)
        final_output_file_path_obj = Path(output_file_path)
        metadata_file_path = get_metadata_file_path(input_file_path_obj)
        loaded_metadata: Dict[str, Any] = {}
        resume_translation = False
        total_chunks = 0 

        try:
            with self._progress_lock: 
                if metadata_file_path.exists():
                    try:
                        loaded_metadata = load_metadata(metadata_file_path)
                        if loaded_metadata: logger.info(f"ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì„±ê³µ: {metadata_file_path}")
                        else: logger.warning(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ '{metadata_file_path}'ì´ ë¹„ì–´ìˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
                    except json.JSONDecodeError as json_err:
                        logger.error(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ '{metadata_file_path}' ì†ìƒ (JSONDecodeError): {json_err}. ìƒˆë¡œ ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                        delete_file(metadata_file_path) 
                    except Exception as e: 
                        logger.error(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ '{metadata_file_path}' ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}. ìƒˆë¡œ ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤.", exc_info=True)
                        delete_file(metadata_file_path) 
                else:
                    logger.info(f"ê¸°ì¡´ ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {metadata_file_path}. ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")

            current_config_hash = _hash_config_for_metadata(self.config)
            previous_config_hash = loaded_metadata.get("config_hash")

            if previous_config_hash and previous_config_hash == current_config_hash:
                resume_translation = True
                logger.info("ì„¤ì • í•´ì‹œê°€ ì¼ì¹˜í•˜ì—¬ ì´ì–´í•˜ê¸° ëª¨ë“œë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
            elif previous_config_hash: 
                logger.warning("í˜„ì¬ ì„¤ì •ì´ ì´ì „ ë©”íƒ€ë°ì´í„°ì˜ ì„¤ì •ê³¼ ë‹¤ë¦…ë‹ˆë‹¤. ìƒˆë¡œ ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                loaded_metadata = {} 
                resume_translation = False
            
            file_content = read_text_file(input_file_path_obj)
            if not file_content:
                logger.warning(f"ì…ë ¥ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {input_file_path_obj}")
                if status_callback: status_callback("ì™„ë£Œ: ì…ë ¥ íŒŒì¼ ë¹„ì–´ìˆìŒ")
                with self._progress_lock:
                    if not loaded_metadata.get("config_hash"): 
                         loaded_metadata = create_new_metadata(input_file_path_obj, 0, self.config)
                    loaded_metadata["status"] = "completed"; loaded_metadata["total_chunks"] = 0
                    loaded_metadata["last_updated"] = time.time()
                    save_metadata(metadata_file_path, loaded_metadata)
                if progress_callback: progress_callback(TranslationJobProgressDTO(0,0,0,0,"ì…ë ¥ íŒŒì¼ ë¹„ì–´ìˆìŒ"))
                with self._translation_lock: self.is_translation_running = False
                return

            all_chunks: List[str] = self.chunk_service.create_chunks_from_file_content(file_content, self.config.get("chunk_size", 6000))
            total_chunks = len(all_chunks) 
            logger.info(f"ì´ {total_chunks}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë¨.")

            # ì„ì‹œ íŒŒì¼(current_run.tmp) ì œê±°, ì²­í¬ ë°±ì—… íŒŒì¼(.chunked.txt)ë§Œ ë‹¨ì¼ ì†ŒìŠ¤ë¡œ ì‚¬ìš©
            chunked_output_file_path = final_output_file_path_obj.with_suffix('.chunked.txt')
            with self._progress_lock: 

                if not resume_translation or not loaded_metadata.get("config_hash"): 
                    logger.info("ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ë®ì–´ì”ë‹ˆë‹¤ (ìƒˆë¡œ ì‹œì‘ ë˜ëŠ” ì„¤ì • ë³€ê²½).")
                    loaded_metadata = create_new_metadata(input_file_path_obj, total_chunks, self.config)
                    logger.info(f"ìƒˆë¡œ ë²ˆì—­ì„ ì‹œì‘í•˜ë¯€ë¡œ ìµœì¢… ì¶œë ¥ íŒŒì¼ '{final_output_file_path_obj}'ì„ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                    delete_file(final_output_file_path_obj) 
                    final_output_file_path_obj.touch() 
                    # ì²­í¬ ë°±ì—… íŒŒì¼ë„ ì´ˆê¸°í™”
                    delete_file(chunked_output_file_path)
                    chunked_output_file_path.touch()
                else: 
                    if loaded_metadata.get("total_chunks") != total_chunks:
                        logger.warning(f"ì…ë ¥ íŒŒì¼ì˜ ì²­í¬ ìˆ˜ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤ ({loaded_metadata.get('total_chunks')} -> {total_chunks}). ë©”íƒ€ë°ì´í„°ë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                        loaded_metadata = create_new_metadata(input_file_path_obj, total_chunks, self.config)
                        resume_translation = False 
                        logger.info(f"ì²­í¬ ìˆ˜ ë³€ê²½ìœ¼ë¡œ ì¸í•´ ìƒˆë¡œ ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤. ìµœì¢… ì¶œë ¥ íŒŒì¼ '{final_output_file_path_obj}'ì„ ë‹¤ì‹œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
                        delete_file(final_output_file_path_obj); final_output_file_path_obj.touch()
                        # ì²­í¬ ë°±ì—… íŒŒì¼ë„ ì´ˆê¸°í™”
                        delete_file(chunked_output_file_path); chunked_output_file_path.touch()
                    else:
                        logger.info(f"ì´ì–´í•˜ê¸° ëª¨ë“œ: ë©”íƒ€ë°ì´í„° ìƒíƒœë¥¼ 'in_progress'ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.")
                    loaded_metadata["status"] = "in_progress" 
                
                loaded_metadata["last_updated"] = time.time()
                save_metadata(metadata_file_path, loaded_metadata)

            # ì´ì–´í•˜ê¸° ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ, í˜¹ì‹œ ë§ˆì§€ë§‰ì— ë¶ˆì™„ì „í•œ ì²­í¬ê°€ ìˆë‹¤ë©´ ì •ë¦¬(ì •ê·œì‹ ë§¤ì¹­ ê°€ëŠ¥í•œ ì™„ì „ ì²­í¬ë§Œ ë³´ì¡´)
            try:
                if chunked_output_file_path.exists():
                    existing_chunks = load_chunks_from_file(chunked_output_file_path)
                    save_merged_chunks_to_file(chunked_output_file_path, existing_chunks)
                    logger.info("ì²­í¬ íŒŒì¼ì„ ìŠ¤ìº”í•˜ì—¬ ì™„ì „í•œ ì²­í¬ë§Œ ìœ ì§€í•˜ë„ë¡ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")
            except Exception as sanitize_e:
                logger.warning(f"ì²­í¬ íŒŒì¼ ì •ë¦¬ ì¤‘ ê²½ê³ : {sanitize_e}")

            chunks_to_process_with_indices: List[Tuple[int, str]] = []
            if retranslate_failed_only:
                if "failed_chunks" in loaded_metadata and loaded_metadata["failed_chunks"]:
                    failed_indices = {int(k) for k in loaded_metadata["failed_chunks"].keys()}
                    for i, chunk_text in enumerate(all_chunks):
                        if i in failed_indices:
                            chunks_to_process_with_indices.append((i, chunk_text))
                    logger.info(f"ì‹¤íŒ¨ ì²­í¬ ì¬ë²ˆì—­ ëª¨ë“œ: {len(chunks_to_process_with_indices)}ê°œ ëŒ€ìƒ.")
                else:
                    logger.info("ì‹¤íŒ¨í•œ ì²­í¬ê°€ ì—†ì–´ ì¬ë²ˆì—­ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            elif resume_translation and "translated_chunks" in loaded_metadata:
                with self._progress_lock:
                    previously_translated_indices = {int(k) for k in loaded_metadata.get("translated_chunks", {}).keys()}
                    self.successful_chunks_count = len(previously_translated_indices)
                    self.processed_chunks_count = self.successful_chunks_count 
                    self.failed_chunks_count = 0 
                for i, chunk_text in enumerate(all_chunks):
                    if i not in previously_translated_indices:
                        chunks_to_process_with_indices.append((i, chunk_text))
                logger.info(f"ì´ì–´í•˜ê¸°: {self.successful_chunks_count}ê°œ ì´ë¯¸ ì™„ë£Œ, {len(chunks_to_process_with_indices)}ê°œ ì¶”ê°€ ë²ˆì—­ ëŒ€ìƒ.")
            else: 
                chunks_to_process_with_indices = list(enumerate(all_chunks))
                logger.info(f"ìƒˆë¡œ ë²ˆì—­: {len(chunks_to_process_with_indices)}ê°œ ë²ˆì—­ ëŒ€ìƒ.")
                
            if not chunks_to_process_with_indices and total_chunks > 0 : 
                logger.info("ë²ˆì—­í•  ìƒˆë¡œìš´ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤ (ëª¨ë“  ì²­í¬ê°€ ì´ë¯¸ ë²ˆì—­ë¨).")
                if status_callback: status_callback("ì™„ë£Œ: ëª¨ë“  ì²­í¬ ì´ë¯¸ ë²ˆì—­ë¨")
                with self._progress_lock:
                    loaded_metadata["status"] = "completed"
                    loaded_metadata["last_updated"] = time.time()
                    save_metadata(metadata_file_path, loaded_metadata)
                if progress_callback:
                    progress_callback(TranslationJobProgressDTO(
                        total_chunks, self.processed_chunks_count, self.successful_chunks_count,
                        self.failed_chunks_count, "ëª¨ë“  ì²­í¬ ì´ë¯¸ ë²ˆì—­ë¨"
                    ))
                if status_callback: status_callback("ì™„ë£Œ: ëª¨ë“  ì²­í¬ ì´ë¯¸ ë²ˆì—­ë¨")
                with self._translation_lock: self.is_translation_running = False
                # current_run.tmpëŠ” ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                return

            initial_status_msg = "ë²ˆì—­ ì¤€ë¹„ ì¤‘..."
            if resume_translation: initial_status_msg = f"ì´ì–´í•˜ê¸° ì¤€ë¹„ (ë‚¨ì€ ì²­í¬: {len(chunks_to_process_with_indices)})"
            if progress_callback:
                with self._progress_lock:
                    progress_callback(TranslationJobProgressDTO(
                        total_chunks, self.processed_chunks_count, self.successful_chunks_count,
                        self.failed_chunks_count, initial_status_msg,
                        (chunks_to_process_with_indices[0][0] + 1 if chunks_to_process_with_indices else None) 
                    ))
            
            max_workers = self.config.get("max_workers", os.cpu_count() or 1)
            if not isinstance(max_workers, int) or max_workers <= 0:
                logger.warning(f"ì˜ëª»ëœ max_workers ê°’ ({max_workers}), ê¸°ë³¸ê°’ (CPU ì½”ì–´ ìˆ˜ ë˜ëŠ” 1)ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
                max_workers = os.cpu_count() or 1
            
            logger.info(f"ìµœëŒ€ {max_workers} ìŠ¤ë ˆë“œë¡œ ë³‘ë ¬ ë²ˆì—­ (ëŒ€ìƒ: {len(chunks_to_process_with_indices)} ì²­í¬)...")

            pbar = None
            if tqdm_file_stream and chunks_to_process_with_indices: 
                pbar = tqdm(total=len(chunks_to_process_with_indices), 
                            desc="ì²­í¬ ë²ˆì—­", 
                            unit="ì²­í¬", 
                            file=tqdm_file_stream, 
                            initial=0, 
                            leave=False,
                            smoothing=0.1)


            self.executor = ThreadPoolExecutor(max_workers=max_workers)
            try:
                future_to_chunk_index: Dict[Any, int] = {}
                for i, chunk_text in chunks_to_process_with_indices:
                    if self.stop_requested:
                        logger.info("ìƒˆ ì‘ì—… ì œì¶œ ì¤‘ë‹¨ë¨ (ì‚¬ìš©ì ìš”ì²­).")
                        break
                    future = self.executor.submit(self._translate_and_save_chunk, i, chunk_text,
                                            chunked_output_file_path,
                                            total_chunks, 
                                            input_file_path_obj, progress_callback)
                    future_to_chunk_index[future] = i

                if self.stop_requested and not future_to_chunk_index:
                    logger.info("ë²ˆì—­ ì‹œì‘ ì „ ì¤‘ì§€ ìš”ì²­ë¨.")
                    if pbar: pbar.close()

                # as_completedì—ì„œ ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦¬ë˜, stop_requestedê°€ Trueê°€ ë˜ë©´ ë£¨í”„ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.
                for future in as_completed(future_to_chunk_index.keys()):
                    if self.stop_requested:
                        # ì‹¤í–‰ ì¤‘ì¸ futureë“¤ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.
                        for f in future_to_chunk_index.keys():
                            if not f.done():
                                f.cancel()
                        logger.info("ì§„í–‰ ì¤‘ì¸ ì‘ì—…ë“¤ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                        break
                    chunk_idx_completed = future_to_chunk_index[future]
                    try:
                        future.result()
                        
                        # ì‹¤ì‹œê°„ í’ˆì§ˆ ê²€ì‚¬ (ìµœì†Œ 5ê°œ ì´ìƒ ì²˜ë¦¬ë˜ì—ˆì„ ë•Œë¶€í„°)
                        if self.processed_chunks_count >= 5:
                            try:
                                # ë©”íƒ€ë°ì´í„° ë¡œë“œ (ìµœì‹  ìƒíƒœ ë°˜ì˜)
                                current_metadata = load_metadata(metadata_file_path)
                                suspicious_chunks = self.quality_check_service.analyze_translation_quality(current_metadata)
                                
                                # ì´ë²ˆì— ì™„ë£Œëœ ì²­í¬ê°€ ì˜ì‹¬ ëª©ë¡ì— ìˆëŠ”ì§€ í™•ì¸
                                for chunk in suspicious_chunks:
                                    if chunk['chunk_index'] == chunk_idx_completed:
                                        issue_type = chunk['issue_type']
                                        z_score = chunk['z_score']
                                        ratio = chunk['ratio']
                                        
                                        # ì›ë¬¸ ë¯¸ë¦¬ë³´ê¸° ìƒì„± (ì‚¬ìš©ìê°€ ë¬¸ì œ íŒŒì•… ìš©ì´)
                                        source_text = all_chunks[chunk_idx_completed] if chunk_idx_completed < len(all_chunks) else ""
                                        text_preview = source_text[:80].replace('\n', ' ') + ('...' if len(source_text) > 80 else '')
                                        
                                        if issue_type == "omission":
                                            logger.warning(f"âš ï¸ ë²ˆì—­ ëˆ„ë½ ì˜ì‹¬ (ì²­í¬ {chunk_idx_completed}): ë¹„ìœ¨ {ratio:.2f}, Z-Score {z_score} | ì›ë¬¸: {text_preview}")
                                        elif issue_type == "hallucination":
                                            logger.warning(f"âš ï¸ AI í™˜ê° ì˜ì‹¬ (ì²­í¬ {chunk_idx_completed}): ë¹„ìœ¨ {ratio:.2f}, Z-Score {z_score} | ì›ë¬¸: {text_preview}")
                            except Exception as qc_e:
                                logger.warning(f"í’ˆì§ˆ ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¬´ì‹œë¨): {qc_e}")

                    except Exception as e_future:
                        logger.error(f"ë³‘ë ¬ ì‘ì—… (ì²­í¬ {chunk_idx_completed}) ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ (as_completed): {e_future}", exc_info=True)
                    finally:
                        if pbar: pbar.update(1)
            finally:
                if pbar: pbar.close()
                self.executor.shutdown(wait=False) # ëª¨ë“  ìŠ¤ë ˆë“œê°€ ì¦‰ì‹œ ì¢…ë£Œë˜ë„ë¡ ë³´ì¥
                logger.info("ThreadPoolExecutorê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê²°ê³¼ ë³‘í•©ì„ ì‹œì‘í•©ë‹ˆë‹¤.") 


            logger.info("ëª¨ë“  ëŒ€ìƒ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ. ê²°ê³¼ ë³‘í•© ë° ìµœì¢… ì €ì¥ ì‹œì‘...")
            # ë‹¨ì¼ ì†ŒìŠ¤(.chunked.txt)ì—ì„œ ìµœì¢… ë³‘í•© ëŒ€ìƒ ë¡œë“œ
            final_merged_chunks: Dict[int, str] = {}
            try:
                final_merged_chunks = load_chunks_from_file(chunked_output_file_path)
                logger.info(f"ìµœì¢… ë³‘í•© ëŒ€ìƒ ì²­í¬ ìˆ˜: {len(final_merged_chunks)}")
            except Exception as e:
                logger.error(f"ì²­í¬ íŒŒì¼ '{chunked_output_file_path}' ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}. ìµœì¢… ì €ì¥ì´ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", exc_info=True)
            
            try:
                save_merged_chunks_to_file(final_output_file_path_obj, final_merged_chunks)
                logger.info(f"ìµœì¢… ë²ˆì—­ ê²°ê³¼ê°€ '{final_output_file_path_obj}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                  # âœ… ê°œì„ : í›„ì²˜ë¦¬ ì‹¤í–‰ (ì„¤ì •ì—ì„œ í™œì„±í™”ëœ ê²½ìš°)
                if self.config.get("enable_post_processing", True):
                    logger.info("ë²ˆì—­ ì™„ë£Œ í›„ í›„ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
                    try:
                        # 1. ë¨¼ì € ì²­í¬ ì¸ë±ìŠ¤ê°€ í¬í•¨ëœ ë°±ì—… íŒŒì¼ ì €ì¥ (í›„ì²˜ë¦¬ ì „ ì›ë³¸ ë³´ì¡´)
                        chunked_backup_path = final_output_file_path_obj.with_suffix('.chunked.txt')
                        save_merged_chunks_to_file(chunked_backup_path, final_merged_chunks)
                        logger.info(f"ì´ì–´í•˜ê¸°ìš© ì²­í¬ ë°±ì—… íŒŒì¼ ì €ì¥ ì™„ë£Œ: {chunked_backup_path}")
                        
                        # 2. ì²­í¬ ë‹¨ìœ„ í›„ì²˜ë¦¬ (í—¤ë” ì œê±°, HTML ì •ë¦¬ ë“±)
                        processed_chunks = self.post_processing_service.post_process_merged_chunks(final_merged_chunks, self.config)
                        
                        # 3. í›„ì²˜ë¦¬ëœ ë‚´ìš©ì„ ì„ì‹œë¡œ ì €ì¥ (ì²­í¬ ì¸ë±ìŠ¤ëŠ” ì—¬ì „íˆ í¬í•¨)
                        save_merged_chunks_to_file(final_output_file_path_obj, processed_chunks)
                        logger.info("ì²­í¬ ë‹¨ìœ„ í›„ì²˜ë¦¬ ì™„ë£Œ ë° ì„ì‹œ ì €ì¥ë¨.")
                        
                        # 4. ìµœì¢…ì ìœ¼ë¡œ ì²­í¬ ì¸ë±ìŠ¤ ë§ˆì»¤ ì œê±° (ì‚¬ìš©ìê°€ ë³´ëŠ” ìµœì¢… íŒŒì¼)
                        if self.post_processing_service.remove_chunk_indexes_from_final_file(final_output_file_path_obj):
                            logger.info("ìµœì¢… ì¶œë ¥ íŒŒì¼ì—ì„œ ì²­í¬ ì¸ë±ìŠ¤ ë§ˆì»¤ ì œê±° ì™„ë£Œ.")
                        else:
                            logger.warning("ì²­í¬ ì¸ë±ìŠ¤ ë§ˆì»¤ ì œê±°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                            
                        # 5. í›„ì²˜ë¦¬ëœ ë‚´ìš©ë„ ë°±ì—… íŒŒì¼ì— ì—…ë°ì´íŠ¸ (ì²­í¬ ì¸ë±ìŠ¤ í¬í•¨)
                        save_merged_chunks_to_file(chunked_backup_path, processed_chunks)
                        logger.info(f"í›„ì²˜ë¦¬ëœ ì²­í¬ ë°±ì—… íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {chunked_backup_path}")
                        
                    except Exception as post_proc_e:
                        logger.error(f"í›„ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {post_proc_e}. í›„ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.", exc_info=True)
                        # í›„ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œì—ë„ ì²­í¬ ë°±ì—… íŒŒì¼ì€ ë³´ì¥
                        try:
                            chunked_backup_path = final_output_file_path_obj.with_suffix('.chunked.txt')
                            save_merged_chunks_to_file(chunked_backup_path, final_merged_chunks)
                            logger.info(f"í›„ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì²­í¬ ë°±ì—… íŒŒì¼ ì €ì¥: {chunked_backup_path}")
                        except Exception as backup_fallback_e:
                            logger.error(f"ì›ë³¸ ì²­í¬ ë°±ì—… íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {backup_fallback_e}", exc_info=True)
                else:
                    logger.info("í›„ì²˜ë¦¬ê°€ ì„¤ì •ì—ì„œ ë¹„í™œì„±í™”ë˜ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                    # í›„ì²˜ë¦¬ê°€ ë¹„í™œì„±í™”ëœ ê²½ìš°ì—ë„ ì²­í¬ ë°±ì—… íŒŒì¼ì€ ì €ì¥ (ì´ì–´í•˜ê¸°ìš©)
                    try:
                        chunked_backup_path = final_output_file_path_obj.with_suffix('.chunked.txt')
                        save_merged_chunks_to_file(chunked_backup_path, final_merged_chunks)
                        logger.info(f"ì²­í¬ ì¸ë±ìŠ¤ í¬í•¨ ë°±ì—… íŒŒì¼ ì €ì¥: {chunked_backup_path} (í›„ì²˜ë¦¬ ë¹„í™œì„±í™”ë¨)")
                    except Exception as backup_e:
                        logger.error(f"ì²­í¬ ë°±ì—… íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {backup_e}", exc_info=True)
                        
            except Exception as e:
                logger.error(f"ìµœì¢… ë²ˆì—­ ê²°ê³¼ íŒŒì¼ '{final_output_file_path_obj}' ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
                raise BtgFileHandlerException(f"ìµœì¢… ì¶œë ¥ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}", original_exception=e)

            # current_run.tmpëŠ” ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ

            final_status_msg = "ë²ˆì—­ ì™„ë£Œ."
            with self._progress_lock:
                # í•­ìƒ ìµœì‹  ë©”íƒ€ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë¡œë“œí•˜ì—¬ ë®ì–´ì“°ê¸° ë¬¸ì œë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
                try:
                    # ì‘ì—… ì‹œì‘ ì‹œì ì˜ ë©”íƒ€ë°ì´í„°ê°€ ì•„ë‹Œ, íŒŒì¼ì— ê¸°ë¡ëœ ìµœì‹  ìƒíƒœë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
                    loaded_metadata = load_metadata(metadata_file_path)
                    if not loaded_metadata:
                        logger.warning("ìµœì¢… ë©”íƒ€ë°ì´í„° ì €ì¥ ë‹¨ê³„ì—ì„œ ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                        # ì´ ê²½ìš°, ì •ë³´ê°€ ì¼ë¶€ ìœ ì‹¤ë  ìˆ˜ ìˆì§€ë§Œ ìµœì†Œí•œì˜ êµ¬ì¡°ëŠ” ë³´ì¡´í•©ë‹ˆë‹¤.
                        loaded_metadata = create_new_metadata(input_file_path_obj, total_chunks, self.config)
                    else:
                        logger.info("ìµœì¢… ì €ì¥ì„ ìœ„í•´ ìµœì‹  ë©”íƒ€ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë‹¤ì‹œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    logger.error(f"ìµœì¢… ë©”íƒ€ë°ì´í„° ì €ì¥ ì „, ìµœì‹  ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}. ì¼ë¶€ ì •ë³´ê°€ ìœ ì‹¤ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    # loaded_metadataëŠ” ì´ì „ ìƒíƒœë¥¼ ìœ ì§€í•˜ì§€ë§Œ, ì˜¤ë¥˜ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.

                if self.stop_requested:
                    final_status_msg = "ë²ˆì—­ ì¤‘ë‹¨ë¨."
                    loaded_metadata["status"] = "stopped"
                elif self.failed_chunks_count > 0:
                    final_status_msg = f"ë²ˆì—­ ì™„ë£Œ (ì‹¤íŒ¨ {self.failed_chunks_count}ê°œ í¬í•¨)."
                    loaded_metadata["status"] = "completed_with_errors"
                elif self.successful_chunks_count == total_chunks: 
                    final_status_msg = "ë²ˆì—­ ì™„ë£Œ (ëª¨ë“  ì²­í¬ ì„±ê³µ)."
                    loaded_metadata["status"] = "completed"
                else: 
                    if self.processed_chunks_count == total_chunks: 
                        final_status_msg = f"ë²ˆì—­ ì™„ë£Œ (ì„±ê³µ: {self.successful_chunks_count}/{total_chunks}, ì‹¤íŒ¨: {self.failed_chunks_count})."
                        loaded_metadata["status"] = "completed_with_errors" if self.failed_chunks_count > 0 else "completed_with_pending" 
                    else: 
                        final_status_msg = f"ë²ˆì—­ ì²˜ë¦¬ë¨ (ì²˜ë¦¬ ì‹œë„: {self.processed_chunks_count}/{total_chunks}, ì„±ê³µ: {self.successful_chunks_count})."
                        loaded_metadata["status"] = "unknown_incomplete" 
                logger.info(f"ìµœì¢… ìƒíƒœ ë©”ì‹œì§€: {final_status_msg}")

                loaded_metadata["last_updated"] = time.time()
                save_metadata(metadata_file_path, loaded_metadata)

            if status_callback: status_callback(final_status_msg)
            if progress_callback:
                with self._progress_lock:
                    progress_callback(TranslationJobProgressDTO(
                        total_chunks, self.processed_chunks_count,
                        self.successful_chunks_count, self.failed_chunks_count,
                        final_status_msg
                    ))

        except Exception as e:
            logger.error(f"ë²ˆì—­ ì„œë¹„ìŠ¤ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            if status_callback: status_callback(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
            tc_for_error_dto = total_chunks if 'total_chunks' in locals() and total_chunks > 0 else 0
            pc_for_error_dto = self.processed_chunks_count if hasattr(self, 'processed_chunks_count') else 0
            sc_for_error_dto = self.successful_chunks_count if hasattr(self, 'successful_chunks_count') else 0
            fc_for_error_dto = self.failed_chunks_count if hasattr(self, 'failed_chunks_count') else 0

            if progress_callback: progress_callback(TranslationJobProgressDTO(
                tc_for_error_dto, pc_for_error_dto, sc_for_error_dto, fc_for_error_dto, f"ì˜¤ë¥˜ ë°œìƒ: {e}"
            ))
            
            with self._progress_lock:
                error_metadata = {}
                try:
                    error_metadata = load_metadata(metadata_file_path) 
                except Exception as load_err:
                    logger.error(f"ì˜¤ë¥˜ ë°œìƒ í›„ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {load_err}")

                if not error_metadata.get("config_hash"): 
                    error_metadata = create_new_metadata(input_file_path_obj, tc_for_error_dto, self.config)

                error_metadata["status"] = "error"
                error_metadata["last_updated"] = time.time()
                if 'total_chunks' in locals(): error_metadata["total_chunks"] = total_chunks 
                save_metadata(metadata_file_path, error_metadata)

            # current_run.tmpëŠ” ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
            raise BtgServiceException(f"ë²ˆì—­ ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}", original_exception=e) from e
        finally:
            # ì¤‘ì§€ ìš”ì²­ì´ ìˆì—ˆë˜ ê²½ìš°, is_translation_runningì€ ì´ë¯¸ Falseë¡œ ì„¤ì •ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            if not self.stop_requested:
                with self._translation_lock:
                    self.is_translation_running = False

    def stop_translation(self):
        if not self.is_translation_running:
            logger.info("ë²ˆì—­ ì‘ì—…ì´ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹ˆë¯€ë¡œ ì¤‘ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        logger.info("ë²ˆì—­ ì¤‘ì§€ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤...")
        self.stop_requested = True

        # ThreadPoolExecutorë¥¼ ì§ì ‘ ì¢…ë£Œí•˜ì—¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë“  ìŠ¤ë ˆë“œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
        if hasattr(self, 'executor') and self.executor:
            # shutdown(wait=False)ëŠ” ëŒ€ê¸° ì¤‘ì¸ futureë¥¼ ì·¨ì†Œí•˜ê³ , ì‹¤í–‰ ì¤‘ì¸ ìŠ¤ë ˆë“œê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤.
            self.executor.shutdown(wait=False)
            logger.info("ThreadPoolExecutorê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

        # is_translation_running í”Œë˜ê·¸ë¥¼ Falseë¡œ ì„¤ì •í•˜ì—¬ ìƒˆë¡œìš´ ì‘ì—…ì´ ì‹œì‘ë˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.
        with self._translation_lock:
            self.is_translation_running = False
        
        logger.info("ë²ˆì—­ ì‘ì—…ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def request_stop_translation(self):
        if self.is_translation_running:
            logger.info("ë²ˆì—­ ì¤‘ì§€ ìš”ì²­ ìˆ˜ì‹ ë¨.")
            self.stop_translation()
        else:
            logger.info("ì‹¤í–‰ ì¤‘ì¸ ë²ˆì—­ ì‘ì—…ì´ ì—†ì–´ ì¤‘ì§€ ìš”ì²­ì„ ë¬´ì‹œí•©ë‹ˆë‹¤.")


if __name__ == '__main__':
    import logging
    from logging import DEBUG # type: ignore

    logger.setLevel(DEBUG)

    test_output_dir = Path("test_app_service_output")
    test_output_dir.mkdir(exist_ok=True)

    temp_config_path = test_output_dir / "temp_config.json"
    sample_app_config_data = {
        "api_key": os.environ.get("GOOGLE_API_KEY", "YOUR_DEFAULT_API_KEY_FOR_TEST"),
        "service_account_file_path": None, 
        "use_vertex_ai": False, 
        "gcp_project": None, 
        "gcp_location": None, 
        "model_name": "gemini-2.0-flash",
        "temperature": 0.7,
        "top_p": 0.9,
        "prompts": "Translate to Korean: {{slot}}",
        "chunk_size": 50, 
        "glossary_json_path": str(test_output_dir / "sample_glossary.json"), # Changed from pronouns_csv
        "requests_per_minute": 60,
        # "max_glossary_entries": 5, # ìš©ì–´ì§‘ìœ¼ë¡œ ëŒ€ì²´ë˜ë©´ì„œ ì´ ì„¤ì •ì€ ì§ì ‘ ì‚¬ìš©ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
        "glossary_sampling_ratio": 100.0, # Changed from pronoun_sample_ratio
        "max_workers": 2 
    }

    with open(temp_config_path, "w", encoding="utf-8") as f:
        json.dump(sample_app_config_data, f, indent=4)

    sample_glossary_file = test_output_dir / "sample_glossary.json" # Changed from sample_pronoun_file
    sample_glossary_content = [
        {"keyword": "BTG", "translated_keyword": "ë¹„í‹°ì§€", "source_language": "en", "target_language": "ko", "occurrence_count": 10},
        {"keyword": "Gemini", "translated_keyword": "ì œë¯¸ë‹ˆ", "source_language": "en", "target_language": "ko", "occurrence_count": 5}
    ]
    with open(sample_glossary_file, "w", encoding="utf-8") as f: # Changed to JSON
        json.dump(sample_glossary_content, f, indent=4, ensure_ascii=False)

    temp_input_file = test_output_dir / "sample_input.txt"
    temp_input_content = (
        "Hello BTG.\nThis is a test for the Gemini API.\n"
        "We are testing the application service layer.\n"
        "Another line for chunking. And one more for Gemini.\n"
        "This is the fifth line.\nAnd the sixth line is here.\n"
        "Seventh line for more data.\nEighth line, almost done."
    )
    with open(temp_input_file, "w", encoding="utf-8") as f:
        f.write(temp_input_content)

    temp_output_file = test_output_dir / "sample_output.txt"
    
    temp_metadata_file = get_metadata_file_path(temp_input_file)
    if temp_metadata_file.exists():
        delete_file(temp_metadata_file)
    if temp_output_file.exists():
        delete_file(temp_output_file)


    app_service: Optional[AppService] = None
    try:
        app_service = AppService(config_file_path=temp_config_path)
        logger.info("AppService ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì„±ê³µ.")
    except Exception as e:
        logger.error(f"AppService ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
        exit()

    if app_service and app_service.gemini_client:
        print("\n--- ëª¨ë¸ ëª©ë¡ ì¡°íšŒ í…ŒìŠ¤íŠ¸ ---")
        try:
            models = app_service.get_available_models()
            if models:
                logger.info(f"ì¡°íšŒëœ ëª¨ë¸ ìˆ˜: {len(models)}")
                for m in models[:2]: 
                    logger.info(f"  - {m.get('display_name', m.get('name'))}")
            else:
                logger.info("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        except BtgApiClientException as e:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        except Exception as e_models:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e_models}", exc_info=True)

    else:
        logger.warning("Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

    if app_service and app_service.glossary_service: # Changed from pronoun_service
        print("\n--- ìš©ì–´ì§‘ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ---") # Changed
        try:
            def _glossary_progress_dto_cb(dto: GlossaryExtractionProgressDTO): # Changed DTO and function name
                logger.debug(f"ìš©ì–´ì§‘ ì§„í–‰ DTO: {dto.processed_segments}/{dto.total_segments} - {dto.current_status_message} (ì¶”ì¶œ í•­ëª©: {dto.extracted_entries_count})") # Changed fields
        
            result_path = app_service.extract_glossary( # Changed method
                temp_input_file,
                progress_callback=_glossary_progress_dto_cb, # Changed callback
                seed_glossary_path=sample_glossary_file # Optionally provide a seed path for testing
            )
            logger.info(f"ìš©ì–´ì§‘ ì¶”ì¶œ ì™„ë£Œ, ê²°ê³¼ íŒŒì¼: {result_path}") # Changed
        except Exception as e:
            logger.error(f"ìš©ì–´ì§‘ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True) # Changed
    else:
        logger.warning("Glossary ì„œë¹„ìŠ¤ê°€ ì—†ì–´ ìš©ì–´ì§‘ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.") # Changed

    if app_service and app_service.translation_service and app_service.gemini_client: # Ensure client exists for translation
        print("\n--- ë²ˆì—­ í…ŒìŠ¤íŠ¸ (ë³‘ë ¬ ì²˜ë¦¬) ---")
        try:
            test_tqdm_stream = sys.stdout 

            def _trans_progress_dto(dto: TranslationJobProgressDTO):
                logger.debug(f"ë²ˆì—­ ì§„í–‰ DTO: {dto.current_chunk_processing or '-'}/{dto.total_chunks}, ì„±ê³µ: {dto.successful_chunks}, ì‹¤íŒ¨: {dto.failed_chunks} - {dto.current_status_message}")
                pass

            def _trans_status(status_msg):
                logger.info(f"ë²ˆì—­ ìƒíƒœ: {status_msg}")


            app_service.start_translation(
                temp_input_file,
                temp_output_file,
                _trans_progress_dto,
                _trans_status,
                tqdm_file_stream=test_tqdm_stream 
            )

            start_time = time.time()
            while app_service.is_translation_running and (time.time() - start_time) < 120: 
                time.sleep(0.5)

            if app_service.is_translation_running:
                logger.warning("ë²ˆì—­ ì‘ì—…ì´ ì‹œê°„ ë‚´ì— ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (í…ŒìŠ¤íŠ¸). ì¤‘ì§€ ìš”ì²­...")
                app_service.request_stop_translation()
                time.sleep(2) 

            if temp_output_file.exists():
                logger.info(f"ë²ˆì—­ ì™„ë£Œ, ê²°ê³¼ íŒŒì¼: {temp_output_file}")
            else:
                logger.error("ë²ˆì—­ ê²°ê³¼ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"ë²ˆì—­ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True)
    else:
        logger.warning("Translation ì„œë¹„ìŠ¤ê°€ ì—†ì–´ ë²ˆì—­ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

    logger.info("AppService í…ŒìŠ¤íŠ¸ ì™„ë£Œ.")
