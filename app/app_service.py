# app_service.py
from pathlib import Path
# typing ëª¨ë“ˆì—ì„œ Tupleì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from typing import Dict, Any, Optional, List, Callable, Union, Tuple
import os
import json
import csv
import logging
import asyncio  # asyncio ì„í¬íŠ¸ ì¶”ê°€
import time
from tqdm import tqdm # tqdm ì„í¬íŠ¸ í™•ì¸
import sys # sys ì„í¬íŠ¸ í™•ì¸ (tqdm_file_stream=sys.stdout ì— ì‚¬ìš©ë  ìˆ˜ ìˆìŒ)

try:
    from infrastructure.logger_config import setup_logger
except ImportError:
    from infrastructure.logger_config import setup_logger

try:
    # file_handlerì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ë“¤ì„ import í•©ë‹ˆë‹¤.
    from infrastructure.file_handler import (
        read_text_file, write_text_file,
        save_chunk_with_index_to_file, get_metadata_file_path, delete_file,
        load_chunks_from_file,
        create_new_metadata, save_metadata, load_metadata,
        update_metadata_for_chunk_completion, update_metadata_for_chunk_failure, # ì¶”ê°€
        _hash_config_for_metadata,
        save_merged_chunks_to_file
    )
    from ..core.config.config_manager import ConfigManager
    from infrastructure.gemini_client import GeminiClient, GeminiAllApiKeysExhaustedException, GeminiInvalidRequestException
    from domain.translation_service import TranslationService
    from domain.glossary_service import SimpleGlossaryService
    from ..utils.chunk_service import ChunkService
    from ..core.exceptions import BtgServiceException, BtgConfigException, BtgFileHandlerException, BtgApiClientException, BtgTranslationException, BtgBusinessLogicException
    from ..core.dtos import TranslationJobProgressDTO, GlossaryExtractionProgressDTO
    from ..utils.post_processing_service import PostProcessingService
    from ..utils.quality_check_service import QualityCheckService
except ImportError:
    # Fallback imports
    from infrastructure.file_handler import (
        read_text_file, write_text_file,
        save_chunk_with_index_to_file, get_metadata_file_path, delete_file,
        load_chunks_from_file,
        create_new_metadata, save_metadata, load_metadata,
        update_metadata_for_chunk_completion, update_metadata_for_chunk_failure, # ì¶”ê°€
        _hash_config_for_metadata,
        save_merged_chunks_to_file
    )
    from core.config.config_manager import ConfigManager
    from infrastructure.gemini_client import GeminiClient, GeminiAllApiKeysExhaustedException, GeminiInvalidRequestException
    from domain.translation_service import TranslationService
    from domain.glossary_service import SimpleGlossaryService
    from utils.chunk_service import ChunkService
    from core.exceptions import BtgServiceException, BtgConfigException, BtgFileHandlerException, BtgApiClientException, BtgTranslationException, BtgBusinessLogicException
    from core.dtos import TranslationJobProgressDTO, GlossaryExtractionProgressDTO
    from utils.post_processing_service import PostProcessingService
    from utils.quality_check_service import QualityCheckService

logger = setup_logger(__name__)

class AppService:
    """
    ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì£¼ìš” ìœ ìŠ¤ì¼€ì´ìŠ¤ë¥¼ ì¡°ì •í•˜ëŠ” ì„œë¹„ìŠ¤ ê³„ì¸µì…ë‹ˆë‹¤.
    í”„ë ˆì  í…Œì´ì…˜ ê³„ì¸µê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§/ì¸í”„ë¼ ê³„ì¸µ ê°„ì˜ ì¸í„°í˜ì´ìŠ¤ ì—­í• ì„ í•©ë‹ˆë‹¤.
    """

    def __init__(self, config_file_path: Optional[Union[str, Path]] = None):
        self.config_manager = ConfigManager(config_file_path)
        self.config: Dict[str, Any] = {}
        self.gemini_client: Optional[GeminiClient] = None
        self.translation_service: Optional[TranslationService] = None
        self.glossary_service: Optional[SimpleGlossaryService] = None # Renamed from pronoun_service
        self.chunk_service = ChunkService()

        # === ë¹„ë™ê¸° ë§ˆì´ê·¸ë ˆì´ì…˜: Lock ì œê±°, Task ê°ì²´ ê¸°ë°˜ ìƒíƒœ ê´€ë¦¬ ===
        # ê¸°ì¡´ ìƒíƒœ í”Œë˜ê·¸ ì œê±° (asyncioëŠ” ë‹¨ì¼ ìŠ¤ë ˆë“œ)
        # self.is_translation_running: bool
        # self.stop_requested: bool
        
        # Task ê°ì²´ë¡œ ìƒíƒœ ê´€ë¦¬ (Lock ë¶ˆí•„ìš”)
        self.current_translation_task: Optional[asyncio.Task] = None
        
        # ì·¨ì†Œ ì‹ í˜¸ ì´ë²¤íŠ¸ (Promise.race íŒ¨í„´)
        self.cancel_event: asyncio.Event = asyncio.Event()
        
        # ì¹´ìš´í„° (asyncio ë‹¨ì¼ ìŠ¤ë ˆë“œì´ë¯€ë¡œ Lock ë¶ˆí•„ìš”)
        self.processed_chunks_count = 0
        self.successful_chunks_count = 0
        self.failed_chunks_count = 0
        self.post_processing_service = PostProcessingService()
        self.quality_check_service = QualityCheckService()

        self.load_app_config()

    def load_app_config(self, runtime_overrides: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        logger.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ë¡œë“œ ì¤‘...")

        try:
            # 1. íŒŒì¼ ë° ê¸°ë³¸ê°’ìœ¼ë¡œë¶€í„° ê¸°ë³¸ ì„¤ì • ë¡œë“œ
            config_from_manager = self.config_manager.load_config()
            self.config = config_from_manager # íŒŒì¼/ê¸°ë³¸ê°’ìœ¼ë¡œ ì‹œì‘

            # 2. ì œê³µëœ runtime_overridesê°€ ìˆë‹¤ë©´, self.configì— ë®ì–´ì“°ê¸°
            if runtime_overrides:
                self.config.update(runtime_overrides)
                logger.info(f"ëŸ°íƒ€ì„ ì˜¤ë²„ë¼ì´ë“œ ì ìš©: {list(runtime_overrides.keys())}")
            logger.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ë¡œë“œ ì™„ë£Œ.")

            auth_credentials_for_gemini_client: Optional[Union[str, List[str], Dict[str, Any]]] = None
            use_vertex = self.config.get("use_vertex_ai", False)
            gcp_project_from_config = self.config.get("gcp_project")
            gcp_location = self.config.get("gcp_location")
            sa_file_path_str = self.config.get("service_account_file_path")

            # ì„¤ì • ìš”ì•½ ë¡œê¹… (ì¡°ê±´ë¶€)
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"ì„¤ì • ìš”ì•½: vertex={use_vertex}, project={gcp_project_from_config}, location={gcp_location}")

            if use_vertex:
                logger.info("Vertex AI ì‚¬ìš© ëª¨ë“œë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
                # Vertex AI ëª¨ë“œì—ì„œëŠ” auth_credentials_for_gemini_clientê°€ SA JSON ë¬¸ìì—´, SA Dict, ë˜ëŠ” None (ADCìš©)ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                if sa_file_path_str:
                    sa_file_path = Path(sa_file_path_str)
                    if sa_file_path.is_file():
                        try:
                            auth_credentials_for_gemini_client = read_text_file(sa_file_path)
                            logger.info(f"Vertex AI SA íŒŒì¼ì—ì„œ ì¸ì¦ ì •ë³´ ë¡œë“œë¨: {sa_file_path.name}")
                        except Exception as e:
                            logger.error(f"Vertex AI SA íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
                            auth_credentials_for_gemini_client = None
                    else:
                        logger.warning(f"Vertex AI SA íŒŒì¼ ê²½ë¡œ ë¬´íš¨: {sa_file_path_str}")
                        auth_conf_val = self.config.get("auth_credentials")
                        if isinstance(auth_conf_val, (str, dict)) and auth_conf_val:
                            auth_credentials_for_gemini_client = auth_conf_val
                            logger.info("auth_credentials ê°’ì„ ëŒ€ì²´ ì‚¬ìš©")
                        else:
                            auth_credentials_for_gemini_client = None
                            logger.info("ADC ì‚¬ìš© ì˜ˆì •")
                elif self.config.get("auth_credentials"):
                    auth_conf_val = self.config.get("auth_credentials")
                    if isinstance(auth_conf_val, (str, dict)) and auth_conf_val:
                        auth_credentials_for_gemini_client = auth_conf_val
                        logger.info("Vertex AI: auth_credentials ê°’ ì‚¬ìš©")
                    else:
                        auth_credentials_for_gemini_client = None
                        logger.info("Vertex AI: ADC ì‚¬ìš© ì˜ˆì •")
                else:
                    auth_credentials_for_gemini_client = None
                    logger.info("Vertex AI: ADC ì‚¬ìš©")
            else:
                logger.info("Gemini Developer API ëª¨ë“œ")
                auth_credentials_for_gemini_client = None

                api_keys_list_val = self.config.get("api_keys", [])
                if isinstance(api_keys_list_val, list):
                    valid_api_keys = [key for key in api_keys_list_val if isinstance(key, str) and key.strip()]
                    if valid_api_keys:
                        auth_credentials_for_gemini_client = valid_api_keys
                        logger.info(f"API í‚¤ {len(valid_api_keys)}ê°œ ì‚¬ìš©")
                
                if auth_credentials_for_gemini_client is None:
                    api_key_val = self.config.get("api_key")
                    if isinstance(api_key_val, str) and api_key_val.strip():
                        auth_credentials_for_gemini_client = api_key_val
                        logger.info("ë‹¨ì¼ API í‚¤ ì‚¬ìš©")

                if auth_credentials_for_gemini_client is None:
                    auth_credentials_conf_val = self.config.get("auth_credentials")
                    if isinstance(auth_credentials_conf_val, str) and auth_credentials_conf_val.strip():
                        auth_credentials_for_gemini_client = auth_credentials_conf_val
                        logger.info("auth_credentials ë¬¸ìì—´ ì‚¬ìš©")
                    elif isinstance(auth_credentials_conf_val, list):
                        valid_keys_from_auth_cred = [k for k in auth_credentials_conf_val if isinstance(k, str) and k.strip()]
                        if valid_keys_from_auth_cred:
                            auth_credentials_for_gemini_client = valid_keys_from_auth_cred
                            logger.info(f"auth_credentialsì—ì„œ API í‚¤ {len(valid_keys_from_auth_cred)}ê°œ ì‚¬ìš©")
                    elif isinstance(auth_credentials_conf_val, dict):
                        auth_credentials_for_gemini_client = auth_credentials_conf_val
                        logger.info("auth_credentials SA dict ì‚¬ìš©")

                if auth_credentials_for_gemini_client is None:
                    logger.warning("API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")

            should_initialize_client = False
            if auth_credentials_for_gemini_client:
                if isinstance(auth_credentials_for_gemini_client, str) and auth_credentials_for_gemini_client.strip():
                    should_initialize_client = True
                elif isinstance(auth_credentials_for_gemini_client, list) and auth_credentials_for_gemini_client:
                    should_initialize_client = True
                elif isinstance(auth_credentials_for_gemini_client, dict):
                    should_initialize_client = True
            elif use_vertex and not auth_credentials_for_gemini_client and \
                 (gcp_project_from_config or os.environ.get("GOOGLE_CLOUD_PROJECT")):
                should_initialize_client = True
                logger.info("Vertex AI ADC ëª¨ë“œë¡œ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜ˆì •")

            if should_initialize_client:
                try:
                    project_to_pass_to_client = gcp_project_from_config if gcp_project_from_config and gcp_project_from_config.strip() else None
                    rpm_value = self.config.get("requests_per_minute")
                    api_timeout_value = self.config.get("api_timeout", 500.0)
                    logger.info(f"GeminiClient ì´ˆê¸°í™”: project={project_to_pass_to_client}, RPM={rpm_value}, Timeout={api_timeout_value}s")
                    self.gemini_client = GeminiClient(
                        auth_credentials=auth_credentials_for_gemini_client,
                        project=project_to_pass_to_client,
                        location=gcp_location,
                        requests_per_minute=rpm_value,
                        api_timeout=api_timeout_value
                    )
                except GeminiInvalidRequestException as e_inv:
                    logger.error(f"GeminiClient ì´ˆê¸°í™” ì‹¤íŒ¨: {e_inv}")
                    self.gemini_client = None
                except Exception as e_client:
                    logger.error(f"GeminiClient ì´ˆê¸°í™” ì˜¤ë¥˜: {e_client}", exc_info=True)
                    self.gemini_client = None
            else:
                logger.warning("API í‚¤ ë˜ëŠ” Vertex AI ì„¤ì •ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”ë¥¼ ì‹œë„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                self.gemini_client = None

            if self.gemini_client:
                self.translation_service = TranslationService(self.gemini_client, self.config)
                self.glossary_service = SimpleGlossaryService(self.gemini_client, self.config) # Changed to SimpleGlossaryService
                logger.info("TranslationService ë° SimpleGlossaryServiceê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.") # Message updated
            else:
                self.translation_service = None
                self.glossary_service = None # Renamed
                logger.warning("Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ë²ˆì—­ ë° ê³ ìœ ëª…ì‚¬ ì„œë¹„ìŠ¤ê°€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

            return self.config
        except FileNotFoundError as e:
            logger.error(f"ì„¤ì • íŒŒì¼ ì°¾ê¸° ì‹¤íŒ¨: {e}")
            self.config = self.config_manager.get_default_config()
            logger.warning("ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤. Gemini í´ë¼ì´ì–¸íŠ¸ëŠ” ì´ˆê¸°í™”ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            self.gemini_client = None
            self.translation_service = None # Keep
            self.glossary_service = None # Renamed
            return self.config
        except Exception as e:
            logger.error(f"ì„¤ì • ë¡œë“œ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            raise BtgConfigException(f"ì„¤ì • ë¡œë“œ ì˜¤ë¥˜: {e}", original_exception=e) from e

    def save_app_config(self, config_data: Dict[str, Any]) -> bool:
        logger.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ì €ì¥ ì¤‘...")
        try:
            success = self.config_manager.save_config(config_data)
            if success:
                logger.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ì €ì¥ ì™„ë£Œ.")
                # ì €ì¥ í›„ì—ëŠ” íŒŒì¼ì—ì„œ ìµœì‹  ì„¤ì •ì„ ë¡œë“œí•˜ë¯€ë¡œ runtime_overrides ì—†ì´ í˜¸ì¶œ
                # config_dataê°€ ìµœì‹  ìƒíƒœì´ë¯€ë¡œ, ì´ë¥¼ self.configì— ë°˜ì˜í•˜ê³  í´ë¼ì´ì–¸íŠ¸ë¥¼ ì¬ì„¤ì •í•  ìˆ˜ë„ ìˆì§€ë§Œ,
                # load_app_config()ë¥¼ í˜¸ì¶œí•˜ì—¬ ì¼ê´€ëœ ë¡œì§ì„ ë”°ë¥´ëŠ” ê²ƒì´ ë” ê°„ë‹¨í•©ë‹ˆë‹¤.
                self.load_app_config() # runtime_overrides=None (ê¸°ë³¸ê°’)
            return success
        except Exception as e:
            logger.error(f"ì„¤ì • ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise BtgConfigException(f"ì„¤ì • ì €ì¥ ì˜¤ë¥˜: {e}", original_exception=e) from e

    async def get_available_models(self) -> List[Dict[str, Any]]:
        if not self.gemini_client:
            logger.error("ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            raise BtgServiceException("Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ ë˜ëŠ” Vertex AI ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        logger.info("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì„œë¹„ìŠ¤ í˜¸ì¶œë¨.")
        try:
            all_models = await self.gemini_client.list_models_async()
            # ëª¨ë¸ í•„í„°ë§ ë¡œì§ ì œê±°ë¨
            logger.info(f"ì´ {len(all_models)}ê°œì˜ ëª¨ë¸ì„ APIë¡œë¶€í„° ì§ì ‘ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return all_models
            
        except BtgApiClientException as e:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ API ì˜¤ë¥˜: {e}")
            raise
        except Exception as e:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True) # type: ignore
            raise BtgServiceException(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}", original_exception=e) from e

    def extract_glossary(
        self,
        input_file_path: Union[str, Path],
        progress_callback: Optional[Callable[[GlossaryExtractionProgressDTO], None]] = None,
        novel_language_code: Optional[str] = None,
        seed_glossary_path: Optional[Union[str, Path]] = None,
        user_override_glossary_extraction_prompt: Optional[str] = None,
        stop_check: Optional[Callable[[], bool]] = None
    ) -> Path:
        """
        ìš©ì–´ì§‘ì„ ì¶”ì¶œí•©ë‹ˆë‹¤ (ë™ê¸° ë˜í¼).
        ë‚´ë¶€ì ìœ¼ë¡œ asyncio.run()ì„ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸° ë²„ì „ì„ í˜¸ì¶œí•©ë‹ˆë‹¤.
        
        Note: CLI ë° í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„±ì„ ìœ„í•œ ë™ê¸° ë˜í¼ì…ë‹ˆë‹¤.
        ìƒˆë¡œìš´ ì½”ë“œì—ì„œëŠ” extract_glossary_async()ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì„¸ìš”.
        """
        logger.info("[ë™ê¸° ë˜í¼] extract_glossary í˜¸ì¶œ -> extract_glossary_asyncë¡œ ì „í™˜")
        return asyncio.run(
            self.extract_glossary_async(
                input_file_path=input_file_path,
                progress_callback=progress_callback,
                novel_language_code=novel_language_code,
                seed_glossary_path=seed_glossary_path,
                user_override_glossary_extraction_prompt=user_override_glossary_extraction_prompt,
                stop_check=stop_check
            )
        )

    async def extract_glossary_async(
        self,
        input_file_path: Union[str, Path],
        progress_callback: Optional[Callable[[GlossaryExtractionProgressDTO], None]] = None,
        novel_language_code: Optional[str] = None,
        seed_glossary_path: Optional[Union[str, Path]] = None,
        user_override_glossary_extraction_prompt: Optional[str] = None,
        stop_check: Optional[Callable[[], bool]] = None
    ) -> Path:
        """
        ìš©ì–´ì§‘ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            input_file_path: ë¶„ì„í•  ì…ë ¥ íŒŒì¼ ê²½ë¡œ
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°±
            novel_language_code: ëª…ì‹œì  ì–¸ì–´ ì½”ë“œ
            seed_glossary_path: ì‹œë“œ ìš©ì–´ì§‘ ê²½ë¡œ
            user_override_glossary_extraction_prompt: ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸
            stop_check: ì¤‘ì§€ í™•ì¸ ì½œë°±
            
        Returns:
            ìƒì„±ëœ ìš©ì–´ì§‘ íŒŒì¼ ê²½ë¡œ
            
        Raises:
            BtgServiceException: ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì•ˆë¨
            BtgFileHandlerException: íŒŒì¼ ì½ê¸° ì‹¤íŒ¨
            asyncio.CancelledError: ì‘ì—… ì·¨ì†Œë¨
        """
        if not self.glossary_service:
            logger.error("ìš©ì–´ì§‘ ì¶”ì¶œ ì„œë¹„ìŠ¤ ì‹¤íŒ¨: ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            raise BtgServiceException("ìš©ì–´ì§‘ ì¶”ì¶œ ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        logger.info(f"ë¹„ë™ê¸° ìš©ì–´ì§‘ ì¶”ì¶œ ì„œë¹„ìŠ¤ ì‹œì‘: {input_file_path}, ì‹œë“œ íŒŒì¼: {seed_glossary_path}")
        try:
            file_content = read_text_file(input_file_path)
            if not file_content:
                logger.warning(f"ì…ë ¥ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {input_file_path}")

            lang_code_for_extraction = novel_language_code or self.config.get("novel_language")

            prompt_to_use = user_override_glossary_extraction_prompt \
                if user_override_glossary_extraction_prompt is not None \
                else self.config.get("user_override_glossary_extraction_prompt")

            max_workers = self.config.get("max_workers", 4)
            rpm = self.config.get("requests_per_minute", 60)
            
            result_path = await self.glossary_service.extract_and_save_glossary_async(
                novel_text_content=file_content,
                input_file_path_for_naming=input_file_path,
                progress_callback=progress_callback,
                seed_glossary_path=seed_glossary_path,
                user_override_glossary_extraction_prompt=prompt_to_use,
                stop_check=stop_check,
                max_workers=max_workers,
                rpm=rpm
            )
            logger.info(f"ë¹„ë™ê¸° ìš©ì–´ì§‘ ì¶”ì¶œ ì™„ë£Œ. ê²°ê³¼ íŒŒì¼: {result_path}")
        
            return result_path
        except asyncio.CancelledError:
            logger.info("ìš©ì–´ì§‘ ì¶”ì¶œì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            raise
        except FileNotFoundError as e:
            logger.error(f"ìš©ì–´ì§‘ ì¶”ì¶œì„ ìœ„í•œ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file_path}")
            if progress_callback:
                progress_callback(GlossaryExtractionProgressDTO(0, 0, f"ì˜¤ë¥˜: ì…ë ¥ íŒŒì¼ ì—†ìŒ - {e.filename}", 0))
            raise BtgFileHandlerException(f"ì…ë ¥ íŒŒì¼ ì—†ìŒ: {input_file_path}", original_exception=e) from e
        except (BtgBusinessLogicException, BtgApiClientException) as e:
            logger.error(f"ìš©ì–´ì§‘ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            if progress_callback:
                progress_callback(GlossaryExtractionProgressDTO(0, 0, f"ì˜¤ë¥˜: {e}", 0))
            raise
        except Exception as e: 
            logger.error(f"ìš©ì–´ì§‘ ì¶”ì¶œ ì„œë¹„ìŠ¤ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
            if progress_callback:
                progress_callback(GlossaryExtractionProgressDTO(0, 0, f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", 0))
    # _translate_and_save_chunk() ë™ê¸° ë©”ì„œë“œ ì œê±°ë¨
    # ë¹„ë™ê¸° ë²„ì „ _translate_and_save_chunk_async()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”

    # ===== ë¹„ë™ê¸° ë©”ì„œë“œ (PySide6 ë§ˆì´ê·¸ë ˆì´ì…˜) =====
    
    async def start_translation_async(
        self,
        input_file_path: Union[str, Path],
        output_file_path: Union[str, Path],
        progress_callback: Optional[Callable[[TranslationJobProgressDTO], None]] = None,
        status_callback: Optional[Callable[[str], None]] = None,
        tqdm_file_stream: Optional[Any] = None,
        retranslate_failed_only: bool = False
    ) -> None:
        """
        ë¹„ë™ê¸° ë²ˆì—­ ì‹œì‘ (GUIì—ì„œ @asyncSlot()ìœ¼ë¡œ í˜¸ì¶œ)
        
        :param input_file_path: ì…ë ¥ íŒŒì¼ ê²½ë¡œ
        :param output_file_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        :param progress_callback: ì§„í–‰ ìƒí™© ì½œë°±
        :param status_callback: ìƒíƒœ ë³€ê²½ ì½œë°±
        :param tqdm_file_stream: ì§„í–‰ë¥  í‘œì‹œ ìŠ¤íŠ¸ë¦¼
        :param retranslate_failed_only: ì‹¤íŒ¨í•œ ì²­í¬ë§Œ ì¬ë²ˆì—­
        :raises BtgServiceException: ì´ë¯¸ ë²ˆì—­ ì¤‘ì¸ ê²½ìš°
        """
        # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì´ë©´ ì˜ˆì™¸ ë°œìƒ
        if self.current_translation_task and not self.current_translation_task.done():
            raise BtgServiceException("ë²ˆì—­ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ë¨¼ì € í˜„ì¬ ì‘ì—…ì„ ì™„ë£Œí•˜ê±°ë‚˜ ì·¨ì†Œí•˜ì„¸ìš”.")
        
        logger.info(f"ë¹„ë™ê¸° ë²ˆì—­ ì‹œì‘: {input_file_path} â†’ {output_file_path}")
        if status_callback:
            status_callback("ë²ˆì—­ ì¤€ë¹„ ì¤‘...")
        
        # === ìš©ì–´ì§‘ ë™ì  ë¡œë”© ë¡œì§ ===
        try:
            input_p = Path(input_file_path)
            glossary_suffix = self.config.get("glossary_output_json_filename_suffix", "_simple_glossary.json")
            assumed_glossary_path = input_p.parent / f"{input_p.stem}{glossary_suffix}"
            
            glossary_to_use = None
            if assumed_glossary_path.exists():
                glossary_to_use = str(assumed_glossary_path)
                logger.info(f"ìš©ì–´ì§‘ '{assumed_glossary_path.name}' ìë™ ë°œê²¬ ë° ì‚¬ìš©")
            else:
                manual_path = self.config.get("glossary_json_path")
                if manual_path and Path(manual_path).exists():
                    glossary_to_use = manual_path
                    logger.info(f"ì„¤ì •ëœ ìš©ì–´ì§‘ ì‚¬ìš©: '{manual_path}'")
                else:
                    logger.info(f"ìš©ì–´ì§‘ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ìš©ì–´ì§‘ ì—†ì´ ì§„í–‰")
            
            if self.translation_service:
                self.config['glossary_json_path'] = glossary_to_use
                self.translation_service.config = self.config
                self.translation_service._load_glossary_data()
        except Exception as e:
            logger.error(f"ìš©ì–´ì§‘ ë™ì  ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        # === ìš©ì–´ì§‘ ë™ì  ë¡œë”© ë¡œì§ ë ===
        
        # âœ¨ Promise.race íŒ¨í„´ êµ¬í˜„ âœ¨
        # ì·¨ì†Œ ì´ë²¤íŠ¸ ì´ˆê¸°í™” (ìƒˆ ë²ˆì—­ ì‹œì‘)
        self.cancel_event.clear()
        
        # ë²ˆì—­ Task ìƒì„±
        translation_task = asyncio.create_task(
            self._do_translation_async(
                input_file_path,
                output_file_path,
                progress_callback,
                status_callback,
                tqdm_file_stream,
                retranslate_failed_only
            ),
            name="translation_main"
        )
        
        # ì·¨ì†Œ ê°ì‹œ Task ìƒì„± (cancelPromise ì—­í• )
        cancel_watch_task = asyncio.create_task(
            self._wait_for_cancel(),
            name="cancel_watcher"
        )
        
        # current_translation_taskëŠ” ë²ˆì—­ Taskë¡œ ì„¤ì • (ìƒíƒœ ê´€ë¦¬ìš©)
        self.current_translation_task = translation_task
        
        logger.info("ğŸ Promise.race ì‹œì‘: ë²ˆì—­ vs ì·¨ì†Œ ê²½í•©")
        
        try:
            # ğŸ Promise.race: ë¨¼ì € ì™„ë£Œë˜ëŠ” Taskì˜ ê²°ê³¼ë¥¼ ë°˜í™˜
            done, pending = await asyncio.wait(
                [translation_task, cancel_watch_task],
                return_when=asyncio.FIRST_COMPLETED
            )
            
            # ê²°ê³¼ ì²˜ë¦¬: ëˆ„ê°€ ì´ê²¼ëŠ”ê°€?
            for task in done:
                if task == cancel_watch_task:
                    # âŒ ì·¨ì†Œ ìŠ¹ë¦¬: ë²ˆì—­ Task ì·¨ì†Œ
                    logger.warning("âŒ ì·¨ì†Œ ìŠ¹ë¦¬! ë²ˆì—­ Task ì·¨ì†Œ ì¤‘...")
                    translation_task.cancel()
                    
                    # ë²ˆì—­ Task ì¢…ë£Œ ëŒ€ê¸° (ì •ë¦¬ ì‘ì—… ì™„ë£Œ ë³´ì¥)
                    try:
                        await translation_task
                    except asyncio.CancelledError:
                        logger.info("âœ… ë²ˆì—­ Task ì·¨ì†Œ ì™„ë£Œ")
                    
                    if status_callback:
                        status_callback("ì¤‘ë‹¨ë¨")
                    raise asyncio.CancelledError("ì‚¬ìš©ìì— ì˜í•´ ì·¨ì†Œë¨")
                
                else:
                    # âœ… ë²ˆì—­ ìŠ¹ë¦¬: ì·¨ì†Œ ê°ì‹œ Task ì •ë¦¬
                    logger.info("âœ… ë²ˆì—­ ìŠ¹ë¦¬! ì·¨ì†Œ ê°ì‹œ Task ì •ë¦¬")
                    cancel_watch_task.cancel()
                    
                    # ë²ˆì—­ ê²°ê³¼ ë°˜í™˜ (awaitë¡œ ì˜ˆì™¸ ì „íŒŒ)
                    await translation_task
                    
        except asyncio.CancelledError:
            logger.info("ë²ˆì—­ì´ ì‚¬ìš©ìì— ì˜í•´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤")
            if status_callback:
                status_callback("ì¤‘ë‹¨ë¨")
            raise
        except Exception as e:
            logger.error(f"ë²ˆì—­ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            if status_callback:
                status_callback(f"ì˜¤ë¥˜: {e}")
            raise
        finally:
            # ì •ë¦¬: ë‚˜ë¨¸ì§€ Task ì·¨ì†Œ
            for task in [translation_task, cancel_watch_task]:
                if not task.done():
                    task.cancel()
                    try:
                        await task
                    except asyncio.CancelledError:
                        pass
            
            self.current_translation_task = None
            logger.info("ğŸ§¹ Promise.race ì¢…ë£Œ ë° ì •ë¦¬ ì™„ë£Œ")
    
    async def cancel_translation_async(self) -> None:
        """
        ë¹„ë™ê¸° ë²ˆì—­ ì·¨ì†Œ (ì¦‰ì‹œ ë°˜ì‘, Promise.race íŒ¨í„´)
        
        asyncio.Eventë¥¼ ì‚¬ìš©í•˜ì—¬ ì·¨ì†Œ ì‹ í˜¸ë¥¼ ì¦‰ì‹œ ì „íŒŒí•©ë‹ˆë‹¤.
        TypeScriptì˜ cancelPromise.reject()ì™€ ë™ì¼í•œ íŒ¨í„´ì…ë‹ˆë‹¤.
        """
        if self.current_translation_task and not self.current_translation_task.done():
            logger.info("ğŸš¨ ë²ˆì—­ ì·¨ì†Œ ìš”ì²­ë¨ (ì·¨ì†Œ ì´ë²¤íŠ¸ ë°œìƒ)")
            self.cancel_event.set()  # âœ… ì¦‰ì‹œ ì·¨ì†Œ ì‹ í˜¸ ë°œìƒ
        else:
            logger.warning("í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ë²ˆì—­ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤")
    
    async def _wait_for_cancel(self) -> None:
        """
        ì·¨ì†Œ ì´ë²¤íŠ¸ ëŒ€ê¸° Task (TypeScript cancelPromise ì—­í• )
        
        ì´ TaskëŠ” cancel_event.wait()ë¡œ ëŒ€ê¸°í•˜ë‹¤ê°€,
        ì·¨ì†Œ ì‹ í˜¸ê°€ ë°œìƒí•˜ë©´ ì¦‰ì‹œ CancelledErrorë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.
        
        Promise.raceì—ì„œ ì´ Taskê°€ ë¨¼ì € ì™„ë£Œë˜ë©´,
        ë²ˆì—­ Taskë¥¼ ì·¨ì†Œí•˜ê³  ì‚¬ìš©ìì—ê²Œ ì·¨ì†Œë¥¼ ì•Œë¦½ë‹ˆë‹¤.
        """
        await self.cancel_event.wait()
        logger.info("â±ï¸ ì·¨ì†Œ ì‹ í˜¸ ê°ì§€ë¨. CancelledError ë°œìƒ")
        raise asyncio.CancelledError("CANCELLED_BY_USER")

    async def _do_translation_async(
        self,
        input_file_path: Union[str, Path],
        output_file_path: Union[str, Path],
        progress_callback: Optional[Callable[[TranslationJobProgressDTO], None]] = None,
        status_callback: Optional[Callable[[str], None]] = None,
        tqdm_file_stream: Optional[Any] = None,
        retranslate_failed_only: bool = False
    ) -> None:
        """
        ë¹„ë™ê¸° ë²ˆì—­ ë©”ì¸ ë¡œì§
        
        - Lock ì œê±° (asyncio ë‹¨ì¼ ìŠ¤ë ˆë“œ)
        - ìƒíƒœëŠ” Task ê°ì²´ë¡œ ê´€ë¦¬
        - ThreadPoolExecutor ì œê±° (asyncio.gather ì‚¬ìš©)
        """
        # ì„œë¹„ìŠ¤ ê²€ì¦
        if not self.translation_service or not self.chunk_service:
            logger.error("ë²ˆì—­ ì„œë¹„ìŠ¤ ì‹¤íŒ¨: ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            if status_callback:
                status_callback("ì˜¤ë¥˜: ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨")
            raise BtgServiceException("ë²ˆì—­ ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        # ìƒíƒœ ì´ˆê¸°í™” (Lock ë¶ˆí•„ìš”)
        self.processed_chunks_count = 0
        self.successful_chunks_count = 0
        self.failed_chunks_count = 0
        
        logger.info(f"ë¹„ë™ê¸° ë²ˆì—­ ì‹œì‘: {input_file_path} â†’ {output_file_path}")
        if status_callback:
            status_callback("ë²ˆì—­ ì‹œì‘ë¨...")
        
        input_file_path_obj = Path(input_file_path)
        final_output_file_path_obj = Path(output_file_path)
        metadata_file_path = get_metadata_file_path(input_file_path_obj)
        loaded_metadata: Dict[str, Any] = {}
        resume_translation = False
        total_chunks = 0
        
        try:
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            if metadata_file_path.exists():
                try:
                    loaded_metadata = load_metadata(metadata_file_path)
                    if loaded_metadata:
                        logger.info(f"ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì„±ê³µ: {metadata_file_path}")
                    else:
                        logger.warning(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
                except json.JSONDecodeError as json_err:
                    logger.error(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ ì†ìƒ (JSONDecodeError): {json_err}. ìƒˆë¡œ ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                    delete_file(metadata_file_path)
                except Exception as e:
                    logger.error(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}. ìƒˆë¡œ ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤.", exc_info=True)
                    delete_file(metadata_file_path)
            else:
                logger.info(f"ê¸°ì¡´ ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
            
            # íŒŒì¼ ì½ê¸° (ë¹„ë™ê¸° ì•„ë‹˜, ë¡œì»¬ I/Oì´ë¯€ë¡œ ë™ê¸° ìœ ì§€)
            try:
                file_content = read_text_file(input_file_path_obj)
            except Exception as file_read_err:
                logger.error(f"ì…ë ¥ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {file_read_err}", exc_info=True)
                if status_callback:
                    status_callback(f"ì˜¤ë¥˜: íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ - {file_read_err}")
                raise
            
            # ì²­í¬ ë¶„í• 
            all_chunks = self.chunk_service.create_chunks_from_file_content(
                file_content,
                self.config.get("chunk_size", 6000)
            )
            total_chunks = len(all_chunks)
            logger.info(f"íŒŒì¼ì´ {total_chunks}ê°œ ì²­í¬ë¡œ ë¶„í• ë¨")
            
            # ì²­í¬ ë°±ì—… íŒŒì¼ ê²½ë¡œ ìƒì„± (ì…ë ¥ íŒŒì¼ ê¸°ì¤€)
            # input.txt â†’ input_translated_chunked.txt
            chunked_output_file_path = input_file_path_obj.parent / f"{input_file_path_obj.stem}_translated_chunked.txt"
            
            # ì„¤ì • í•´ì‹œ í™•ì¸ (ì´ì–´í•˜ê¸° ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨)
            current_config_hash = _hash_config_for_metadata(self.config)
            previous_config_hash = loaded_metadata.get("config_hash")
            
            if previous_config_hash and previous_config_hash == current_config_hash:
                # ì²­í¬ ìˆ˜ ë³€ê²½ ê°ì§€
                if loaded_metadata.get("total_chunks") != total_chunks:
                    logger.warning(f"ì…ë ¥ íŒŒì¼ì˜ ì²­í¬ ìˆ˜ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤ ({loaded_metadata.get('total_chunks')} -> {total_chunks}). ë©”íƒ€ë°ì´í„°ë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                    resume_translation = False
                    loaded_metadata = create_new_metadata(input_file_path_obj, total_chunks, self.config)
                    loaded_metadata["status"] = "in_progress"
                    loaded_metadata["last_updated"] = time.time()
                    save_metadata(metadata_file_path, loaded_metadata)
                    logger.info("ì²­í¬ ìˆ˜ ë³€ê²½ìœ¼ë¡œ ìƒˆ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ")
                    
                    # ì¶œë ¥ íŒŒì¼ ì´ˆê¸°í™”
                    delete_file(final_output_file_path_obj)
                    final_output_file_path_obj.touch()
                    # ì²­í¬ ë°±ì—… íŒŒì¼ë„ ì´ˆê¸°í™”
                    delete_file(chunked_output_file_path)
                    chunked_output_file_path.touch()
                    logger.info(f"ì¶œë ¥ íŒŒì¼ ë° ì²­í¬ ë°±ì—… íŒŒì¼ ì´ˆê¸°í™” ì™„ë£Œ: {final_output_file_path_obj}")
                else:
                    resume_translation = True
                    # ì´ì–´í•˜ê¸° ì‹œ ë©”íƒ€ë°ì´í„° ìƒíƒœ ì—…ë°ì´íŠ¸
                    loaded_metadata["status"] = "in_progress"
                    loaded_metadata["last_updated"] = time.time()
                    save_metadata(metadata_file_path, loaded_metadata)
                    logger.info("ì´ì „ ë²ˆì—­ì„ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤ (ì„¤ì • ë™ì¼)")
            else:
                # config_hash ì—†ê±°ë‚˜ ë¶ˆì¼ì¹˜ â†’ ìƒˆë¡œ ì‹œì‘
                if not previous_config_hash:
                    logger.info("ì„¤ì • í•´ì‹œ ì—†ìŒ (ì˜¤ë˜ëœ ë©”íƒ€ë°ì´í„°) â†’ ìƒˆë¡œìš´ ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤")
                else:
                    logger.info("ìƒˆë¡œìš´ ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤ (ì„¤ì • ë³€ê²½)")
                resume_translation = False
                loaded_metadata = create_new_metadata(input_file_path_obj, total_chunks, self.config)
                loaded_metadata["status"] = "in_progress"
                loaded_metadata["last_updated"] = time.time()
                save_metadata(metadata_file_path, loaded_metadata)
                logger.info("ìƒˆ ë©”íƒ€ë°ì´í„° ìƒì„± ë° ì €ì¥ ì™„ë£Œ")
                
                # ì¶œë ¥ íŒŒì¼ ì´ˆê¸°í™”
                delete_file(final_output_file_path_obj)
                final_output_file_path_obj.touch()
                # ì²­í¬ ë°±ì—… íŒŒì¼ë„ ì´ˆê¸°í™”
                delete_file(chunked_output_file_path)
                chunked_output_file_path.touch()
                logger.info(f"ì¶œë ¥ íŒŒì¼ ë° ì²­í¬ ë°±ì—… íŒŒì¼ ì´ˆê¸°í™” ì™„ë£Œ: {final_output_file_path_obj}")
            
            # ì´ì–´í•˜ê¸° ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ, í˜¹ì‹œ ë§ˆì§€ë§‰ì— ë¶ˆì™„ì „í•œ ì²­í¬ê°€ ìˆë‹¤ë©´ ì •ë¦¬
            try:
                if chunked_output_file_path.exists():
                    existing_chunks = load_chunks_from_file(chunked_output_file_path)
                    save_merged_chunks_to_file(chunked_output_file_path, existing_chunks)
                    logger.info("ì²­í¬ íŒŒì¼ì„ ìŠ¤ìº”í•˜ì—¬ ì™„ì „í•œ ì²­í¬ë§Œ ìœ ì§€í•˜ë„ë¡ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")
            except Exception as sanitize_e:
                logger.warning(f"ì²­í¬ íŒŒì¼ ì •ë¦¬ ì¤‘ ê²½ê³ : {sanitize_e}")
            
            # ì´ì–´í•˜ê¸° ë˜ëŠ” ìƒˆë¡œ ì‹œì‘
            if resume_translation:
                # ì´ë¯¸ ë²ˆì—­ëœ ì²­í¬ ì°¾ê¸°
                translated_chunks = loaded_metadata.get("translated_chunks", {})
                failed_chunks = loaded_metadata.get("failed_chunks", {})
                
                # ğŸ”§ ì´ì–´í•˜ê¸° ì‹œ ì´ë¯¸ ì™„ë£Œëœ ì²­í¬ ìˆ˜ë¡œ ì´ˆê¸°í™”
                self.processed_chunks_count = len(translated_chunks)
                self.successful_chunks_count = len(translated_chunks)
                logger.info(f"ì´ì–´í•˜ê¸°: processed_chunks_count ì´ˆê¸°í™” â†’ {self.processed_chunks_count}")
                
                if retranslate_failed_only:
                    # ì‹¤íŒ¨í•œ ì²­í¬ë§Œ ì¬ë²ˆì—­ (ì•ˆì „í•œ ë”•ì…”ë„ˆë¦¬ ì²´í¬)
                    if failed_chunks:
                        chunks_to_process = [
                            (i, chunk) for i, chunk in enumerate(all_chunks)
                            if str(i) in failed_chunks
                        ]
                        logger.info(f"ì‹¤íŒ¨ ì²­í¬ ì¬ë²ˆì—­ ëª¨ë“œ: {len(chunks_to_process)}ê°œ ëŒ€ìƒ")
                    else:
                        chunks_to_process = []
                        logger.info("ì‹¤íŒ¨í•œ ì²­í¬ê°€ ì—†ì–´ ì¬ë²ˆì—­ì„ ê±´ë„ˆëœë‹ˆë‹¤")
                else:
                    # ëª¨ë“  ë¯¸ë²ˆì—­ ì²­í¬ ì²˜ë¦¬
                    chunks_to_process = [
                        (i, chunk) for i, chunk in enumerate(all_chunks)
                        if str(i) not in translated_chunks
                    ]
                    logger.info(f"ì´ì–´í•˜ê¸°: {len(translated_chunks)}ê°œ ì´ë¯¸ ì™„ë£Œ, {len(chunks_to_process)}ê°œ ì¶”ê°€ ë²ˆì—­ ëŒ€ìƒ")
            else:
                chunks_to_process = list(enumerate(all_chunks))
                logger.info(f"ìƒˆë¡œ ë²ˆì—­: {len(chunks_to_process)}ê°œ ë²ˆì—­ ëŒ€ìƒ")
            
            if not chunks_to_process and total_chunks > 0:
                logger.info("ë²ˆì—­í•  ìƒˆë¡œìš´ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤ (ëª¨ë“  ì²­í¬ê°€ ì´ë¯¸ ë²ˆì—­ë¨)")
                if status_callback:
                    status_callback("ì™„ë£Œ: ëª¨ë“  ì²­í¬ ì´ë¯¸ ë²ˆì—­ë¨")
                loaded_metadata["status"] = "completed"
                loaded_metadata["last_updated"] = time.time()
                save_metadata(metadata_file_path, loaded_metadata)
                return
            
            logger.info(f"ì²˜ë¦¬ ëŒ€ìƒ: {len(chunks_to_process)} ì²­í¬ (ì´ {total_chunks}ê°œ)")
            
            # ë©”íƒ€ë°ì´í„° ìƒíƒœ ì—…ë°ì´íŠ¸ (ë²ˆì—­ ì‹œì‘)
            if loaded_metadata.get("status") != "in_progress":
                loaded_metadata["status"] = "in_progress"
                loaded_metadata["last_updated"] = time.time()
                save_metadata(metadata_file_path, loaded_metadata)
                logger.info("ë²ˆì—­ ì‹œì‘: ë©”íƒ€ë°ì´í„° ìƒíƒœë¥¼ 'in_progress'ë¡œ ì—…ë°ì´íŠ¸")
            
            # ì²­í¬ ë³‘ë ¬ ì²˜ë¦¬ (ì²­í¬ ë°±ì—… íŒŒì¼ì— ì €ì¥)
            await self._translate_chunks_async(
                chunks_to_process,
                chunked_output_file_path,
                total_chunks,
                metadata_file_path,
                input_file_path_obj,
                progress_callback,
                tqdm_file_stream
            )
            
            logger.info("ëª¨ë“  ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ. ê²°ê³¼ ë³‘í•© ë° ìµœì¢… ì €ì¥ ì‹œì‘...")
            
            # ì²­í¬ ë°±ì—… íŒŒì¼ì—ì„œ ìµœì¢… ë³‘í•© ëŒ€ìƒ ë¡œë“œ (ë™ê¸° ë²„ì „ê³¼ ë™ì¼)
            final_merged_chunks: Dict[int, str] = {}
            try:
                final_merged_chunks = load_chunks_from_file(chunked_output_file_path)
                logger.info(f"ìµœì¢… ë³‘í•© ëŒ€ìƒ ì²­í¬ ìˆ˜: {len(final_merged_chunks)}")
            except Exception as e:
                logger.error(f"ì²­í¬ íŒŒì¼ '{chunked_output_file_path}' ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}. ìµœì¢… ì €ì¥ì´ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", exc_info=True)
            
            try:
                # âœ… í›„ì²˜ë¦¬ ì‹¤í–‰ (ì„¤ì •ì—ì„œ í™œì„±í™”ëœ ê²½ìš°)
                if self.config.get("enable_post_processing", True):
                    logger.info("ë²ˆì—­ ì™„ë£Œ í›„ í›„ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
                    try:
                        # 1. ì²­í¬ ë°±ì—… íŒŒì¼ì€ ì´ë¯¸ chunked_output_file_pathì— ì €ì¥ë˜ì–´ ìˆìŒ (ë²ˆì—­ ì¤‘ ìƒì„±)
                        logger.info(f"ì´ì–´í•˜ê¸°ìš© ì²­í¬ ë°±ì—… íŒŒì¼ì´ ë²ˆì—­ ì¤‘ ìƒì„±ë¨: {chunked_output_file_path}")
                        
                        # 2. ì²­í¬ ë‹¨ìœ„ í›„ì²˜ë¦¬ (í—¤ë” ì œê±°, HTML ì •ë¦¬ ë“±)
                        processed_chunks = self.post_processing_service.post_process_merged_chunks(final_merged_chunks, self.config)
                        
                        # 3. í›„ì²˜ë¦¬ëœ ë‚´ìš©ì„ ìµœì¢… ì¶œë ¥ íŒŒì¼ì— ì €ì¥ (ì²­í¬ ì¸ë±ìŠ¤ëŠ” ì—¬ì „íˆ í¬í•¨)
                        save_merged_chunks_to_file(final_output_file_path_obj, processed_chunks)
                        logger.info(f"ì²­í¬ ë‹¨ìœ„ í›„ì²˜ë¦¬ ì™„ë£Œ ë° ìµœì¢… ì¶œë ¥ íŒŒì¼ ì €ì¥: {final_output_file_path_obj}")
                        
                        # 4. ìµœì¢…ì ìœ¼ë¡œ ì²­í¬ ì¸ë±ìŠ¤ ë§ˆì»¤ ì œê±° (ì‚¬ìš©ìê°€ ë³´ëŠ” ìµœì¢… íŒŒì¼)
                        if self.post_processing_service.remove_chunk_indexes_from_final_file(final_output_file_path_obj):
                            logger.info("ìµœì¢… ì¶œë ¥ íŒŒì¼ì—ì„œ ì²­í¬ ì¸ë±ìŠ¤ ë§ˆì»¤ ì œê±° ì™„ë£Œ.")
                        else:
                            logger.warning("ì²­í¬ ì¸ë±ìŠ¤ ë§ˆì»¤ ì œê±°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                            
                    except Exception as post_proc_e:
                        logger.error(f"í›„ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {post_proc_e}. í›„ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.", exc_info=True)
                        # í›„ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë³‘í•© ê²°ê³¼ë¥¼ ìµœì¢… ì¶œë ¥ íŒŒì¼ì— ì €ì¥
                        save_merged_chunks_to_file(final_output_file_path_obj, final_merged_chunks)
                        logger.info(f"í›„ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë³‘í•© ê²°ê³¼ ì €ì¥: {final_output_file_path_obj}")
                else:
                    logger.info("í›„ì²˜ë¦¬ê°€ ì„¤ì •ì—ì„œ ë¹„í™œì„±í™”ë˜ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                    # í›„ì²˜ë¦¬ê°€ ë¹„í™œì„±í™”ëœ ê²½ìš° ì›ë³¸ ë³‘í•© ê²°ê³¼ë¥¼ ìµœì¢… ì¶œë ¥ íŒŒì¼ì— ì €ì¥
                    save_merged_chunks_to_file(final_output_file_path_obj, final_merged_chunks)
                    logger.info(f"í›„ì²˜ë¦¬ ì—†ì´ ì›ë³¸ ë³‘í•© ê²°ê³¼ ì €ì¥: {final_output_file_path_obj}")
                
                # ì²­í¬ ë°±ì—… íŒŒì¼(ì´ì–´í•˜ê¸°ìš©)ì€ ì´ë¯¸ chunked_output_file_pathì— ì¡´ì¬í•¨
                logger.info(f"âœ… ë²ˆì—­ ì™„ë£Œ! ìµœì¢… íŒŒì¼: {final_output_file_path_obj}, ë°±ì—…: {chunked_output_file_path}")
                
                if status_callback:
                    status_callback("ì™„ë£Œ!")
            except Exception as merge_err:
                logger.error(f"ìµœì¢… ì €ì¥ ì¤‘ ì˜¤ë¥˜: {merge_err}", exc_info=True)
                if status_callback:
                    status_callback(f"ì˜¤ë¥˜: ìµœì¢… ì €ì¥ ì‹¤íŒ¨ - {merge_err}")
                raise
            
            # ë©”íƒ€ë°ì´í„° ìµœì¢… ì—…ë°ì´íŠ¸
            # âš ï¸ ì¤‘ìš”: ê° ì²­í¬ ì²˜ë¦¬ ì¤‘ update_metadata_for_chunk_completionì´ íŒŒì¼ì„ ì—…ë°ì´íŠ¸í–ˆìœ¼ë¯€ë¡œ,
            # ë©”ëª¨ë¦¬ì˜ loaded_metadataê°€ ì•„ë‹Œ ìµœì‹  íŒŒì¼ ë‚´ìš©ì„ ë¡œë“œí•˜ì—¬ statusë§Œ ì—…ë°ì´íŠ¸
            try:
                current_metadata = load_metadata(metadata_file_path)
                current_metadata["status"] = "completed"
                current_metadata["last_updated"] = time.time()
                save_metadata(metadata_file_path, current_metadata)
                logger.info(f"ë©”íƒ€ë°ì´í„° ìµœì¢… ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(current_metadata.get('translated_chunks', {}))}ê°œ ì²­í¬ ì •ë³´ ë³´ì¡´")
            except Exception as meta_save_err:
                logger.error(f"ë©”íƒ€ë°ì´í„° ìµœì¢… ì €ì¥ ì¤‘ ì˜¤ë¥˜: {meta_save_err}", exc_info=True)
                # ì‹¤íŒ¨í•´ë„ ë²ˆì—­ íŒŒì¼ì€ ì •ìƒì´ë¯€ë¡œ ê³„ì† ì§„í–‰
            
        except asyncio.CancelledError:
            logger.info("ë¹„ë™ê¸° ë²ˆì—­ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤")
            if status_callback:
                status_callback("ì¤‘ë‹¨ë¨")
            raise
        except Exception as e:
            logger.error(f"ë¹„ë™ê¸° ë²ˆì—­ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            if status_callback:
                status_callback(f"ì˜¤ë¥˜: {e}")
            raise

    async def _translate_chunks_async(
        self,
        chunks: List[Tuple[int, str]],
        output_file: Path,
        total_chunks: int,
        metadata_file_path: Path,
        input_file_path: Path,
        progress_callback: Optional[Callable[[TranslationJobProgressDTO], None]] = None,
        tqdm_file_stream: Optional[Any] = None
    ) -> None:
        """
        ì²­í¬ë“¤ì„ ë¹„ë™ê¸°ë¡œ ë³‘ë ¬ ì²˜ë¦¬
        
        - ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ (max_workers ì ìš©)
        - RPM ì†ë„ ì œí•œ ì ìš©
        - Task.cancel()ë¡œ ì¦‰ì‹œ ì·¨ì†Œ ê°€ëŠ¥
        - tqdm ì§„í–‰ë¥  í‘œì‹œ ì§€ì›
        """
        if not chunks:
            logger.info("ì²˜ë¦¬í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
        
        max_workers = self.config.get("max_workers", 4)
        rpm = self.config.get("requests_per_minute", 60)
        
        logger.info(f"ë¹„ë™ê¸° ì²­í¬ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘: {len(chunks)} ì²­í¬ (ë™ì‹œ ì‘ì—…: {max_workers}, RPM: {rpm})")
        
        # ì„¸ë§ˆí¬ì–´: ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ
        semaphore = asyncio.Semaphore(max_workers)
        
        # RPM ì†ë„ ì œí•œ
        request_interval = 60.0 / rpm if rpm > 0 else 0
        last_request_time = 0
        
        # tqdm ì§„í–‰ë¥  í‘œì‹œ (ë¹„ë™ê¸° í™˜ê²½ì—ì„œë„ ì‚¬ìš© ê°€ëŠ¥)
        pbar = None
        if tqdm_file_stream:
            try:
                from tqdm import tqdm
                pbar = tqdm(
                    total=len(chunks),
                    desc="ë²ˆì—­ ì§„í–‰",
                    unit="ì²­í¬",
                    file=tqdm_file_stream,
                    ncols=100,
                    bar_format='{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'
                )
                logger.debug(f"ë¹„ë™ê¸° tqdm ì§„í–‰ë¥  í‘œì‹œ ì´ˆê¸°í™” ì™„ë£Œ (ì´ {len(chunks)} ì²­í¬)")
            except ImportError:
                logger.warning("tqdmì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§„í–‰ë¥  í‘œì‹œê°€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
            except Exception as tqdm_init_e:
                logger.error(f"tqdm ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {tqdm_init_e}. ì§„í–‰ë¥  í‘œì‹œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        async def rate_limited_translate(chunk_index: int, chunk_text: str) -> bool:
            """RPM ì œí•œì„ ê³ ë ¤í•œ ë²ˆì—­ í•¨ìˆ˜"""
            nonlocal last_request_time
            
            # âœ… ì·¨ì†Œ ì‹ í˜¸ í™•ì¸ (ì„¸ë§ˆí¬ì–´ ì§„ì… ì „ì— ì¦‰ì‹œ ë°˜ì‘)
            if self.cancel_event.is_set():
                logger.info(f"ì²­í¬ {chunk_index + 1} ì·¨ì†Œ ì‹ í˜¸ ê°ì§€í•˜ì—¬ ê±´ë„ˆëœ€")
                raise asyncio.CancelledError("ì·¨ì†Œ ì‹ í˜¸ ê°ì§€")
            
            # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì‹¤í–‰ ì œí•œ
            async with semaphore:
                # âœ… ì„¸ë§ˆí¬ì–´ ì§„ì… í›„ ë‹¤ì‹œ ì·¨ì†Œ ì‹ í˜¸ í™•ì¸ (ëŒ€ê¸° ì¤‘ ì‹ í˜¸ ë°›ì„ ìˆ˜ ìˆìŒ)
                if self.cancel_event.is_set():
                    logger.info(f"ì²­í¬ {chunk_index + 1} ì„¸ë§ˆí¬ì–´ ëŒ€ê¸° ì¤‘ ì·¨ì†Œ ì‹ í˜¸ ê°ì§€")
                    raise asyncio.CancelledError("ì·¨ì†Œ ì‹ í˜¸ ê°ì§€")
                
                # RPM ì†ë„ ì œí•œ ì ìš©
                current_time = asyncio.get_event_loop().time()
                elapsed = current_time - last_request_time
                if elapsed < request_interval:
                    # âœ… asyncio.sleepë„ ì·¨ì†Œì— ë°˜ì‘í•˜ë„ë¡ ì„¤ì •
                    try:
                        await asyncio.sleep(request_interval - elapsed)
                    except asyncio.CancelledError:
                        logger.info(f"ì²­í¬ {chunk_index + 1} RPM ëŒ€ê¸° ì¤‘ ì·¨ì†Œë¨")
                        raise
                
                # âœ… RPM ì§€ì—° í›„ ì·¨ì†Œ ì‹ í˜¸ ì¬í™•ì¸
                if self.cancel_event.is_set():
                    logger.info(f"ì²­í¬ {chunk_index + 1} RPM ì§€ì—° í›„ ì·¨ì†Œ ì‹ í˜¸ ê°ì§€")
                    raise asyncio.CancelledError("ì·¨ì†Œ ì‹ í˜¸ ê°ì§€")
                
                last_request_time = asyncio.get_event_loop().time()
                
                return await self._translate_and_save_chunk_async(
                    chunk_index,
                    chunk_text,
                    output_file,
                    total_chunks,
                    metadata_file_path,
                    input_file_path,
                    progress_callback
                )
        
        # Task ë¦¬ìŠ¤íŠ¸ ìƒì„±
        tasks = []
        for chunk_index, chunk_text in chunks:
            task = asyncio.create_task(rate_limited_translate(chunk_index, chunk_text))
            tasks.append(task)
        
        # ëª¨ë“  Task ì™„ë£Œ ëŒ€ê¸° (ì˜ˆì™¸ ë¬´ì‹œ)
        logger.info(f"{len(tasks)}ê°œ ë¹„ë™ê¸° Task ì‹¤í–‰ ì¤‘...")
        
        try:
            # ë¹„ë™ê¸°ë¡œ Taskë“¤ì„ ì²˜ë¦¬í•˜ë©´ì„œ tqdm ì—…ë°ì´íŠ¸
            results = []
            for task in asyncio.as_completed(tasks):
                try:
                    result = await task
                    results.append(result)
                    # tqdm ì—…ë°ì´íŠ¸
                    if pbar:
                        pbar.update(1)
                except Exception as e:
                    results.append(e)
                    if pbar:
                        pbar.update(1)
        finally:
            # tqdm ì¢…ë£Œ
            if pbar:
                try:
                    pbar.close()
                    logger.debug("ë¹„ë™ê¸° tqdm ì§„í–‰ë¥  í‘œì‹œ ì¢…ë£Œ")
                except Exception as pbar_close_e:
                    logger.warning(f"tqdm ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {pbar_close_e}")
        
        # ê²°ê³¼ ë¶„ì„
        success_count = 0
        error_count = 0
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                if not isinstance(result, asyncio.CancelledError):
                    logger.error(f"ì²­í¬ {i} ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸: {result}")
                error_count += 1
            else:
                if result:
                    success_count += 1
        
        logger.info(f"ì²­í¬ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ: ì„±ê³µ {success_count}, ì‹¤íŒ¨ {error_count}")

    async def _translate_and_save_chunk_async(
        self,
        chunk_index: int,
        chunk_text: str,
        output_file: Path,
        total_chunks: int,
        metadata_file_path: Path,
        input_file_path: Path,
        progress_callback: Optional[Callable[[TranslationJobProgressDTO], None]] = None
    ) -> bool:
        """
        ë¹„ë™ê¸° ì²­í¬ ì²˜ë¦¬ (ë™ê¸° ë²„ì „ê³¼ ë™ì¼í•œ ë¡œê¹… êµ¬ì¡°)
        
        - Lock ì œê±° (asyncio ë‹¨ì¼ ìŠ¤ë ˆë“œ)
        - ë¹„ë™ê¸° ë²ˆì—­ í˜¸ì¶œ
        - íŒŒì¼ ì“°ê¸°ëŠ” ìˆœì°¨ ì²˜ë¦¬
        - íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ í¬í•¨
        """
        current_chunk_info_msg = f"ì²­í¬ {chunk_index + 1}/{total_chunks}"
        
        # ì²­í¬ ë¶„ì„ (ë¡œê¹… ìµœì í™”: í†µê³„ëŠ” DEBUG ë ˆë²¨ì—ì„œë§Œ ìƒì„¸ ì¶œë ¥)
        chunk_chars = len(chunk_text)
        start_time = time.time()
        
        # í†µí•© ë¡œê·¸: ì‹œì‘ ì •ë³´ì™€ ê¸°ë³¸ í†µê³„ë¥¼ í•œ ì¤„ë¡œ
        logger.info(f"{current_chunk_info_msg} ì²˜ë¦¬ ì‹œì‘ (ê¸¸ì´: {chunk_chars}ì)")
        
        # ìƒì„¸ ì •ë³´ëŠ” DEBUG ë ˆë²¨ì—ì„œë§Œ ì¶œë ¥
        if logger.isEnabledFor(logging.DEBUG):
            chunk_lines = chunk_text.count('\n') + 1
            chunk_words = len(chunk_text.split())
            chunk_preview = chunk_text[:100].replace('\n', ' ') + '...' if len(chunk_text) > 100 else chunk_text
            logger.debug(f"  ğŸ“ ë¯¸ë¦¬ë³´ê¸°: {chunk_preview}")
            logger.debug(f"  ğŸ“Š í†µê³„: ê¸€ì={chunk_chars}, ë‹¨ì–´={chunk_words}, ì¤„={chunk_lines}")
        
        last_error = ""
        success = False
        translated_chunk = ""
        
        try:
            # ë¹ˆ ì²­í¬ ì²´í¬
            if not chunk_text.strip():
                logger.warning(f"  âš ï¸ {current_chunk_info_msg} ë¹ˆ ì²­í¬ (ê±´ë„ˆëœ€)")
                return False
            
            # ë²ˆì—­ ì„¤ì • ë¡œë“œ
            model_name = self.config.get("model_name", "gemini-2.0-flash")
            
            # ë²ˆì—­ ì„¤ì • ìƒì„¸ëŠ” DEBUGì—ì„œë§Œ ì¶œë ¥
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"  âš™ï¸ ì„¤ì •: ëª¨ë¸={model_name}, íƒ€ì„ì•„ì›ƒ=300ì´ˆ")
            
            translation_start_time = time.time()
            
            # ë¹„ë™ê¸° ë²ˆì—­ í˜¸ì¶œ (timeoutì€ GeminiClientì˜ http_optionsì— ì˜í•´ ìë™ ì ìš©)
            try:
                translated_chunk = await self.translation_service.translate_chunk_async(
                    chunk_text
                )
                success = True
                
                translation_time = time.time() - translation_start_time
                translated_length = len(translated_chunk)
                
                # ë²ˆì—­ ì„±ëŠ¥ ìƒì„¸ëŠ” DEBUGì—ì„œë§Œ
                if logger.isEnabledFor(logging.DEBUG):
                    speed = chunk_chars / translation_time if translation_time > 0 else 0
                    logger.debug(f"  âœ… ë²ˆì—­ì™„ë£Œ: {translated_length}ì, {translation_time:.2f}ì´ˆ, {speed:.0f}ì/ì´ˆ")
                
            except asyncio.TimeoutError:
                translation_time = time.time() - translation_start_time
                logger.error(f"  âŒ {current_chunk_info_msg} íƒ€ì„ì•„ì›ƒ (300ì´ˆ ì´ˆê³¼, ì‹¤ì œ: {translation_time:.1f}ì´ˆ)")
                translated_chunk = f"[íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ë²ˆì—­ ì‹¤íŒ¨]\n\n--- ì›ë¬¸ ë‚´ìš© ---\n{chunk_text}"
                last_error = "Timeout (300ì´ˆ ì´ˆê³¼)"
                success = False
                
            except asyncio.CancelledError:
                logger.warning(f"  âš ï¸ {current_chunk_info_msg} ì·¨ì†Œë¨")
                raise
            
            # íŒŒì¼ ì €ì¥ (Lock ë¶ˆí•„ìš”, asyncio ë‹¨ì¼ ìŠ¤ë ˆë“œ)
            save_chunk_with_index_to_file(output_file, chunk_index, translated_chunk)
            
            if success:
                ratio = len(translated_chunk) / len(chunk_text) if len(chunk_text) > 0 else 0.0
                total_processing_time = time.time() - start_time
                logger.info(f"  ğŸ¯ {current_chunk_info_msg} ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ (ì´ ì†Œìš”: {total_processing_time:.2f}ì´ˆ, ê¸¸ì´ë¹„ìœ¨: {ratio:.2f})")
            
        except BtgTranslationException as e_trans:
            processing_time = time.time() - start_time
            error_type = "ì½˜í…ì¸  ê²€ì—´" if "ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œ" in str(e_trans) else "ë²ˆì—­ ì„œë¹„ìŠ¤"
            logger.error(f"  âŒ {current_chunk_info_msg} ì‹¤íŒ¨: {error_type} - {e_trans} ({processing_time:.2f}ì´ˆ)")
            
            save_chunk_with_index_to_file(output_file, chunk_index, f"[ë²ˆì—­ ì‹¤íŒ¨: {e_trans}]\n\n--- ì›ë¬¸ ë‚´ìš© ---\n{chunk_text}")
            last_error = str(e_trans)
            success = False
            
        except BtgApiClientException as e_api:
            processing_time = time.time() - start_time
            # API ì˜¤ë¥˜ ìœ í˜• íŒë³„
            error_detail = ""
            if "ì‚¬ìš©ëŸ‰ ì œí•œ" in str(e_api) or "429" in str(e_api):
                error_detail = " [ì‚¬ìš©ëŸ‰ ì œí•œ]"
            elif "í‚¤" in str(e_api).lower() or "ì¸ì¦" in str(e_api):
                error_detail = " [ì¸ì¦ ì˜¤ë¥˜]"
            logger.error(f"  âŒ {current_chunk_info_msg} API ì˜¤ë¥˜{error_detail}: {e_api} ({processing_time:.2f}ì´ˆ)")
            
            save_chunk_with_index_to_file(output_file, chunk_index, f"[API ì˜¤ë¥˜ë¡œ ë²ˆì—­ ì‹¤íŒ¨: {e_api}]\n\n--- ì›ë¬¸ ë‚´ìš© ---\n{chunk_text}")
            last_error = str(e_api)
            success = False
            
        except asyncio.CancelledError:
            logger.info(f"  âš ï¸ {current_chunk_info_msg} ì·¨ì†Œë¨ (CancelledError)")
            raise
            
        except Exception as e_gen:
            processing_time = time.time() - start_time
            logger.error(f"  âŒ {current_chunk_info_msg} ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {type(e_gen).__name__} - {e_gen} ({processing_time:.2f}ì´ˆ)", exc_info=True)
            
            try:
                save_chunk_with_index_to_file(
                    output_file,
                    chunk_index,
                    f"[ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ë¡œ ë²ˆì—­ ì‹¤íŒ¨: {e_gen}]\n\n--- ì›ë¬¸ ë‚´ìš© ---\n{chunk_text}"
                )
            except Exception as save_err:
                logger.error(f"  âŒ ì‹¤íŒ¨ ì²­í¬ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {save_err}")
            
            last_error = str(e_gen)
            success = False
        
        finally:
            total_time = time.time() - start_time
            # ìƒíƒœ ì—…ë°ì´íŠ¸ (Lock ë¶ˆí•„ìš”, asyncio ë‹¨ì¼ ìŠ¤ë ˆë“œ)
            self.processed_chunks_count += 1
            if success:
                self.successful_chunks_count += 1
                # âœ… ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸: translated_chunksì— ì™„ë£Œëœ ì²­í¬ ê¸°ë¡
                try:
                    metadata_updated = update_metadata_for_chunk_completion(
                        input_file_path,
                        chunk_index,
                        source_length=len(chunk_text),
                        translated_length=len(translated_chunk)
                    )
                    if metadata_updated:
                        logger.debug(f"  ğŸ’¾ {current_chunk_info_msg} ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                    else:
                        logger.warning(f"  âš ï¸ {current_chunk_info_msg} ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
                except Exception as meta_e:
                    logger.error(f"  âŒ {current_chunk_info_msg} ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {meta_e}")
            else:
                self.failed_chunks_count += 1
                # âŒ ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ ê¸°ë¡
                if last_error:
                    try:
                        update_metadata_for_chunk_failure(input_file_path, chunk_index, last_error)
                        logger.debug(f"  ğŸ’¾ {current_chunk_info_msg} ì‹¤íŒ¨ ì •ë³´ ë©”íƒ€ë°ì´í„°ì— ê¸°ë¡ ì™„ë£Œ")
                    except Exception as meta_fail_e:
                        logger.error(f"  âŒ {current_chunk_info_msg} ì‹¤íŒ¨ ì •ë³´ ë©”íƒ€ë°ì´í„° ê¸°ë¡ ì¤‘ ì˜¤ë¥˜: {meta_fail_e}")
            
            # ì§„í–‰ë¥  ê³„ì‚° ë° í†µí•© ë¡œê¹… (2ê°œ ë¡œê·¸ â†’ 1ê°œ)
            progress_percentage = (self.processed_chunks_count / total_chunks) * 100
            success_rate = (self.successful_chunks_count / self.processed_chunks_count) * 100 if self.processed_chunks_count > 0 else 0
            
            # ë§¤ 10% ë˜ëŠ” ë§ˆì§€ë§‰ ì²­í¬ì—ì„œë§Œ ìƒì„¸ ë¡œê·¸ ì¶œë ¥ (ë¡œê·¸ ë¹ˆë„ ìµœì í™”)
            should_log_progress = (self.processed_chunks_count % max(1, total_chunks // 10) == 0) or (self.processed_chunks_count == total_chunks)
            if should_log_progress:
                logger.info(f"  ğŸ“ˆ ì§„í–‰ë¥ : {progress_percentage:.0f}% ({self.processed_chunks_count}/{total_chunks}) | ì„±ê³µë¥ : {success_rate:.0f}% (âœ…{self.successful_chunks_count} âŒ{self.failed_chunks_count})")
            
            # ì§„í–‰ë¥  ì½œë°±
            if progress_callback:
                if success:
                    status_msg_for_dto = f"âœ… ì²­í¬ {chunk_index + 1}/{total_chunks} ì™„ë£Œ ({total_time:.1f}ì´ˆ)"
                else:
                    status_msg_for_dto = f"âŒ ì²­í¬ {chunk_index + 1}/{total_chunks} ì‹¤íŒ¨ ({total_time:.1f}ì´ˆ)"
                    if last_error:
                        status_msg_for_dto += f" - {last_error[:50]}..."
                
                progress_dto = TranslationJobProgressDTO(
                    total_chunks=total_chunks,
                    processed_chunks=self.processed_chunks_count,
                    successful_chunks=self.successful_chunks_count,
                    failed_chunks=self.failed_chunks_count,
                    current_status_message=status_msg_for_dto,
                    current_chunk_processing=chunk_index + 1,
                    last_error_message=last_error
                )
                progress_callback(progress_dto)
            
            logger.debug(f"  {current_chunk_info_msg} ì²˜ë¦¬ ì™„ë£Œ ë°˜í™˜: {success}")
            return success

    # ===== ë: ë¹„ë™ê¸° ë©”ì„œë“œ =====

    # === LEGACY SYNC METHODS REMOVED ===
    # ë‹¤ìŒ ë©”ì„œë“œë“¤ì€ ë¹„ë™ê¸° ë§ˆì´ê·¸ë ˆì´ì…˜ìœ¼ë¡œ ì¸í•´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤:
    # - start_translation() (êµ¬ L1304-L1360)
    # - _translation_task() (êµ¬ L1363-L1756)
    # - stop_translation() (êµ¬ L1757-L1775)
    # ëŒ€ì‹  start_translation_async()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    # CLI: asyncio.run(app_service.start_translation_async(...))
    # GUI (PySide6): await app_service.start_translation_async(...) with @asyncSlot







    def request_stop_translation(self):
        """
        ë¹„ë™ê¸° ë²ˆì—­ ì‘ì—… ì·¨ì†Œ (ì¦‰ì‹œ ë°˜ì‘)
        
        Task.cancel()ì„ ì‚¬ìš©í•˜ì—¬ í˜„ì¬ ì§„í–‰ ì¤‘ì¸ asyncio Taskë¥¼ ì¦‰ì‹œ ì·¨ì†Œí•©ë‹ˆë‹¤.
        ê¸°ì¡´ ìŠ¤ë ˆë“œ ê¸°ë°˜ì˜ 5-30ì´ˆ ëŒ€ë¹„ <1ì´ˆë¡œ ê°œì„ ë©ë‹ˆë‹¤.
        """
        if self.current_translation_task and not self.current_translation_task.done():
            logger.info("ë²ˆì—­ ì·¨ì†Œ ìš”ì²­ë¨ (Task.cancel() í˜¸ì¶œ)")
            self.current_translation_task.cancel()
        else:
            logger.warning("í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ë²ˆì—­ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤")

    def translate_single_chunk(
        self,
        input_file_path: Union[str, Path],
        chunk_file_path: Union[str, Path],
        chunk_index: int,
        progress_callback: Optional[Callable[[str], None]] = None
    ) -> Tuple[bool, str]:
        """
        ë‹¨ì¼ ì²­í¬ë¥¼ ì¬ë²ˆì—­í•©ë‹ˆë‹¤.
        
        Args:
            input_file_path: ì›ë³¸ ì…ë ¥ íŒŒì¼ ê²½ë¡œ.
            chunk_file_path: ì—…ë°ì´íŠ¸í•  ì²­í¬ íŒŒì¼ ê²½ë¡œ.
            chunk_index: ì¬ë²ˆì—­í•  ì²­í¬ ì¸ë±ìŠ¤
            progress_callback: ì§„í–‰ ìƒíƒœ ì½œë°± (ìƒíƒœ ë©”ì‹œì§€)
            
        Returns:
            Tuple[bool, str]: (ì„±ê³µ ì—¬ë¶€, ë²ˆì—­ëœ í…ìŠ¤íŠ¸ ë˜ëŠ” ì˜¤ë¥˜ ë©”ì‹œì§€)
        """
        if not self.translation_service:
            error_msg = "TranslationServiceê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            logger.error(error_msg)
            return False, error_msg
        
        input_file_path_obj = Path(input_file_path)
        chunk_file_path_obj = Path(chunk_file_path)
        
        try:
            # 1. ì›ë¬¸ì„ ì›ë³¸ íŒŒì¼ì—ì„œ ë™ì ìœ¼ë¡œ ì²­í‚¹í•˜ì—¬ ë¡œë“œ
            if not input_file_path_obj.exists():
                error_msg = f"ì›ë³¸ ì…ë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_file_path_obj}"
                logger.error(error_msg)
                return False, error_msg
            
            file_content = read_text_file(input_file_path_obj)
            if not file_content:
                error_msg = "ì›ë³¸ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
                logger.error(error_msg)
                return False, error_msg
            
            chunk_size = self.config.get('chunk_size', 6000)
            all_chunks = self.chunk_service.create_chunks_from_file_content(file_content, chunk_size)
            
            if chunk_index >= len(all_chunks):
                error_msg = f"ì²­í¬ #{chunk_index}ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤ (ì´ {len(all_chunks)}ê°œ)."
                logger.error(error_msg)
                return False, error_msg
            
            chunk_text = all_chunks[chunk_index]
            
            if progress_callback:
                progress_callback(f"ì²­í¬ #{chunk_index} ë²ˆì—­ ì¤‘...")
            
            logger.info(f"ë‹¨ì¼ ì²­í¬ ì¬ë²ˆì—­ ì‹œì‘: ì²­í¬ #{chunk_index} (ê¸¸ì´: {len(chunk_text)}ì)")
            
            # 2. ë²ˆì—­ ì„¤ì • ë¡œë“œ
            use_content_safety_retry = self.config.get("use_content_safety_retry", True)
            max_split_attempts = self.config.get("max_content_safety_split_attempts", 3)
            min_chunk_size = self.config.get("min_content_safety_chunk_size", 100)
            
            # 3. ìš©ì–´ì§‘ ë™ì  ë¡œë”© (ì…ë ¥ íŒŒì¼ì— ë§ëŠ” ìš©ì–´ì§‘ ìë™ ë°œê²¬)
            try:
                glossary_suffix = self.config.get("glossary_output_json_filename_suffix", "_simple_glossary.json")
                assumed_glossary_path = input_file_path_obj.parent / f"{input_file_path_obj.stem}{glossary_suffix}"
                
                if assumed_glossary_path.exists():
                    self.config['glossary_json_path'] = str(assumed_glossary_path)
                    self.translation_service.config = self.config
                    self.translation_service._load_glossary_data()
                    logger.debug(f"ì¬ë²ˆì—­ì„ ìœ„í•´ ìš©ì–´ì§‘ ë¡œë“œ: {assumed_glossary_path.name}")
            except Exception as e_glossary:
                logger.warning(f"ìš©ì–´ì§‘ ë¡œë”© ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e_glossary}")
            
            # 4. ë²ˆì—­ ìˆ˜í–‰ (ë¹„ë™ê¸° ë²„ì „ ì‚¬ìš©)
            start_time = time.time()
            
            # asyncio.run()ì„ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸° ë©”ì„œë“œ í˜¸ì¶œ
            if use_content_safety_retry:
                translated_text = asyncio.run(
                    self.translation_service.translate_text_with_content_safety_retry_async(
                        chunk_text, max_split_attempts, min_chunk_size
                    )
                )
            else:
                translated_text = asyncio.run(
                    self.translation_service.translate_text_async(chunk_text)
                )
            
            translation_time = time.time() - start_time
            
            if not translated_text:
                error_msg = "ë²ˆì—­ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
                logger.error(f"ì²­í¬ #{chunk_index} ì¬ë²ˆì—­ ì‹¤íŒ¨: {error_msg}")
                return False, error_msg
            
            # 5. ë²ˆì—­ëœ ì²­í¬ íŒŒì¼ ì—…ë°ì´íŠ¸ (ì „ë‹¬ë°›ì€ chunk_file_path_obj ì‚¬ìš©)
            translated_chunked_path = chunk_file_path_obj
            
            # ê¸°ì¡´ ë²ˆì—­ëœ ì²­í¬ ë¡œë“œ
            translated_chunks = {}
            if translated_chunked_path.exists():
                translated_chunks = load_chunks_from_file(translated_chunked_path)
            
            # í•´ë‹¹ ì²­í¬ ì—…ë°ì´íŠ¸
            translated_chunks[chunk_index] = translated_text
            
            # íŒŒì¼ì— ì €ì¥
            save_merged_chunks_to_file(translated_chunked_path, translated_chunks)
            
            # 6. ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            update_metadata_for_chunk_completion(
                input_file_path_obj,
                chunk_index,
                source_length=len(chunk_text),
                translated_length=len(translated_text)
            )
            
            logger.info(f"ì²­í¬ #{chunk_index} ì¬ë²ˆì—­ ì™„ë£Œ ({translation_time:.2f}ì´ˆ, {len(translated_text)}ì)")
            
            if progress_callback:
                progress_callback(f"ì²­í¬ #{chunk_index} ì¬ë²ˆì—­ ì™„ë£Œ!")
            
            return True, translated_text
            
        except BtgTranslationException as e_trans:
            error_msg = f"ë²ˆì—­ ì˜¤ë¥˜: {e_trans}"
            logger.error(f"ì²­í¬ #{chunk_index} ì¬ë²ˆì—­ ì‹¤íŒ¨: {error_msg}")
            
            # ì‹¤íŒ¨ ì •ë³´ ê¸°ë¡
            try:
                update_metadata_for_chunk_failure(input_file_path_obj, chunk_index, str(e_trans))
            except Exception:
                pass
                
            return False, error_msg
            
        except BtgApiClientException as e_api:
            error_msg = f"API ì˜¤ë¥˜: {e_api}"
            logger.error(f"ì²­í¬ #{chunk_index} ì¬ë²ˆì—­ ì‹¤íŒ¨: {error_msg}")
            
            try:
                update_metadata_for_chunk_failure(input_file_path_obj, chunk_index, str(e_api))
            except Exception:
                pass
                
            return False, error_msg
            
        except Exception as e_gen:
            error_msg = f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e_gen}"
            logger.error(f"ì²­í¬ #{chunk_index} ì¬ë²ˆì—­ ì‹¤íŒ¨: {error_msg}", exc_info=True)
            
            try:
                update_metadata_for_chunk_failure(input_file_path_obj, chunk_index, str(e_gen))
            except Exception:
                pass
                
            return False, error_msg


if __name__ == '__main__':
    import logging
    from logging import DEBUG # type: ignore

    logger.setLevel(DEBUG)

    test_output_dir = Path("test_app_service_output")
    test_output_dir.mkdir(exist_ok=True)

    temp_config_path = test_output_dir / "temp_config.json"
    sample_app_config_data = {
        "api_key": os.environ.get("GOOGLE_API_KEY", "YOUR_DEFAULT_API_KEY_FOR_TEST"),
        "service_account_file_path": None, 
        "use_vertex_ai": False, 
        "gcp_project": None, 
        "gcp_location": None, 
        "model_name": "gemini-2.0-flash",
        "temperature": 0.7,
        "top_p": 0.9,
        "prompts": "Translate to Korean: {{slot}}",
        "chunk_size": 50, 
        "glossary_json_path": str(test_output_dir / "sample_glossary.json"), # Changed from pronouns_csv
        "requests_per_minute": 60,
        # "max_glossary_entries": 5, # ìš©ì–´ì§‘ìœ¼ë¡œ ëŒ€ì²´ë˜ë©´ì„œ ì´ ì„¤ì •ì€ ì§ì ‘ ì‚¬ìš©ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
        "glossary_sampling_ratio": 100.0, # Changed from pronoun_sample_ratio
        "max_workers": 2 
    }

    with open(temp_config_path, "w", encoding="utf-8") as f:
        json.dump(sample_app_config_data, f, indent=4)

    sample_glossary_file = test_output_dir / "sample_glossary.json" # Changed from sample_pronoun_file
    sample_glossary_content = [
        {"keyword": "BTG", "translated_keyword": "ë¹„í‹°ì§€", "source_language": "en", "target_language": "ko", "occurrence_count": 10},
        {"keyword": "Gemini", "translated_keyword": "ì œë¯¸ë‹ˆ", "source_language": "en", "target_language": "ko", "occurrence_count": 5}
    ]
    with open(sample_glossary_file, "w", encoding="utf-8") as f: # Changed to JSON
        json.dump(sample_glossary_content, f, indent=4, ensure_ascii=False)

    temp_input_file = test_output_dir / "sample_input.txt"
    temp_input_content = (
        "Hello BTG.\nThis is a test for the Gemini API.\n"
        "We are testing the application service layer.\n"
        "Another line for chunking. And one more for Gemini.\n"
        "This is the fifth line.\nAnd the sixth line is here.\n"
        "Seventh line for more data.\nEighth line, almost done."
    )
    with open(temp_input_file, "w", encoding="utf-8") as f:
        f.write(temp_input_content)

    temp_output_file = test_output_dir / "sample_output.txt"
    
    temp_metadata_file = get_metadata_file_path(temp_input_file)
    if temp_metadata_file.exists():
        delete_file(temp_metadata_file)
    if temp_output_file.exists():
        delete_file(temp_output_file)


    app_service: Optional[AppService] = None
    try:
        app_service = AppService(config_file_path=temp_config_path)
        logger.info("AppService ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì„±ê³µ.")
    except Exception as e:
        logger.error(f"AppService ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
        exit()

    if app_service and app_service.gemini_client:
        print("\n--- ëª¨ë¸ ëª©ë¡ ì¡°íšŒ í…ŒìŠ¤íŠ¸ ---")
        try:
            models = app_service.get_available_models()
            if models:
                logger.info(f"ì¡°íšŒëœ ëª¨ë¸ ìˆ˜: {len(models)}")
                for m in models[:2]: 
                    logger.info(f"  - {m.get('display_name', m.get('name'))}")
            else:
                logger.info("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        except BtgApiClientException as e:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        except Exception as e_models:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e_models}", exc_info=True)

    else:
        logger.warning("Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

    if app_service and app_service.glossary_service: # Changed from pronoun_service
        print("\n--- ìš©ì–´ì§‘ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ---") # Changed
        try:
            def _glossary_progress_dto_cb(dto: GlossaryExtractionProgressDTO): # Changed DTO and function name
                logger.debug(f"ìš©ì–´ì§‘ ì§„í–‰ DTO: {dto.processed_segments}/{dto.total_segments} - {dto.current_status_message} (ì¶”ì¶œ í•­ëª©: {dto.extracted_entries_count})") # Changed fields
        
            result_path = app_service.extract_glossary( # Changed method
                temp_input_file,
                progress_callback=_glossary_progress_dto_cb, # Changed callback
                seed_glossary_path=sample_glossary_file # Optionally provide a seed path for testing
            )
            logger.info(f"ìš©ì–´ì§‘ ì¶”ì¶œ ì™„ë£Œ, ê²°ê³¼ íŒŒì¼: {result_path}") # Changed
        except Exception as e:
            logger.error(f"ìš©ì–´ì§‘ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True) # Changed
    else:
        logger.warning("Glossary ì„œë¹„ìŠ¤ê°€ ì—†ì–´ ìš©ì–´ì§‘ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.") # Changed

    if app_service and app_service.translation_service and app_service.gemini_client: # Ensure client exists for translation
        print("\n--- ë²ˆì—­ í…ŒìŠ¤íŠ¸ (ë³‘ë ¬ ì²˜ë¦¬) ---")
        try:
            test_tqdm_stream = sys.stdout 

            def _trans_progress_dto(dto: TranslationJobProgressDTO):
                logger.debug(f"ë²ˆì—­ ì§„í–‰ DTO: {dto.current_chunk_processing or '-'}/{dto.total_chunks}, ì„±ê³µ: {dto.successful_chunks}, ì‹¤íŒ¨: {dto.failed_chunks} - {dto.current_status_message}")
                pass

            def _trans_status(status_msg):
                logger.info(f"ë²ˆì—­ ìƒíƒœ: {status_msg}")


            app_service.start_translation(
                temp_input_file,
                temp_output_file,
                _trans_progress_dto,
                _trans_status,
                tqdm_file_stream=test_tqdm_stream 
            )

            start_time = time.time()
            while app_service.is_translation_running and (time.time() - start_time) < 120: 
                time.sleep(0.5)

            if app_service.is_translation_running:
                logger.warning("ë²ˆì—­ ì‘ì—…ì´ ì‹œê°„ ë‚´ì— ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (í…ŒìŠ¤íŠ¸). ì¤‘ì§€ ìš”ì²­...")
                app_service.request_stop_translation()
                time.sleep(2) 

            if temp_output_file.exists():
                logger.info(f"ë²ˆì—­ ì™„ë£Œ, ê²°ê³¼ íŒŒì¼: {temp_output_file}")
            else:
                logger.error("ë²ˆì—­ ê²°ê³¼ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"ë²ˆì—­ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True)
    else:
        logger.warning("Translation ì„œë¹„ìŠ¤ê°€ ì—†ì–´ ë²ˆì—­ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

    logger.info("AppService í…ŒìŠ¤íŠ¸ ì™„ë£Œ.")
