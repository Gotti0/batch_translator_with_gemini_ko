# app_service.py
from pathlib import Path
# typing ëª¨ë“ˆì—ì„œ Tupleì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from typing import Dict, Any, Optional, List, Callable, Union, Tuple
import os
import json
import csv
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from tqdm import tqdm # tqdm ì„í¬íŠ¸ í™•ì¸
import sys # sys ì„í¬íŠ¸ í™•ì¸ (tqdm_file_stream=sys.stdout ì— ì‚¬ìš©ë  ìˆ˜ ìˆìŒ)

try:
    from .logger_config import setup_logger
except ImportError:
    from logger_config import setup_logger

try:
    # file_handlerì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ë“¤ì„ import í•©ë‹ˆë‹¤.
    from .file_handler import (
        read_text_file, write_text_file,
        save_chunk_with_index_to_file, get_metadata_file_path, delete_file,
        load_chunks_from_file,
        create_new_metadata, save_metadata, load_metadata,
        update_metadata_for_chunk_completion,
        _hash_config_for_metadata,
        save_merged_chunks_to_file
    )
    from .config_manager import ConfigManager
    from .gemini_client import GeminiClient, GeminiAllApiKeysExhaustedException, GeminiInvalidRequestException
    from .translation_service import TranslationService # Keep
    from .lorebook_service import LorebookService 
    from .chunk_service import ChunkService
    from .exceptions import BtgServiceException, BtgConfigException, BtgFileHandlerException, BtgApiClientException, BtgTranslationException, BtgBusinessLogicException
    from .dtos import TranslationJobProgressDTO, LorebookExtractionProgressDTO # DTO ì„í¬íŠ¸ í™•ì¸
    from .post_processing_service import PostProcessingService
except ImportError:
    # Fallback imports
    from file_handler import (
        read_text_file, write_text_file,
        save_chunk_with_index_to_file, get_metadata_file_path, delete_file,
        load_chunks_from_file,
        create_new_metadata, save_metadata, load_metadata,
        update_metadata_for_chunk_completion,
        _hash_config_for_metadata,
        save_merged_chunks_to_file
    )
    from config_manager import ConfigManager
    from gemini_client import GeminiClient, GeminiAllApiKeysExhaustedException, GeminiInvalidRequestException
    from translation_service import TranslationService
    from lorebook_service import LorebookService
    from chunk_service import ChunkService
    from exceptions import BtgServiceException, BtgConfigException, BtgFileHandlerException, BtgApiClientException, BtgTranslationException, BtgBusinessLogicException
    from dtos import TranslationJobProgressDTO, LorebookExtractionProgressDTO # DTO ì„í¬íŠ¸ í™•ì¸
    from post_processing_service import PostProcessingService

logger = setup_logger(__name__)

class AppService:
    """
    ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì£¼ìš” ìœ ìŠ¤ì¼€ì´ìŠ¤ë¥¼ ì¡°ì •í•˜ëŠ” ì„œë¹„ìŠ¤ ê³„ì¸µì…ë‹ˆë‹¤.
    í”„ë ˆì  í…Œì´ì…˜ ê³„ì¸µê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§/ì¸í”„ë¼ ê³„ì¸µ ê°„ì˜ ì¸í„°í˜ì´ìŠ¤ ì—­í• ì„ í•©ë‹ˆë‹¤.
    """

    def __init__(self, config_file_path: Optional[Union[str, Path]] = None):
        self.config_manager = ConfigManager(config_file_path)
        self.config: Dict[str, Any] = {}
        self.gemini_client: Optional[GeminiClient] = None
        self.translation_service: Optional[TranslationService] = None
        self.lorebook_service: Optional[LorebookService] = None # Renamed from pronoun_service
        self.chunk_service = ChunkService()

        self.is_translation_running = False
        self.stop_requested = False
        self._translation_lock = threading.Lock()
        self._progress_lock = threading.Lock()
        self.processed_chunks_count = 0
        self.successful_chunks_count = 0
        self.failed_chunks_count = 0
        self.post_processing_service = PostProcessingService()

        self.load_app_config()

    def load_app_config(self) -> Dict[str, Any]:
        logger.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ë¡œë“œ ì¤‘...")
        try:
            self.config = self.config_manager.load_config()
            logger.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ë¡œë“œ ì™„ë£Œ.")

            auth_credentials_for_gemini_client: Optional[Union[str, List[str], Dict[str, Any]]] = None
            use_vertex = self.config.get("use_vertex_ai", False)
            gcp_project_from_config = self.config.get("gcp_project")
            gcp_location = self.config.get("gcp_location")
            sa_file_path_str = self.config.get("service_account_file_path")

            logger.debug(f"[AppService.load_app_config] Vertex AI ì‚¬ìš© ì—¬ë¶€ (use_vertex): {use_vertex}")
            logger.debug(f"[AppService.load_app_config] ì„¤ì • íŒŒì¼ ë‚´ GCP í”„ë¡œì íŠ¸ (gcp_project_from_config): '{gcp_project_from_config}'")
            logger.debug(f"[AppService.load_app_config] ì„¤ì • íŒŒì¼ ë‚´ GCP ìœ„ì¹˜ (gcp_location): '{gcp_location}'")
            logger.debug(f"[AppService.load_app_config] ì„œë¹„ìŠ¤ ê³„ì • íŒŒì¼ ê²½ë¡œ (sa_file_path_str): '{sa_file_path_str}'")

            if use_vertex:
                logger.info("Vertex AI ì‚¬ìš© ëª¨ë“œì…ë‹ˆë‹¤.")
                if sa_file_path_str:
                    sa_file_path = Path(sa_file_path_str)
                    if sa_file_path.is_file():
                        try:
                            auth_credentials_for_gemini_client = read_text_file(sa_file_path)
                            logger.info(f"Vertex AI ì„œë¹„ìŠ¤ ê³„ì • íŒŒì¼ ë¡œë“œ ì„±ê³µ: {sa_file_path}")
                        except Exception as e:
                            logger.error(f"Vertex AI ì„œë¹„ìŠ¤ ê³„ì • íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({sa_file_path}): {e}")
                            auth_credentials_for_gemini_client = None
                    else:
                        logger.warning(f"Vertex AI ì„œë¹„ìŠ¤ ê³„ì • íŒŒì¼ ê²½ë¡œê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤: {sa_file_path_str}")
                        auth_credentials_for_gemini_client = self.config.get("auth_credentials")
                        if auth_credentials_for_gemini_client:
                             logger.info("ì„œë¹„ìŠ¤ ê³„ì • íŒŒì¼ ê²½ë¡œê°€ ìœ íš¨í•˜ì§€ ì•Šì•„ 'auth_credentials' ê°’ì„ ì§ì ‘ ì‚¬ìš© ì‹œë„í•©ë‹ˆë‹¤.")
                elif self.config.get("auth_credentials"):
                    auth_credentials_for_gemini_client = self.config.get("auth_credentials")
                    logger.info("Vertex AI ì‚¬ìš©ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìœ¼ë‚˜ ì„œë¹„ìŠ¤ ê³„ì • íŒŒì¼ ê²½ë¡œê°€ ì—†ì–´, 'auth_credentials' ê°’ì„ ì§ì ‘ ì‚¬ìš© ì‹œë„í•©ë‹ˆë‹¤.")
            else:
                logger.info("Gemini Developer API ì‚¬ìš© ëª¨ë“œì…ë‹ˆë‹¤.")
                api_keys_list = self.config.get("api_keys", [])
                if api_keys_list:
                    auth_credentials_for_gemini_client = api_keys_list
                    logger.info(f"{len(api_keys_list)}ê°œì˜ API í‚¤ ëª©ë¡ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                elif self.config.get("api_key"):
                    auth_credentials_for_gemini_client = [self.config.get("api_key")]
                    logger.info("ë‹¨ì¼ API í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (api_keys ëª©ë¡ì´ ë¹„ì–´ ìˆìŒ).")
                elif self.config.get("auth_credentials"):
                    auth_credentials_for_gemini_client = [self.config.get("auth_credentials")]
                    logger.info("auth_credentials ê°’ì„ API í‚¤ë¡œ ì‚¬ìš© ì‹œë„í•©ë‹ˆë‹¤ (api_key, api_keys ëª¨ë‘ ì—†ìŒ).")
                else:
                    logger.warning("Gemini Developer API ëª¨ë“œì´ì§€ë§Œ ì‚¬ìš©í•  API í‚¤ê°€ ì„¤ì •ì— ì—†ìŠµë‹ˆë‹¤.")
                    auth_credentials_for_gemini_client = None

            should_initialize_client = False
            if auth_credentials_for_gemini_client:
                if isinstance(auth_credentials_for_gemini_client, str) and auth_credentials_for_gemini_client.strip():
                    should_initialize_client = True
                elif isinstance(auth_credentials_for_gemini_client, list) and auth_credentials_for_gemini_client:
                    should_initialize_client = True
                elif isinstance(auth_credentials_for_gemini_client, dict):
                    should_initialize_client = True
            elif use_vertex and not auth_credentials_for_gemini_client and \
                 (gcp_project_from_config or os.environ.get("GOOGLE_CLOUD_PROJECT")): # ADC ì‚¬ìš© ê¸°ëŒ€
                should_initialize_client = True
                logger.info("Vertex AI ì‚¬ìš© ë° í”„ë¡œì íŠ¸ ID ì¡´ì¬ (ì„¤ì • ë˜ëŠ” í™˜ê²½ë³€ìˆ˜)ë¡œ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¡°ê±´ ì¶©ì¡± (ì¸ì¦ì •ë³´ëŠ” ADC ê¸°ëŒ€).")


            logger.debug(f"[AppService.load_app_config] GeminiClient ì´ˆê¸°í™” ì „: should_initialize_client={should_initialize_client}")
            logger.debug(f"[AppService.load_app_config] auth_credentials_for_gemini_client íƒ€ì…: {type(auth_credentials_for_gemini_client)}")
            if isinstance(auth_credentials_for_gemini_client, str) and len(auth_credentials_for_gemini_client) > 200:
                 logger.debug(f"[AppService.load_app_config] auth_credentials_for_gemini_client (ì¼ë¶€): {auth_credentials_for_gemini_client[:100]}...{auth_credentials_for_gemini_client[-100:]}")
            elif isinstance(auth_credentials_for_gemini_client, dict):
                 logger.debug(f"[AppService.load_app_config] auth_credentials_for_gemini_client (í‚¤ ëª©ë¡): {list(auth_credentials_for_gemini_client.keys())}")
            else:
                 logger.debug(f"[AppService.load_app_config] auth_credentials_for_gemini_client: {auth_credentials_for_gemini_client}")

            if should_initialize_client:
                try:
                    project_to_pass_to_client = gcp_project_from_config if gcp_project_from_config and gcp_project_from_config.strip() else None
                    rpm_value = self.config.get("requests_per_minute")
                    logger.info(f"GeminiClient ì´ˆê¸°í™” ì‹œë„: project='{project_to_pass_to_client}', location='{gcp_location}', RPM='{rpm_value}'")
                    self.gemini_client = GeminiClient(
                        auth_credentials=auth_credentials_for_gemini_client,
                        project=project_to_pass_to_client,
                        location=gcp_location,
                        requests_per_minute=rpm_value
                    )
                except GeminiInvalidRequestException as e_inv:
                    logger.error(f"GeminiClient ì´ˆê¸°í™” ì‹¤íŒ¨ (ì˜ëª»ëœ ìš”ì²­/ì¸ì¦): {e_inv}")
                    self.gemini_client = None
                except Exception as e_client:
                    logger.error(f"GeminiClient ì´ˆê¸°í™” ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e_client}", exc_info=True)
                    self.gemini_client = None
            else:
                logger.warning("API í‚¤ ë˜ëŠ” Vertex AI ì„¤ì •ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”ë¥¼ ì‹œë„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                self.gemini_client = None

            if self.gemini_client:
                self.translation_service = TranslationService(self.gemini_client, self.config)
                self.lorebook_service = LorebookService(self.gemini_client, self.config) # Changed to LorebookService
                logger.info("TranslationService ë° LorebookServiceê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.") # Message updated
            else:
                self.translation_service = None
                self.lorebook_service = None # Renamed
                logger.warning("Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ë²ˆì—­ ë° ê³ ìœ ëª…ì‚¬ ì„œë¹„ìŠ¤ê°€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

            return self.config
        except FileNotFoundError as e:
            logger.error(f"ì„¤ì • íŒŒì¼ ì°¾ê¸° ì‹¤íŒ¨: {e}")
            self.config = self.config_manager.get_default_config()
            logger.warning("ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤. Gemini í´ë¼ì´ì–¸íŠ¸ëŠ” ì´ˆê¸°í™”ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            self.gemini_client = None
            self.translation_service = None # Keep
            self.lorebook_service = None # Renamed
            return self.config
        except Exception as e:
            logger.error(f"ì„¤ì • ë¡œë“œ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            raise BtgConfigException(f"ì„¤ì • ë¡œë“œ ì˜¤ë¥˜: {e}", original_exception=e) from e

    def save_app_config(self, config_data: Dict[str, Any]) -> bool:
        logger.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ì €ì¥ ì¤‘...")
        try:
            success = self.config_manager.save_config(config_data)
            if success:
                logger.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ì €ì¥ ì™„ë£Œ.")
                self.load_app_config()
            return success
        except Exception as e:
            logger.error(f"ì„¤ì • ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise BtgConfigException(f"ì„¤ì • ì €ì¥ ì˜¤ë¥˜: {e}", original_exception=e) from e

    def get_available_models(self) -> List[Dict[str, Any]]:
        if not self.gemini_client:
            logger.error("ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            raise BtgServiceException("Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ ë˜ëŠ” Vertex AI ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        logger.info("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì„œë¹„ìŠ¤ í˜¸ì¶œë¨.")
        try:
            all_models = self.gemini_client.list_models()
            # ëª¨ë¸ í•„í„°ë§ ë¡œì§ ì œê±°ë¨
            logger.info(f"ì´ {len(all_models)}ê°œì˜ ëª¨ë¸ì„ APIë¡œë¶€í„° ì§ì ‘ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return all_models
            
        except BtgApiClientException as e:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ API ì˜¤ë¥˜: {e}")
            raise
        except Exception as e:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
            raise BtgServiceException(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}", original_exception=e) from e

    def extract_lorebook( # Renamed from extract_pronouns
        self,
        input_file_path: Union[str, Path],
        progress_callback: Optional[Callable[[LorebookExtractionProgressDTO], None]] = None, # DTO Changed
        novel_language_code: Optional[str] = None, # ëª…ì‹œì  ì–¸ì–´ ì½”ë“œ ì „ë‹¬
        seed_lorebook_path: Optional[Union[str, Path]] = None # CLIì—ì„œ ì „ë‹¬ëœ ì‹œë“œ ë¡œì–´ë¶ ê²½ë¡œ
        # tqdm_file_stream is not typically used by lorebook extraction directly in AppService,
        # but can be passed down if LorebookService supports it (currently it doesn't directly)
        # For CLI, tqdm is handled in the CLI module itself.
    ) -> Path:
        if not self.lorebook_service: # Changed from pronoun_service
            logger.error("ë¡œì–´ë¶ ì¶”ì¶œ ì„œë¹„ìŠ¤ ì‹¤íŒ¨: ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.") # Message updated
            raise BtgServiceException("ë¡œì–´ë¶ ì¶”ì¶œ ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.") # Message updated

        logger.info(f"ë¡œì–´ë¶ ì¶”ì¶œ ì„œë¹„ìŠ¤ ì‹œì‘: {input_file_path}, ì‹œë“œ íŒŒì¼: {seed_lorebook_path}") # Message updated
        try:
            file_content = read_text_file(input_file_path)
            if not file_content:
                logger.warning(f"ì…ë ¥ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {input_file_path}")
                # For lorebook, an empty input means an empty lorebook, unless a seed is provided.
                # LorebookService.extract_and_save_lorebook handles empty content.

            # ë¡œì–´ë¶ ì¶”ì¶œ ì‹œ ì‚¬ìš©í•  ì–¸ì–´ ì½”ë“œ ê²°ì •
            # 1. ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬ëœ novel_language_code
            # 2. ì„¤ì • íŒŒì¼ì˜ novel_language (í†µí•©ë¨)
            # 3. None (LorebookServiceì—ì„œ ìì²´ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê±°ë‚˜ ì–¸ì–´ íŠ¹ì • ê¸°ëŠ¥ ë¹„í™œì„±í™”)
            lang_code_for_extraction = novel_language_code or self.config.get("novel_language") # í†µí•©ëœ ì„¤ì • ì‚¬ìš©
            result_path = self.lorebook_service.extract_and_save_lorebook( # Method changed
                file_content, # Pass content directly
                input_file_path, 
                lang_code_for_extraction, # ê²°ì •ëœ ì–¸ì–´ ì½”ë“œ ì „ë‹¬
                progress_callback, # ì½œë°± ìœ„ì¹˜ ë³€ê²½
                seed_lorebook_path=seed_lorebook_path # ì‹œë“œ ë¡œì–´ë¶ ê²½ë¡œ ì „ë‹¬
            )
            logger.info(f"ë¡œì–´ë¶ ì¶”ì¶œ ì™„ë£Œ. ê²°ê³¼ íŒŒì¼: {result_path}") # Message updated

            return result_path
        except FileNotFoundError as e:
            logger.error(f"ë¡œì–´ë¶ ì¶”ì¶œì„ ìœ„í•œ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file_path}") # Message updated
            if progress_callback:
                progress_callback(LorebookExtractionProgressDTO(0,0,f"ì˜¤ë¥˜: ì…ë ¥ íŒŒì¼ ì—†ìŒ - {e.filename}",0)) # DTO Changed
            raise BtgFileHandlerException(f"ì…ë ¥ íŒŒì¼ ì—†ìŒ: {input_file_path}", original_exception=e) from e
        except (BtgBusinessLogicException, BtgApiClientException) as e: # BtgPronounException replaced with BtgBusinessLogicException
            logger.error(f"ë¡œì–´ë¶ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}") # Message updated
            if progress_callback:
                progress_callback(LorebookExtractionProgressDTO(0,0,f"ì˜¤ë¥˜: {e}",0)) # DTO Changed
            raise
        except Exception as e: 
            logger.error(f"ë¡œì–´ë¶ ì¶”ì¶œ ì„œë¹„ìŠ¤ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)  # Message updated
            if progress_callback:
                progress_callback(LorebookExtractionProgressDTO(0,0,f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}",0)) # DTO Changed
            raise BtgServiceException(f"ë¡œì–´ë¶ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}", original_exception=e) from e # Message updated


    def _translate_and_save_chunk(self, chunk_index: int, chunk_text: str,
                            current_run_output_file: Path,
                            total_chunks: int,
                            input_file_path_for_metadata: Path,
                            progress_callback: Optional[Callable[[TranslationJobProgressDTO], None]] = None) -> bool:
        current_chunk_info_msg = f"ì²­í¬ {chunk_index + 1}/{total_chunks}"
        
        # ì²­í¬ ë¶„ì„ ë° ìƒì„¸ ì •ë³´ ë¡œê¹…
        chunk_lines = chunk_text.count('\n') + 1
        chunk_words = len(chunk_text.split())
        chunk_chars = len(chunk_text)
        chunk_preview = chunk_text[:100].replace('\n', ' ') + '...' if len(chunk_text) > 100 else chunk_text
        
        logger.info(f"{current_chunk_info_msg} ì²˜ë¦¬ ì‹œì‘")
        logger.info(f"  ğŸ“ ì²­í¬ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {chunk_preview}")
        logger.debug(f"  ğŸ“Š ì²­í¬ í†µê³„: ê¸€ì ìˆ˜={chunk_chars}, ë‹¨ì–´ ìˆ˜={chunk_words}, ì¤„ ìˆ˜={chunk_lines}")
        
        start_time = time.time()
        last_error = None
        success = False

        

        try:
            if self.stop_requested:
                logger.info(f"{current_chunk_info_msg} â¸ï¸ ì²˜ë¦¬ ì¤‘ì§€ë¨ (ì‚¬ìš©ì ìš”ì²­)")
                return False

            if not self.translation_service:
                raise BtgServiceException("TranslationServiceê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            # ë²ˆì—­ ì„¤ì • ì •ë³´ ë¡œê¹…
            use_content_safety_retry = self.config.get("use_content_safety_retry", True)
            max_split_attempts = self.config.get("max_content_safety_split_attempts", 3)
            min_chunk_size = self.config.get("min_content_safety_chunk_size", 100)
            model_name = self.config.get("model_name", "gemini-1.5-flash-latest")
            
            logger.debug(f"  âš™ï¸ ë²ˆì—­ ì„¤ì •: ëª¨ë¸={model_name}, ì•ˆì „ì¬ì‹œë„={use_content_safety_retry}")
            if use_content_safety_retry:
                logger.debug(f"  ğŸ”„ ê²€ì—´ ì¬ì‹œë„ ì„¤ì •: ìµœëŒ€ì‹œë„={max_split_attempts}, ìµœì†Œí¬ê¸°={min_chunk_size}")
                logger.info(f"  ğŸ”„ {current_chunk_info_msg} ë²ˆì—­ API í˜¸ì¶œ ì‹œì‘...")
                translation_start_time = time.time()
                
            if use_content_safety_retry:
                logger.debug(f"  ğŸ›¡ï¸ ì½˜í…ì¸  ì•ˆì „ ì¬ì‹œë„ ëª¨ë“œë¡œ ë²ˆì—­ ì‹œì‘")
                translated_chunk = self.translation_service.translate_text_with_content_safety_retry(
                    chunk_text, max_split_attempts, min_chunk_size
                )
            else:
                logger.debug(f"  ğŸ“ ì¼ë°˜ ë²ˆì—­ ëª¨ë“œë¡œ ë²ˆì—­ ì‹œì‘")
                translated_chunk = self.translation_service.translate_text(chunk_text)
            
            translation_time = time.time() - translation_start_time
            translated_length = len(translated_chunk) if translated_chunk else 0
            
            logger.info(f"  âœ… {current_chunk_info_msg} ë²ˆì—­ ì™„ë£Œ (ì†Œìš”: {translation_time:.2f}ì´ˆ)")
            logger.debug(f"    ë²ˆì—­ ê²°ê³¼ ê¸¸ì´: {translated_length} ê¸€ì")
            logger.debug(f"    ë²ˆì—­ ì†ë„: {chunk_chars/translation_time:.1f} ê¸€ì/ì´ˆ" if translation_time > 0 else "    ë²ˆì—­ ì†ë„: ì¦‰ì‹œ ì™„ë£Œ")# ìƒˆë¡œìš´ ê²€ì—´ ì¬ì‹œë„ ë¡œì§ ì‚¬ìš©
            # íŒŒì¼ ì €ì¥ ê³¼ì • ë¡œê¹…
            logger.debug(f"  ğŸ’¾ {current_chunk_info_msg} ê²°ê³¼ ì €ì¥ ì‹œì‘...")
            save_start_time = time.time()
            
            save_chunk_with_index_to_file(current_run_output_file, chunk_index, translated_chunk)
            
            save_time = time.time() - save_start_time
            logger.debug(f"  ğŸ’¾ íŒŒì¼ ì €ì¥ ì™„ë£Œ (ì†Œìš”: {save_time:.3f}ì´ˆ)")
            
            success = True
            
            total_processing_time = time.time() - start_time
            logger.info(f"  ğŸ¯ {current_chunk_info_msg} ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ (ì´ ì†Œìš”: {total_processing_time:.2f}ì´ˆ)")

        except BtgTranslationException as e_trans:
            processing_time = time.time() - start_time
            logger.error(f"  âŒ {current_chunk_info_msg} ë²ˆì—­ ì‹¤íŒ¨ (ì†Œìš”: {processing_time:.2f}ì´ˆ)")
            logger.error(f"    ì˜¤ë¥˜ ìœ í˜•: ë²ˆì—­ ì„œë¹„ìŠ¤ ì˜¤ë¥˜")
            logger.error(f"    ì˜¤ë¥˜ ë‚´ìš©: {e_trans}")
            
            if "ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œ" in str(e_trans):
                logger.warning(f"    ğŸ›¡ï¸ ì½˜í…ì¸  ê²€ì—´ë¡œ ì¸í•œ ì‹¤íŒ¨")
            
            save_chunk_with_index_to_file(current_run_output_file, chunk_index, f"[ë²ˆì—­ ì‹¤íŒ¨: {e_trans}]")
            last_error = str(e_trans)
            success = False

        except BtgApiClientException as e_api:
            processing_time = time.time() - start_time
            logger.error(f"  âŒ {current_chunk_info_msg} API ì˜¤ë¥˜ë¡œ ë²ˆì—­ ì‹¤íŒ¨ (ì†Œìš”: {processing_time:.2f}ì´ˆ)")
            logger.error(f"    ì˜¤ë¥˜ ìœ í˜•: API í´ë¼ì´ì–¸íŠ¸ ì˜¤ë¥˜")
            logger.error(f"    ì˜¤ë¥˜ ë‚´ìš©: {e_api}")
            
            # API ì˜¤ë¥˜ ìœ í˜•ë³„ ë¶„ë¥˜
            if "ì‚¬ìš©ëŸ‰ ì œí•œ" in str(e_api) or "429" in str(e_api):
                logger.warning(f"    âš ï¸ API ì‚¬ìš©ëŸ‰ ì œí•œ ì˜¤ë¥˜")
            elif "í‚¤" in str(e_api).lower() or "ì¸ì¦" in str(e_api):
                logger.warning(f"    ğŸ”‘ API ì¸ì¦ ê´€ë ¨ ì˜¤ë¥˜")
            
            save_chunk_with_index_to_file(current_run_output_file, chunk_index, f"[API ì˜¤ë¥˜ë¡œ ë²ˆì—­ ì‹¤íŒ¨: {e_api}]")
            last_error = str(e_api)
            success = False

        except Exception as e_gen:
            processing_time = time.time() - start_time
            logger.error(f"  âŒ {current_chunk_info_msg} ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ (ì†Œìš”: {processing_time:.2f}ì´ˆ)", exc_info=True)
            logger.error(f"    ì˜¤ë¥˜ ìœ í˜•: {type(e_gen).__name__}")
            logger.error(f"    ì˜¤ë¥˜ ë‚´ìš©: {e_gen}")
            
            save_chunk_with_index_to_file(current_run_output_file, chunk_index, f"[ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ë¡œ ë²ˆì—­ ì‹¤íŒ¨: {e_gen}]")
            last_error = str(e_gen)
            success = False
                    

        
        finally:
            total_time = time.time() - start_time
            with self._progress_lock:
                # 1ë‹¨ê³„: ë¨¼ì € processed_chunks_count ì¦ê°€
                self.processed_chunks_count += 1
                
                # 2ë‹¨ê³„: ê²°ê³¼ì— ë”°ë¼ ì„±ê³µ/ì‹¤íŒ¨ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
                if success:
                    self.successful_chunks_count += 1
                    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                elif not self.stop_requested:
                    self.failed_chunks_count += 1
                
                # 3ë‹¨ê³„: ëª¨ë“  ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ í›„ ì§„í–‰ë¥  ê³„ì‚°
                progress_percentage = (self.processed_chunks_count / total_chunks) * 100
                logger.info(f"  ğŸ“ˆ ì „ì²´ ì§„í–‰ë¥ : {progress_percentage:.1f}% ({self.processed_chunks_count}/{total_chunks})")
                
                # ì„±ê³µë¥  ê³„ì‚°
                if self.processed_chunks_count > 0:
                    success_rate = (self.successful_chunks_count / self.processed_chunks_count) * 100
                    logger.info(f"  ğŸ“Š ì„±ê³µë¥ : {success_rate:.1f}% (ì„±ê³µ: {self.successful_chunks_count}, ì‹¤íŒ¨: {self.failed_chunks_count})")

                # ì˜ˆìƒ ì™„ë£Œ ì‹œê°„ ê³„ì‚° (ì„ íƒì‚¬í•­)
                if total_time > 0 and self.processed_chunks_count > 0:
                    avg_time_per_chunk = total_time / 1  # í˜„ì¬ ì²­í¬ ê¸°ì¤€
                    remaining_chunks = total_chunks - self.processed_chunks_count
                    estimated_remaining_time = remaining_chunks * avg_time_per_chunk
                    logger.debug(f"  â±ï¸ ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {estimated_remaining_time:.1f}ì´ˆ (í‰ê·  {avg_time_per_chunk:.2f}ì´ˆ/ì²­í¬)")


                if progress_callback:
                    if success:
                        status_msg_for_dto = f"âœ… ì²­í¬ {chunk_index + 1}/{total_chunks} ì™„ë£Œ ({total_time:.1f}ì´ˆ)"
                    else:
                        status_msg_for_dto = f"âŒ ì²­í¬ {chunk_index + 1}/{total_chunks} ì‹¤íŒ¨ ({total_time:.1f}ì´ˆ)"
                        if last_error:
                            status_msg_for_dto += f" - {last_error[:50]}..."

                    progress_dto = TranslationJobProgressDTO(
                        total_chunks=total_chunks,
                        processed_chunks=self.processed_chunks_count,
                        successful_chunks=self.successful_chunks_count,
                        failed_chunks=self.failed_chunks_count,
                        current_status_message=status_msg_for_dto,
                        current_chunk_processing=chunk_index + 1,
                        last_error_message=last_error
                    )
                    progress_callback(progress_dto)
                
            logger.debug(f"  ğŸ {current_chunk_info_msg} ì²˜ë¦¬ ì™„ë£Œ ë°˜í™˜: {success}")
            return success



    def start_translation(
        self,
        input_file_path: Union[str, Path],
        output_file_path: Union[str, Path],
        progress_callback: Optional[Callable[[TranslationJobProgressDTO], None]] = None,
        status_callback: Optional[Callable[[str], None]] = None,
        tqdm_file_stream: Optional[Any] = None 
    ) -> None:
        if not self.translation_service or not self.chunk_service:
            logger.error("ë²ˆì—­ ì„œë¹„ìŠ¤ ì‹¤íŒ¨: ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            if status_callback: status_callback("ì˜¤ë¥˜: ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨")
            raise BtgServiceException("ë²ˆì—­ ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")

        with self._translation_lock:
            if self.is_translation_running:
                logger.warning("ë²ˆì—­ ì‘ì—…ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
                if status_callback: status_callback("ê²½ê³ : ë²ˆì—­ ì‘ì—… ì´ë¯¸ ì‹¤í–‰ ì¤‘")
                return
            self.is_translation_running = True
            self.stop_requested = False
            self.processed_chunks_count = 0
            self.successful_chunks_count = 0
            self.failed_chunks_count = 0

        logger.info(f"ë²ˆì—­ ì„œë¹„ìŠ¤ ì‹œì‘: ì…ë ¥={input_file_path}, ìµœì¢… ì¶œë ¥={output_file_path}")
        if status_callback: status_callback("ë²ˆì—­ ì‹œì‘ë¨...")

        input_file_path_obj = Path(input_file_path)
        final_output_file_path_obj = Path(output_file_path)
        metadata_file_path = get_metadata_file_path(input_file_path_obj)
        loaded_metadata: Dict[str, Any] = {}
        resume_translation = False
        total_chunks = 0 

        try:
            with self._progress_lock: 
                if metadata_file_path.exists():
                    try:
                        loaded_metadata = load_metadata(metadata_file_path)
                        if loaded_metadata: logger.info(f"ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì„±ê³µ: {metadata_file_path}")
                        else: logger.warning(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ '{metadata_file_path}'ì´ ë¹„ì–´ìˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
                    except json.JSONDecodeError as json_err:
                        logger.error(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ '{metadata_file_path}' ì†ìƒ (JSONDecodeError): {json_err}. ìƒˆë¡œ ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                        delete_file(metadata_file_path) 
                    except Exception as e: 
                        logger.error(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ '{metadata_file_path}' ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}. ìƒˆë¡œ ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤.", exc_info=True)
                        delete_file(metadata_file_path) 
                else:
                    logger.info(f"ê¸°ì¡´ ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {metadata_file_path}. ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")

            current_config_hash = _hash_config_for_metadata(self.config)
            previous_config_hash = loaded_metadata.get("config_hash")

            if previous_config_hash and previous_config_hash == current_config_hash:
                resume_translation = True
                logger.info("ì„¤ì • í•´ì‹œê°€ ì¼ì¹˜í•˜ì—¬ ì´ì–´í•˜ê¸° ëª¨ë“œë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
            elif previous_config_hash: 
                logger.warning("í˜„ì¬ ì„¤ì •ì´ ì´ì „ ë©”íƒ€ë°ì´í„°ì˜ ì„¤ì •ê³¼ ë‹¤ë¦…ë‹ˆë‹¤. ìƒˆë¡œ ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                loaded_metadata = {} 
                resume_translation = False
            
            file_content = read_text_file(input_file_path_obj)
            if not file_content:
                logger.warning(f"ì…ë ¥ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {input_file_path_obj}")
                if status_callback: status_callback("ì™„ë£Œ: ì…ë ¥ íŒŒì¼ ë¹„ì–´ìˆìŒ")
                with self._progress_lock:
                    if not loaded_metadata.get("config_hash"): 
                         loaded_metadata = create_new_metadata(input_file_path_obj, 0, self.config)
                    loaded_metadata["status"] = "completed"; loaded_metadata["total_chunks"] = 0
                    loaded_metadata["last_updated"] = time.time()
                    save_metadata(metadata_file_path, loaded_metadata)
                if progress_callback: progress_callback(TranslationJobProgressDTO(0,0,0,0,"ì…ë ¥ íŒŒì¼ ë¹„ì–´ìˆìŒ"))
                with self._translation_lock: self.is_translation_running = False
                return

            all_chunks: List[str] = self.chunk_service.create_chunks_from_file_content(file_content, self.config.get("chunk_size", 6000))
            total_chunks = len(all_chunks) 
            logger.info(f"ì´ {total_chunks}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë¨.")

            current_run_output_file_path = final_output_file_path_obj.with_suffix(final_output_file_path_obj.suffix + '.current_run.tmp')
            with self._progress_lock: 
                delete_file(current_run_output_file_path) 
                current_run_output_file_path.touch()

                if not resume_translation or not loaded_metadata.get("config_hash"): 
                    logger.info("ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ë®ì–´ì”ë‹ˆë‹¤ (ìƒˆë¡œ ì‹œì‘ ë˜ëŠ” ì„¤ì • ë³€ê²½).")
                    loaded_metadata = create_new_metadata(input_file_path_obj, total_chunks, self.config)
                    logger.info(f"ìƒˆë¡œ ë²ˆì—­ì„ ì‹œì‘í•˜ë¯€ë¡œ ìµœì¢… ì¶œë ¥ íŒŒì¼ '{final_output_file_path_obj}'ì„ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                    delete_file(final_output_file_path_obj) 
                    final_output_file_path_obj.touch() 
                else: 
                    if loaded_metadata.get("total_chunks") != total_chunks:
                        logger.warning(f"ì…ë ¥ íŒŒì¼ì˜ ì²­í¬ ìˆ˜ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤ ({loaded_metadata.get('total_chunks')} -> {total_chunks}). ë©”íƒ€ë°ì´í„°ë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                        loaded_metadata = create_new_metadata(input_file_path_obj, total_chunks, self.config)
                        resume_translation = False 
                        logger.info(f"ì²­í¬ ìˆ˜ ë³€ê²½ìœ¼ë¡œ ì¸í•´ ìƒˆë¡œ ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤. ìµœì¢… ì¶œë ¥ íŒŒì¼ '{final_output_file_path_obj}'ì„ ë‹¤ì‹œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
                        delete_file(final_output_file_path_obj); final_output_file_path_obj.touch()
                    else:
                        logger.info(f"ì´ì–´í•˜ê¸° ëª¨ë“œ: ë©”íƒ€ë°ì´í„° ìƒíƒœë¥¼ 'in_progress'ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.")
                    loaded_metadata["status"] = "in_progress" 
                
                loaded_metadata["last_updated"] = time.time()
                save_metadata(metadata_file_path, loaded_metadata)

            chunks_to_process_with_indices: List[Tuple[int, str]] = []
            if resume_translation and "translated_chunks" in loaded_metadata:
                with self._progress_lock:
                    previously_translated_indices = {int(k) for k in loaded_metadata.get("translated_chunks", {}).keys()}
                    self.successful_chunks_count = len(previously_translated_indices)
                    self.processed_chunks_count = self.successful_chunks_count 
                    self.failed_chunks_count = 0 
                for i, chunk_text in enumerate(all_chunks):
                    if i not in previously_translated_indices:
                        chunks_to_process_with_indices.append((i, chunk_text))
                logger.info(f"ì´ì–´í•˜ê¸°: {self.successful_chunks_count}ê°œ ì´ë¯¸ ì™„ë£Œ, {len(chunks_to_process_with_indices)}ê°œ ì¶”ê°€ ë²ˆì—­ ëŒ€ìƒ.")
            else: 
                chunks_to_process_with_indices = list(enumerate(all_chunks))
                logger.info(f"ìƒˆë¡œ ë²ˆì—­: {len(chunks_to_process_with_indices)}ê°œ ë²ˆì—­ ëŒ€ìƒ.")
                
            if not chunks_to_process_with_indices and total_chunks > 0 : 
                logger.info("ë²ˆì—­í•  ìƒˆë¡œìš´ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤ (ëª¨ë“  ì²­í¬ê°€ ì´ë¯¸ ë²ˆì—­ë¨).")
                if status_callback: status_callback("ì™„ë£Œ: ëª¨ë“  ì²­í¬ ì´ë¯¸ ë²ˆì—­ë¨")
                with self._progress_lock:
                    loaded_metadata["status"] = "completed"
                    loaded_metadata["last_updated"] = time.time()
                    save_metadata(metadata_file_path, loaded_metadata)
                if progress_callback:
                    progress_callback(TranslationJobProgressDTO(
                        total_chunks, self.processed_chunks_count, self.successful_chunks_count,
                        self.failed_chunks_count, "ëª¨ë“  ì²­í¬ ì´ë¯¸ ë²ˆì—­ë¨"
                    ))
                with self._translation_lock: self.is_translation_running = False
                delete_file(current_run_output_file_path) 
                return

            initial_status_msg = "ë²ˆì—­ ì¤€ë¹„ ì¤‘..."
            if resume_translation: initial_status_msg = f"ì´ì–´í•˜ê¸° ì¤€ë¹„ (ë‚¨ì€ ì²­í¬: {len(chunks_to_process_with_indices)})"
            if progress_callback:
                with self._progress_lock:
                    progress_callback(TranslationJobProgressDTO(
                        total_chunks, self.processed_chunks_count, self.successful_chunks_count,
                        self.failed_chunks_count, initial_status_msg,
                        (chunks_to_process_with_indices[0][0] + 1 if chunks_to_process_with_indices else None) 
                    ))
            
            max_workers = self.config.get("max_workers", os.cpu_count() or 1)
            if not isinstance(max_workers, int) or max_workers <= 0:
                logger.warning(f"ì˜ëª»ëœ max_workers ê°’ ({max_workers}), ê¸°ë³¸ê°’ (CPU ì½”ì–´ ìˆ˜ ë˜ëŠ” 1)ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
                max_workers = os.cpu_count() or 1
            
            logger.info(f"ìµœëŒ€ {max_workers} ìŠ¤ë ˆë“œë¡œ ë³‘ë ¬ ë²ˆì—­ (ëŒ€ìƒ: {len(chunks_to_process_with_indices)} ì²­í¬)...")

            pbar = None
            if tqdm_file_stream and chunks_to_process_with_indices: 
                pbar = tqdm(total=len(chunks_to_process_with_indices), 
                            desc="ì²­í¬ ë²ˆì—­", 
                            unit="ì²­í¬", 
                            file=tqdm_file_stream, 
                            initial=0, 
                            leave=False)


            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_chunk_index: Dict[Any, int] = {}
                for i, chunk_text in chunks_to_process_with_indices:
                    if self.stop_requested:
                        logger.info("ìƒˆ ì‘ì—… ì œì¶œ ì¤‘ë‹¨ë¨ (ì‚¬ìš©ì ìš”ì²­).")
                        break
                    future = executor.submit(self._translate_and_save_chunk, i, chunk_text,
                                    current_run_output_file_path,
                                    total_chunks, 
                                    input_file_path_obj, progress_callback)
                    future_to_chunk_index[future] = i

                if self.stop_requested and not future_to_chunk_index: 
                     logger.info("ë²ˆì—­ ì‹œì‘ ì „ ì¤‘ì§€ ìš”ì²­ë¨.")
                     if pbar: pbar.close() 

                for future in as_completed(future_to_chunk_index.keys()):
                    chunk_idx_completed = future_to_chunk_index[future]
                    try:
                        future.result() 
                    except Exception as e_future:
                        logger.error(f"ë³‘ë ¬ ì‘ì—… (ì²­í¬ {chunk_idx_completed + 1}) ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ (as_completed): {e_future}", exc_info=True)
                    finally:
                        if pbar: pbar.update(1) 

            if pbar: pbar.close() 


            logger.info("ëª¨ë“  ëŒ€ìƒ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ. ê²°ê³¼ ë³‘í•© ë° ìµœì¢… ì €ì¥ ì‹œì‘...")
            newly_translated_chunks: Dict[int, str] = {}
            previously_translated_chunks_from_main_output: Dict[int, str] = {}

            try:
                newly_translated_chunks = load_chunks_from_file(current_run_output_file_path)
            except Exception as e:
                logger.error(f"ì„ì‹œ ë²ˆì—­ íŒŒì¼ '{current_run_output_file_path}' ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}. ë³‘í•©ì´ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", exc_info=True)

            final_merged_chunks: Dict[int, str] = {}
            if resume_translation and final_output_file_path_obj.exists(): 
                logger.info(f"ì´ì „ ë²ˆì—­ ê²°ê³¼ íŒŒì¼ '{final_output_file_path_obj}'ì—ì„œ ì²­í¬ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
                try:
                    previously_translated_chunks_from_main_output = load_chunks_from_file(final_output_file_path_obj)
                    final_merged_chunks.update(previously_translated_chunks_from_main_output)
                    logger.info(f"{len(previously_translated_chunks_from_main_output)}ê°œì˜ ì´ì „ ì²­í¬ ë¡œë“œë¨.")
                except Exception as e:
                    logger.error(f"ì´ì „ ìµœì¢… ì¶œë ¥ íŒŒì¼ '{final_output_file_path_obj}' ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}. ì´ì „ ë‚´ìš©ì€ ë³‘í•©ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.", exc_info=True)

            final_merged_chunks.update(newly_translated_chunks) 
            logger.info(f"{len(newly_translated_chunks)}ê°œì˜ ìƒˆ ì²­í¬ ì¶”ê°€/ë®ì–´ì“°ê¸°ë¨. ì´ {len(final_merged_chunks)} ì²­í¬ ë³‘í•© ì¤€ë¹„ ì™„ë£Œ.")

            try:
                save_merged_chunks_to_file(final_output_file_path_obj, final_merged_chunks)
                logger.info(f"ìµœì¢… ë²ˆì—­ ê²°ê³¼ê°€ '{final_output_file_path_obj}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                logger.error(f"ìµœì¢… ë²ˆì—­ ê²°ê³¼ íŒŒì¼ '{final_output_file_path_obj}' ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
                raise BtgFileHandlerException(f"ìµœì¢… ì¶œë ¥ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}", original_exception=e)

            delete_file(current_run_output_file_path)
            logger.info(f"ì„ì‹œ íŒŒì¼ '{current_run_output_file_path}' ì‚­ì œë¨.")

            final_status_msg = "ë²ˆì—­ ì™„ë£Œ."
            with self._progress_lock:
                if self.stop_requested:
                    # ì¤‘ë‹¨ ì‹œ ìµœì‹  ë©”íƒ€ë°ì´í„° ì¬ë¡œë“œí•˜ì—¬ ì™„ë£Œëœ ì²­í¬ ì •ë³´ ë³´ì¡´
                    try:
                        current_metadata = load_metadata(metadata_file_path)
                        if current_metadata and current_metadata.get("translated_chunks"):
                            # ìµœì‹  ë©”íƒ€ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì´ë¥¼ ì‚¬ìš©
                            loaded_metadata = current_metadata
                            logger.info(f"ì¤‘ë‹¨ ì‹œ ìµœì‹  ë©”íƒ€ë°ì´í„° ì¬ë¡œë“œ ì™„ë£Œ. ë³´ì¡´ëœ ì²­í¬: {len(current_metadata.get('translated_chunks', {}))}")
                        else:
                            logger.warning("ì¤‘ë‹¨ ì‹œ ìµœì‹  ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ë˜ëŠ” ë¹ˆ ë°ì´í„°, ì‹œì‘ ì‹œì  ë©”íƒ€ë°ì´í„° ì‚¬ìš©")
                    except Exception as e:
                        logger.error(f"ì¤‘ë‹¨ ì‹œ ìµœì‹  ë©”íƒ€ë°ì´í„° ì¬ë¡œë“œ ì‹¤íŒ¨: {e}. ì‹œì‘ ì‹œì  ë©”íƒ€ë°ì´í„° ì‚¬ìš©")
                    
                    final_status_msg = "ë²ˆì—­ ì¤‘ë‹¨ë¨."
                    loaded_metadata["status"] = "stopped"
                elif self.failed_chunks_count > 0:
                    final_status_msg = f"ë²ˆì—­ ì™„ë£Œ (ì‹¤íŒ¨ {self.failed_chunks_count}ê°œ í¬í•¨)."
                    loaded_metadata["status"] = "completed_with_errors"
                elif self.successful_chunks_count == total_chunks: 
                    final_status_msg = "ë²ˆì—­ ì™„ë£Œ (ëª¨ë“  ì²­í¬ ì„±ê³µ)."
                    loaded_metadata["status"] = "completed"
                else: 
                    if self.processed_chunks_count == total_chunks: 
                        final_status_msg = f"ë²ˆì—­ ì™„ë£Œ (ì„±ê³µ: {self.successful_chunks_count}/{total_chunks}, ì‹¤íŒ¨: {self.failed_chunks_count})."
                        loaded_metadata["status"] = "completed_with_errors" if self.failed_chunks_count > 0 else "completed_with_pending" 
                    else: 
                        final_status_msg = f"ë²ˆì—­ ì²˜ë¦¬ë¨ (ì²˜ë¦¬ ì‹œë„: {self.processed_chunks_count}/{total_chunks}, ì„±ê³µ: {self.successful_chunks_count})."
                        loaded_metadata["status"] = "unknown_incomplete" 
                logger.info(f"ìµœì¢… ìƒíƒœ ë©”ì‹œì§€: {final_status_msg}")

                loaded_metadata["last_updated"] = time.time()
                save_metadata(metadata_file_path, loaded_metadata)

            if status_callback: status_callback(final_status_msg)
            if progress_callback:
                with self._progress_lock:
                    progress_callback(TranslationJobProgressDTO(
                        total_chunks, self.processed_chunks_count,
                        self.successful_chunks_count, self.failed_chunks_count,
                        final_status_msg
                    ))

        except Exception as e:
            logger.error(f"ë²ˆì—­ ì„œë¹„ìŠ¤ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            if status_callback: status_callback(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
            tc_for_error_dto = total_chunks if 'total_chunks' in locals() and total_chunks > 0 else 0
            pc_for_error_dto = self.processed_chunks_count if hasattr(self, 'processed_chunks_count') else 0
            sc_for_error_dto = self.successful_chunks_count if hasattr(self, 'successful_chunks_count') else 0
            fc_for_error_dto = self.failed_chunks_count if hasattr(self, 'failed_chunks_count') else 0

            if progress_callback: progress_callback(TranslationJobProgressDTO(
                tc_for_error_dto, pc_for_error_dto, sc_for_error_dto, fc_for_error_dto, f"ì˜¤ë¥˜ ë°œìƒ: {e}"
            ))
            
            with self._progress_lock:
                error_metadata = {}
                try:
                    error_metadata = load_metadata(metadata_file_path) 
                except Exception as load_err:
                    logger.error(f"ì˜¤ë¥˜ ë°œìƒ í›„ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {load_err}")

                if not error_metadata.get("config_hash"): 
                    error_metadata = create_new_metadata(input_file_path_obj, tc_for_error_dto, self.config)

                error_metadata["status"] = "error"
                error_metadata["last_updated"] = time.time()
                if 'total_chunks' in locals(): error_metadata["total_chunks"] = total_chunks 
                save_metadata(metadata_file_path, error_metadata)

            if 'current_run_output_file_path' in locals() and current_run_output_file_path.exists():
                delete_file(current_run_output_file_path)
                logger.info(f"ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ì„ì‹œ íŒŒì¼ '{current_run_output_file_path}' ì‚­ì œ ì‹œë„ë¨.")
            raise BtgServiceException(f"ë²ˆì—­ ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}", original_exception=e) from e
        finally:
            with self._translation_lock:
                self.is_translation_running = False

        try:
            # 1ë‹¨ê³„: ì²­í¬ ë‚´ìš© í›„ì²˜ë¦¬ (ì²­í¬ ì¸ë±ìŠ¤ëŠ” ìœ ì§€)
            logger.info("ë²ˆì—­ ê²°ê³¼ ì²­í¬ ë‚´ìš© í›„ì²˜ë¦¬ ì‹œì‘...")
            processed_chunks = self.post_processing_service.post_process_merged_chunks(final_merged_chunks)
            
            # 2ë‹¨ê³„: í›„ì²˜ë¦¬ëœ ì²­í¬ë“¤ì„ íŒŒì¼ì— ì €ì¥ (ì²­í¬ ì¸ë±ìŠ¤ í¬í•¨)
            save_merged_chunks_to_file(final_output_file_path_obj, processed_chunks)
            logger.info(f"í›„ì²˜ë¦¬ëœ ë²ˆì—­ ê²°ê³¼ê°€ '{final_output_file_path_obj}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # 3ë‹¨ê³„: ìµœì¢… íŒŒì¼ì—ì„œ ì²­í¬ ì¸ë±ìŠ¤ ë§ˆì»¤ë“¤ ì œê±°
            logger.info("ìµœì¢… íŒŒì¼ì—ì„œ ì²­í¬ ì¸ë±ìŠ¤ ì œê±° ì¤‘...")
            index_removal_success = self.post_processing_service.remove_chunk_indexes_from_final_file(final_output_file_path_obj)
            
            if index_removal_success:
                logger.info(f"ìµœì¢… í›„ì²˜ë¦¬ ì™„ë£Œ: '{final_output_file_path_obj}' (ì²­í¬ ì¸ë±ìŠ¤ ì œê±°ë¨)")
            else:
                logger.warning(f"ì²­í¬ ì¸ë±ìŠ¤ ì œê±°ì— ì‹¤íŒ¨í–ˆì§€ë§Œ ë²ˆì—­ íŒŒì¼ì€ ì €ì¥ë¨: '{final_output_file_path_obj}'")
            
        except Exception as e:
            logger.error(f"ìµœì¢… ë²ˆì—­ ê²°ê³¼ íŒŒì¼ '{final_output_file_path_obj}' ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            raise BtgFileHandlerException(f"ìµœì¢… ì¶œë ¥ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}", original_exception=e)

    def request_stop_translation(self):
        if self.is_translation_running:
            logger.info("ë²ˆì—­ ì¤‘ì§€ ìš”ì²­ ìˆ˜ì‹ ë¨.")
            with self._translation_lock: 
                self.stop_requested = True
        else:
            logger.info("ì‹¤í–‰ ì¤‘ì¸ ë²ˆì—­ ì‘ì—…ì´ ì—†ì–´ ì¤‘ì§€ ìš”ì²­ì„ ë¬´ì‹œí•©ë‹ˆë‹¤.")


if __name__ == '__main__':
    import logging

    logger.setLevel(logging.DEBUG) 

    test_output_dir = Path("test_app_service_output")
    test_output_dir.mkdir(exist_ok=True)

    temp_config_path = test_output_dir / "temp_config.json"
    sample_app_config_data = {
        "api_key": os.environ.get("GOOGLE_API_KEY", "YOUR_DEFAULT_API_KEY_FOR_TEST"),
        "service_account_file_path": None, 
        "use_vertex_ai": False, 
        "gcp_project": None, 
        "gcp_location": None, 
        "model_name": "gemini-1.5-flash-latest",
        "temperature": 0.7,
        "top_p": 0.9,
        "prompts": "Translate to Korean: {{slot}}",
        "chunk_size": 50, 
        "pronouns_csv": str(test_output_dir / "sample_pronouns.csv"),
        "requests_per_minute": 60,
        # "max_pronoun_entries": 5, # ë¡œì–´ë¶ìœ¼ë¡œ ëŒ€ì²´ë˜ë©´ì„œ ì´ ì„¤ì •ì€ ì§ì ‘ ì‚¬ìš©ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
        "pronoun_sample_ratio": 100.0, 
        "max_workers": 2 
    }

    with open(temp_config_path, "w", encoding="utf-8") as f:
        json.dump(sample_app_config_data, f, indent=4)

    sample_pronoun_file = test_output_dir / "sample_pronouns.csv"
    with open(sample_pronoun_file, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["ì™¸êµ­ì–´", "í•œêµ­ì–´", "ë“±ì¥íšŸìˆ˜"]) 
        writer.writerow(["BTG", "ë¹„í‹°ì§€", "10"])
        writer.writerow(["Gemini", "ì œë¯¸ë‹ˆ", "5"])

    temp_input_file = test_output_dir / "sample_input.txt"
    temp_input_content = (
        "Hello BTG.\nThis is a test for the Gemini API.\n"
        "We are testing the application service layer.\n"
        "Another line for chunking. And one more for Gemini.\n"
        "This is the fifth line.\nAnd the sixth line is here.\n"
        "Seventh line for more data.\nEighth line, almost done."
    )
    with open(temp_input_file, "w", encoding="utf-8") as f:
        f.write(temp_input_content)

    temp_output_file = test_output_dir / "sample_output.txt"
    
    temp_metadata_file = get_metadata_file_path(temp_input_file)
    if temp_metadata_file.exists():
        delete_file(temp_metadata_file)
    if temp_output_file.exists():
        delete_file(temp_output_file)


    app_service: Optional[AppService] = None
    try:
        app_service = AppService(config_file_path=temp_config_path)
        logger.info("AppService ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì„±ê³µ.")
    except Exception as e:
        logger.error(f"AppService ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
        exit()

    if app_service and app_service.gemini_client:
        print("\n--- ëª¨ë¸ ëª©ë¡ ì¡°íšŒ í…ŒìŠ¤íŠ¸ ---")
        try:
            models = app_service.get_available_models()
            if models:
                logger.info(f"ì¡°íšŒëœ ëª¨ë¸ ìˆ˜: {len(models)}")
                for m in models[:2]: 
                    logger.info(f"  - {m.get('display_name', m.get('name'))}")
            else:
                logger.info("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        except BtgApiClientException as e:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        except Exception as e_models:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e_models}", exc_info=True)

    else:
        logger.warning("Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

    if app_service and app_service.lorebook_service: # Changed from pronoun_service
        print("\n--- ë¡œì–´ë¶ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ---") # Changed
        try:
            def _lorebook_progress_dto_cb(dto: LorebookExtractionProgressDTO): # Changed DTO
                logger.debug(f"ë¡œì–´ë¶ ì§„í–‰ DTO: {dto.processed_segments}/{dto.total_segments} - {dto.current_status_message} (ì¶”ì¶œ í•­ëª©: {dto.extracted_entries_count})") # Changed fields

            result_path = app_service.extract_lorebook( # Changed method
                temp_input_file,
                progress_callback=_lorebook_progress_dto_cb,
                seed_lorebook_path=None # Optionally provide a seed path for testing
            )
            logger.info(f"ë¡œì–´ë¶ ì¶”ì¶œ ì™„ë£Œ, ê²°ê³¼ íŒŒì¼: {result_path}") # Changed
        except Exception as e:
            logger.error(f"ë¡œì–´ë¶ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True) # Changed
    else:
        logger.warning("Lorebook ì„œë¹„ìŠ¤ê°€ ì—†ì–´ ë¡œì–´ë¶ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.") # Changed

    if app_service and app_service.translation_service:
        print("\n--- ë²ˆì—­ í…ŒìŠ¤íŠ¸ (ë³‘ë ¬ ì²˜ë¦¬) ---")
        try:
            test_tqdm_stream = sys.stdout 

            def _trans_progress_dto(dto: TranslationJobProgressDTO):
                logger.debug(f"ë²ˆì—­ ì§„í–‰ DTO: {dto.current_chunk_processing or '-'}/{dto.total_chunks}, ì„±ê³µ: {dto.successful_chunks}, ì‹¤íŒ¨: {dto.failed_chunks} - {dto.current_status_message}")
                pass

            def _trans_status(status_msg):
                logger.info(f"ë²ˆì—­ ìƒíƒœ: {status_msg}")


            app_service.start_translation(
                temp_input_file,
                temp_output_file,
                _trans_progress_dto,
                _trans_status,
                tqdm_file_stream=test_tqdm_stream 
            )

            start_time = time.time()
            while app_service.is_translation_running and (time.time() - start_time) < 120: 
                time.sleep(0.5)

            if app_service.is_translation_running:
                logger.warning("ë²ˆì—­ ì‘ì—…ì´ ì‹œê°„ ë‚´ì— ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (í…ŒìŠ¤íŠ¸). ì¤‘ì§€ ìš”ì²­...")
                app_service.request_stop_translation()
                time.sleep(2) 

            if temp_output_file.exists():
                logger.info(f"ë²ˆì—­ ì™„ë£Œ, ê²°ê³¼ íŒŒì¼: {temp_output_file}")
            else:
                logger.error("ë²ˆì—­ ê²°ê³¼ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"ë²ˆì—­ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True)
    else:
        logger.warning("Translation ì„œë¹„ìŠ¤ê°€ ì—†ì–´ ë²ˆì—­ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

    logger.info("AppService í…ŒìŠ¤íŠ¸ ì™„ë£Œ.")
