# translation_service.py
import time
import random
import re
import csv
import asyncio
from pathlib import Path
from typing import Dict, Any, Optional, List, Union, Callable
import os
import copy # Moved here

try:
    from infrastructure.gemini_client import (
        GeminiClient,
        GeminiContentSafetyException,
        GeminiRateLimitException,
        GeminiApiException,
        GeminiInvalidRequestException,
        GeminiAllApiKeysExhaustedException 
    )
    from infrastructure.file_handler import read_json_file
    from infrastructure.logger_config import setup_logger
    from core.exceptions import BtgTranslationException, BtgApiClientException
    from utils.chunk_service import ChunkService
    # types ëª¨ë“ˆì€ gemini_clientì—ì„œ ì‚¬ìš©ë˜ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ì§ì ‘ì ì¸ ì˜ì¡´ì„±ì´ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. # ë¡œì–´ë¶ -> ìš©ì–´ì§‘
    # ë§Œì•½ ì´ íŒŒì¼ ë‚´ì—ì„œ types.Part ë“±ì„ ì§ì ‘ ì‚¬ìš©í•œë‹¤ë©´, ì•„ë˜ì™€ ê°™ì´ ì„í¬íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤. # ë¡œì–´ë¶ -> ìš©ì–´ì§‘
    from google.genai import types as genai_types
    from core.dtos import GlossaryEntryDTO
except ImportError:
    from infrastructure.gemini_client import (  # type: ignore
        GeminiClient,
        GeminiContentSafetyException,
        GeminiRateLimitException,
        GeminiApiException,
        GeminiInvalidRequestException,
        GeminiAllApiKeysExhaustedException 
    )
    from infrastructure.file_handler import read_json_file  # type: ignore
    from infrastructure.logger_config import setup_logger  # type: ignore
    from core.exceptions import BtgTranslationException, BtgApiClientException  # type: ignore
    from utils.chunk_service import ChunkService  # type: ignore
    from core.dtos import GlossaryEntryDTO # type: ignore
    from google.genai import types as genai_types # Fallback import

logger = setup_logger(__name__)

def _format_glossary_for_prompt( # í•¨ìˆ˜ëª… ë³€ê²½
    glossary_entries: List[GlossaryEntryDTO], # DTOëŠ” GlossaryEntryDTO (ê²½ëŸ‰í™”ëœ ë²„ì „)
    max_entries: int,
    max_chars: int
) -> str:
    if not glossary_entries:
        return "ìš©ì–´ì§‘ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ" # ë©”ì‹œì§€ ë³€ê²½

    selected_entries_str = []
    current_chars = 0
    entries_count = 0

    # ë“±ì¥ íšŸìˆ˜ ë§ì€ ìˆœ, ê°™ìœ¼ë©´ í‚¤ì›Œë“œ ê°€ë‚˜ë‹¤ ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_entries = sorted(glossary_entries, key=lambda x: (-x.occurrence_count, x.keyword.lower()))

    for entry in sorted_entries:
        if entries_count >= max_entries:
            break
        
        # í˜„ì¬ í•­ëª© ì¶”ê°€ ì‹œ ìµœëŒ€ ê¸€ì ìˆ˜ ì´ˆê³¼í•˜ë©´ ì¤‘ë‹¨ (ë‹¨, ìµœì†Œ 1ê°œëŠ” í¬í•¨ë˜ë„ë¡)
        # DTOì—ì„œ source_languageê°€ ì œê±°ë˜ì—ˆìœ¼ë¯€ë¡œ í•´ë‹¹ ë¶€ë¶„ í¬ë§·íŒ…ì—ì„œ ì œì™¸
        entry_str = (f"- {entry.keyword} "
                     f"-> {entry.translated_keyword} ({entry.target_language}) "
                     f"(ë“±ì¥: {entry.occurrence_count}íšŒ)")
        if current_chars + len(entry_str) > max_chars and entries_count > 0:
            break
        
        selected_entries_str.append(entry_str)
        current_chars += len(entry_str) + 1 # +1 for newline
        entries_count += 1

    if not selected_entries_str:
        return "ìš©ì–´ì§‘ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ (ì œí•œìœ¼ë¡œ ì¸í•´ ì„ íƒëœ í•­ëª© ì—†ìŒ)" # ë©”ì‹œì§€ ë³€ê²½
        
    return "\n".join(selected_entries_str)

def _inject_slots_into_history(
    history: List[genai_types.Content], 
    replacements: Dict[str, str]
) -> tuple[List[genai_types.Content], bool]:
    """
    íˆìŠ¤í† ë¦¬ ë‚´ì˜ Content ê°ì²´ë“¤ì„ ìˆœíšŒí•˜ë©° ìŠ¬ë¡¯({{slot}} ë“±)ì„ ì‹¤ì œ ê°’ìœ¼ë¡œ ì¹˜í™˜í•©ë‹ˆë‹¤.
    ë°˜í™˜ê°’: (ìˆ˜ì •ëœ íˆìŠ¤í† ë¦¬, ì¹˜í™˜ ë°œìƒ ì—¬ë¶€)
    """
    # ê¹Šì€ ë³µì‚¬ë¡œ ì›ë³¸ ì˜¤ì—¼ ë°©ì§€
    new_history = copy.deepcopy(history)
    replacement_occurred = False

    for content in new_history:
        if not hasattr(content, 'parts'):
            continue
            
        for part in content.parts:
            if hasattr(part, 'text') and part.text:
                original_text = part.text
                modified_text = original_text
                
                for key, value in replacements.items():
                    if key in modified_text:
                        modified_text = modified_text.replace(key, value)
                        replacement_occurred = True
                
                if original_text != modified_text:
                    part.text = modified_text
    
    return new_history, replacement_occurred

class TranslationService:
    def __init__(self, gemini_client: GeminiClient, config: Dict[str, Any]):
        self.gemini_client = gemini_client
        self.config = config
        self.chunk_service = ChunkService()
        self.glossary_entries_for_injection: List[GlossaryEntryDTO] = [] # Renamed and type changed
        self.stop_check_callback: Optional[Callable[[], bool]] = None  # ì¤‘ë‹¨ ìš”ì²­ í™•ì¸ìš© ì½œë°±

        if self.config.get("enable_dynamic_glossary_injection", False): # Key changed
            self._load_glossary_data() # í•¨ìˆ˜ëª… ë³€ê²½
            logger.info("ë™ì  ìš©ì–´ì§‘ ì£¼ì… í™œì„±í™”ë¨. ìš©ì–´ì§‘ ë°ì´í„° ë¡œë“œ ì‹œë„.") # ë©”ì‹œì§€ ë³€ê²½
        else:
            logger.info("ë™ì  ìš©ì–´ì§‘ ì£¼ì… ë¹„í™œì„±í™”ë¨. ìš©ì–´ì§‘ ì»¨í…ìŠ¤íŠ¸ ì—†ì´ ë²ˆì—­í•©ë‹ˆë‹¤.") # ë©”ì‹œì§€ ë³€ê²½

    def _load_glossary_data(self): # í•¨ìˆ˜ëª… ë³€ê²½
        # ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê¸° ì „ì— í•­ìƒ ëª©ë¡ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        self.glossary_entries_for_injection = []
        
        # í†µí•©ëœ ìš©ì–´ì§‘ ê²½ë¡œ ì‚¬ìš©
        lorebook_json_path_str = self.config.get("glossary_json_path")
        if lorebook_json_path_str and os.path.exists(lorebook_json_path_str):
            lorebook_json_path = Path(lorebook_json_path_str)
            try:
                raw_data = read_json_file(lorebook_json_path)
                if isinstance(raw_data, list):
                    for item_dict in raw_data:
                        if isinstance(item_dict, dict) and \
                           "keyword" in item_dict and \
                           "translated_keyword" in item_dict and \
                           "target_language" in item_dict:
                            try:
                                entry = GlossaryEntryDTO( # Explicitly use GlossaryEntryDTO
                                    keyword=item_dict.get("keyword", ""),
                                    translated_keyword=item_dict.get("translated_keyword", ""),
                                    target_language=item_dict.get("target_language", ""),
                                    occurrence_count=int(item_dict.get("occurrence_count", 0))
                                )
                                if all([entry.keyword, entry.translated_keyword, entry.target_language]): # í•„ìˆ˜ í•„ë“œ í™•ì¸ (source_language ì œê±°)
                                    self.glossary_entries_for_injection.append(entry)
                                else:
                                    logger.warning(f"ê²½ëŸ‰ ìš©ì–´ì§‘ í•­ëª©ì— í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {item_dict}")
                            except (TypeError, ValueError) as e_dto:
                                logger.warning(f"ìš©ì–´ì§‘ í•­ëª© DTO ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {item_dict}, ì˜¤ë¥˜: {e_dto}") # ë©”ì‹œì§€ ë³€ê²½
                        else:
                            logger.warning(f"ì˜ëª»ëœ ìš©ì–´ì§‘ í•­ëª© í˜•ì‹ (ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹ˆê±°ë‚˜ í•„ìˆ˜ í‚¤ 'keyword' ë˜ëŠ” 'translated_keyword' ëˆ„ë½) ê±´ë„ˆëœ€: {item_dict}") # ë©”ì‹œì§€ ë³€ê²½
                    logger.info(f"{len(self.glossary_entries_for_injection)}ê°œì˜ ìš©ì–´ì§‘ í•­ëª©ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {lorebook_json_path}") # ë©”ì‹œì§€ ë³€ê²½
                else: # type: ignore
                    logger.error(f"ìš©ì–´ì§‘ JSON íŒŒì¼ì´ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤: {lorebook_json_path}, íƒ€ì…: {type(raw_data)}") # ë©”ì‹œì§€ ë³€ê²½
            except Exception as e:
                logger.error(f"ìš©ì–´ì§‘ JSON íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ({lorebook_json_path}): {e}", exc_info=True) # ë©”ì‹œì§€ ë³€ê²½
                self.glossary_entries_for_injection = []
        else:
            logger.info(f"ìš©ì–´ì§‘ JSON íŒŒì¼({lorebook_json_path_str})ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë™ì  ì£¼ì…ì„ ìœ„í•´ ìš©ì–´ì§‘ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.") # ë©”ì‹œì§€ ë³€ê²½
            self.glossary_entries_for_injection = []

    def _construct_prompt(self, chunk_text: str) -> str:
        prompt_template = self.config.get("prompts", "Translate to Korean: {{slot}}")
        if isinstance(prompt_template, (list, tuple)):
            prompt_template = prompt_template[0] if prompt_template else "Translate to Korean: {{slot}}"

        final_prompt = prompt_template

        # Determine the source language for the current chunk to filter glossary entries
        config_source_lang = self.config.get("novel_language") # í†µí•©ëœ ì„¤ì • ì‚¬ìš©
        # Fallback language from config, with a hardcoded default if the config key itself is missing
        config_fallback_lang = self.config.get("novel_language_fallback", "ja") # í†µí•©ëœ í´ë°± ì„¤ì • ì‚¬ìš©

        # "auto" ëª¨ë“œì¼ ë•Œ, LLMì´ ì–¸ì–´ë¥¼ ê°ì§€í•˜ê³  ìš©ì–´ì§‘ì„ í•„í„°ë§í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ê°€ êµ¬ì„±ë©ë‹ˆë‹¤.
        # Python ë‹¨ì—ì„œ current_source_lang_for_translationì„ í™•ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        # ë¡œê¹…ì´ë‚˜ íŠ¹ì • ì¡°ê±´ë¶€ ë¡œì§ì„ ìœ„í•´ì„  ì—¬ì „íˆ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë‚˜, ìš©ì–´ì§‘ í•„í„°ë§ì€ LLMìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.
        current_source_lang_for_glossary_filtering: Optional[str] = None

        if config_source_lang == "auto":
            logger.info(f"ë²ˆì—­ ì¶œë°œ ì–¸ì–´ ì„¤ì •: 'auto'. LLMì´ í”„ë¡¬í”„íŠ¸ ë‚´ì—ì„œ ì–¸ì–´ë¥¼ ê°ì§€í•˜ê³  ìš©ì–´ì§‘ì„ ì ìš©í•˜ë„ë¡ í•©ë‹ˆë‹¤.") # ë©”ì‹œì§€ ë³€ê²½
            # current_source_lang_for_glossary_filteringëŠ” Noneìœ¼ë¡œ ìœ ì§€í•˜ê±°ë‚˜ "auto"ë¡œ ì„¤ì •.
            # ìš©ì–´ì§‘ í•„í„°ë§ì€ LLMì˜ ì—­í• ì´ ë©ë‹ˆë‹¤.
        elif config_source_lang and isinstance(config_source_lang, str) and config_source_lang.strip(): # Specific language code provided
            current_source_lang_for_glossary_filtering = config_source_lang
            logger.info(f"ëª…ì‹œì  ë²ˆì—­ ì¶œë°œ ì–¸ì–´ '{current_source_lang_for_glossary_filtering}' ì‚¬ìš©. ìš©ì–´ì§‘ë„ ì´ ì–¸ì–´ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§ë©ë‹ˆë‹¤.") # ë©”ì‹œì§€ ë³€ê²½
        else: # config_source_lang is None, empty string, or not "auto"
            current_source_lang_for_glossary_filtering = config_fallback_lang
            logger.warning(f"ë²ˆì—­ ì¶œë°œ ì–¸ì–´ê°€ ìœ íš¨í•˜ê²Œ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ 'auto'ê°€ ì•„ë‹™ë‹ˆë‹¤. í´ë°± ì–¸ì–´ '{current_source_lang_for_glossary_filtering}'ë¥¼ ìš©ì–´ì§‘ í•„í„°ë§ì— ì‚¬ìš©.")

        # 1. Dynamic Glossary Injection
        if self.config.get("enable_dynamic_glossary_injection", False) and \
           self.glossary_entries_for_injection and \
           "{{glossary_context}}" in final_prompt: # Placeholder changed
            
            relevant_entries_for_chunk: List[GlossaryEntryDTO] = []
            chunk_text_lower = chunk_text.lower() # For case-insensitive keyword matching
            # ìµœì¢… ë²ˆì—­ ëª©í‘œ ì–¸ì–´ (ì˜ˆ: "ko")
            # ì´ ì„¤ì •ì€ config.json ë˜ëŠ” ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì œê³µë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
            final_target_lang = self.config.get("target_translation_language", "ko").lower()

            if config_source_lang == "auto":
                # "auto" ëª¨ë“œ: ì²­í¬ì˜ ì–¸ì–´ëŠ” LLMì´ ê°ì§€.
                # ìš©ì–´ì§‘ í•­ëª©ì˜ target_languageê°€ ìµœì¢… ë²ˆì—­ ëª©í‘œ ì–¸ì–´ì™€ ì¼ì¹˜í•˜ëŠ” ê²ƒë§Œ ê³ ë ¤.
                # source_language í•„í„°ë§ì€ LLMì˜ ë¬¸ë§¥ ì´í•´ì— ë§¡ê¸°ê±°ë‚˜, ì—¬ê¸°ì„œ ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ë§Œ ìˆ˜í–‰.
                logger.info(f"ìë™ ì–¸ì–´ ê°ì§€ ëª¨ë“œ: ìš©ì–´ì§‘ì€ í‚¤ì›Œë“œ ì¼ì¹˜ ë° ìµœì¢… ëª©í‘œ ì–¸ì–´({final_target_lang}) ì¼ì¹˜ë¡œ í•„í„°ë§ í›„ LLMì— ì „ë‹¬.") # ë©”ì‹œì§€ ë³€ê²½
                for entry in self.glossary_entries_for_injection:
                    if entry.target_language.lower() == final_target_lang and \
                       entry.keyword.lower() in chunk_text_lower:
                        relevant_entries_for_chunk.append(entry)
            else:
                # ëª…ì‹œì  ì–¸ì–´ ì„¤ì • ëª¨ë“œ: Pythonì—ì„œ ì–¸ì–´ ë° í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ í•„í„°ë§.
                logger.info(f"ëª…ì‹œì  ì–¸ì–´ ëª¨ë“œ ('{current_source_lang_for_glossary_filtering}'): ìš©ì–´ì§‘ì„ ì¶œë°œì–´/ë„ì°©ì–´ ë° í‚¤ì›Œë“œ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§.") # ë©”ì‹œì§€ ë³€ê²½
                for entry in self.glossary_entries_for_injection:
                    # source_language í•„í„°ë§ ì œê±°. DTOì— í•´ë‹¹ í•„ë“œê°€ ì—†ìœ¼ë¯€ë¡œ.
                    if entry.target_language.lower() == final_target_lang and \
                       entry.keyword.lower() in chunk_text_lower:
                        relevant_entries_for_chunk.append(entry)
                    # source_language ê´€ë ¨ ë¡œê¹… ì œê±°
                    elif not (entry.target_language.lower() == final_target_lang): # target_language ë¶ˆì¼ì¹˜ ë¡œê¹…ì€ ìœ ì§€
                        logger.debug(f"ìš©ì–´ì§‘ í•­ëª© '{entry.keyword}' ê±´ë„ˆëœ€: ë„ì°© ì–¸ì–´ ë¶ˆì¼ì¹˜ (ìš©ì–´ì§‘TL: {entry.target_language}, ìµœì¢…TL: {final_target_lang}).")
                        continue
            
            logger.debug(f"í˜„ì¬ ì²­í¬ì— ëŒ€í•´ {len(relevant_entries_for_chunk)}ê°œì˜ ê´€ë ¨ ìš©ì–´ì§‘ í•­ëª© ë°œê²¬.") # ë©”ì‹œì§€ ë³€ê²½

            # 1.b. Format the relevant entries for the prompt
            max_entries = self.config.get("max_glossary_entries_per_chunk_injection", 3) # Key changed
            max_chars = self.config.get("max_glossary_chars_per_chunk_injection", 500) # Key changed
            
            formatted_glossary_context = _format_glossary_for_prompt( # í•¨ìˆ˜ëª… ë³€ê²½
                relevant_entries_for_chunk, max_entries, max_chars # Pass only relevant entries
            )
            
            # Check if actual content was formatted (not just "ì—†ìŒ" messages)
            final_prompt = final_prompt.replace("{{glossary_context}}", formatted_glossary_context) # Placeholder changed
        else:
            if "{{glossary_context}}" in final_prompt: # Placeholder changed
                 final_prompt = final_prompt.replace("{{glossary_context}}", "ìš©ì–´ì§‘ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ (ì£¼ì… ë¹„í™œì„±í™” ë˜ëŠ” í•´ë‹¹ í•­ëª© ì—†ìŒ)") # Placeholder changed
                 logger.debug("ë™ì  ìš©ì–´ì§‘ ì£¼ì… ë¹„í™œì„±í™” ë˜ëŠ” í”Œë ˆì´ìŠ¤í™€ë” ë¶€ì¬ë¡œ 'ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ' ë©”ì‹œì§€ ì‚¬ìš©.")
        
        # 3. Main content slot - This should be done *after* all other placeholders are processed.
        final_prompt = final_prompt.replace("{{slot}}", chunk_text)
        return final_prompt

    def translate_text(self, text_chunk: str, stream: bool = False) -> str:
        """
        ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤.
        ìŠ¬ë¡¯ ì£¼ì…(Slot Injection) ë°©ì‹ì„ ì§€ì›í•˜ë„ë¡ ë¦¬íŒ©í† ë§ë˜ì—ˆìŠµë‹ˆë‹¤.
        """
        if not text_chunk.strip():
            logger.debug("Translate_text: ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆì–´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜.")
            return ""
        
        # 1. ìš©ì–´ì§‘ ë° í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
        # ê¸°ì¡´ _construct_prompt ë¡œì§ ì¤‘ ìš©ì–´ì§‘ ìƒì„± ë¶€ë¶„ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        # (ë‹¨, Slot Injection ëª¨ë“œì—ì„œëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì „ì²´ë¥¼ ê°€ì ¸ì˜¤ëŠ”ê²Œ ì•„ë‹ˆë¼ ìš©ì–´ì§‘ ë¬¸ìì—´ë§Œ í•„ìš”í•¨)
        glossary_context_str = "ìš©ì–´ì§‘ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ"
        
        # ìš©ì–´ì§‘ ë¡œì§ ìˆ˜í–‰ (ê¸°ì¡´ _construct_prompt ì°¸ì¡°í•˜ì—¬ ë¬¸ìì—´ë§Œ ì¶”ì¶œ)
        if self.config.get("enable_dynamic_glossary_injection", False) and self.glossary_entries_for_injection:
             logger.info("ìš©ì–´ì§‘ ì»¨í…ìŠ¤íŠ¸ ì£¼ì… í™œì„±í™”ë¨ (ì²­í¬ ë‚´ ê´€ë ¨ í‚¤ì›Œë“œ ì²´í¬).")
             # ... (ê¸°ì¡´ ìš©ì–´ì§‘ í•„í„°ë§ ë¡œì§ê³¼ ë™ì¼í•˜ê²Œ ìˆ˜í–‰í•˜ì—¬ glossary_context_str ìƒì„±) ...
             # ì½”ë“œ ê°„ê²°í™”ë¥¼ ìœ„í•´ í•µì‹¬ ë¡œì§ë§Œ ìš”ì•½:
             chunk_text_lower = text_chunk.lower()
             final_target_lang = self.config.get("target_translation_language", "ko").lower()
             relevant_entries = []
             
             # ìš©ì–´ì§‘ í•„í„°ë§ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼)
             config_source_lang = self.config.get("novel_language", "auto")
             for entry in self.glossary_entries_for_injection:
                if entry.target_language.lower() == final_target_lang and entry.keyword.lower() in chunk_text_lower:
                    relevant_entries.append(entry)
             
             max_entries = self.config.get("max_glossary_entries_per_chunk_injection", 3)
             max_chars = self.config.get("max_glossary_chars_per_chunk_injection", 500)
             glossary_context_str = _format_glossary_for_prompt(relevant_entries, max_entries, max_chars)

             if not glossary_context_str.startswith("ìš©ì–´ì§‘ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ"):
                logger.info(f"API ìš”ì²­ì— ì£¼ì…í•  ìš©ì–´ì§‘ ì»¨í…ìŠ¤íŠ¸ ìƒì„±ë¨. ë‚´ìš© ì¼ë¶€: {glossary_context_str[:100]}...")
             else:
                logger.debug("í˜„ì¬ ì²­í¬ì— ì£¼ì…í•  ê´€ë ¨ ìš©ì–´ì§‘ í•­ëª©ì„ ì°¾ì§€ ëª»í•¨.")
        
        # 2. ì¹˜í™˜ ë°ì´í„° ë§µ ì¤€ë¹„
        replacements = {
            "{{slot}}": text_chunk,
            "{{glossary_context}}": glossary_context_str
        }

        api_prompt_for_gemini_client: List[genai_types.Content] = []
        api_system_instruction: Optional[str] = None

        # 3. í”„ë¦¬í•„ ë° íˆìŠ¤í† ë¦¬ êµ¬ì„± ë¡œì§ (í•µì‹¬ ë³€ê²½ ì‚¬í•­)
        if self.config.get("enable_prefill_translation", False):
            logger.info("í”„ë¦¬í•„ ë²ˆì—­ ëª¨ë“œ í™œì„±í™”ë¨ (Slot Injection ì²´í¬).")
            
            # ì‹œìŠ¤í…œ ì§€ì¹¨ ì„¤ì •
            api_system_instruction = self.config.get("prefill_system_instruction", "")
            
            # ìºì‹œëœ íˆìŠ¤í† ë¦¬ ë¡œë“œ ë° Content ê°ì²´ ë³€í™˜
            prefill_cached_history_raw = self.config.get("prefill_cached_history", [])
            base_history: List[genai_types.Content] = []
            
            if isinstance(prefill_cached_history_raw, list):
                for item in prefill_cached_history_raw:
                    if isinstance(item, dict) and "role" in item and "parts" in item:
                        sdk_parts = []
                        for part_item in item.get("parts", []):
                            if isinstance(part_item, str):
                                sdk_parts.append(genai_types.Part.from_text(text=part_item))
                        if sdk_parts:
                            base_history.append(genai_types.Content(role=item["role"], parts=sdk_parts))

            # [Slot Injection] íˆìŠ¤í† ë¦¬ ë‚´ ìŠ¬ë¡¯ ì¹˜í™˜ ì‹œë„
            injected_history, injected = _inject_slots_into_history(base_history, replacements)

            if injected:
                logger.info("íˆìŠ¤í† ë¦¬ ë‚´ë¶€ì—ì„œ '{{slot}}'ì´ ê°ì§€ë˜ì–´ ì›ë¬¸ì„ ì£¼ì…í–ˆìŠµë‹ˆë‹¤ (Jailbreak ëª¨ë“œ).")
                api_prompt_for_gemini_client = injected_history
                
                # [Trigger Logic] ë§ˆì§€ë§‰ ë©”ì‹œì§€ í™•ì¸
                if api_prompt_for_gemini_client:
                    last_msg = api_prompt_for_gemini_client[-1]
                    
                    if last_msg.role == "model":
                        # ë§ˆì§€ë§‰ì´ ëª¨ë¸(í”„ë¦¬í•„)ì´ë©´, ì´ì–´ ì“°ê¸°ë¥¼ ìœ ë„í•˜ê¸° ìœ„í•´ ë¹ˆ ìœ ì € ë©”ì‹œì§€(Trigger) ì¶”ê°€
                        logger.debug("ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ Modelì´ë¯€ë¡œ ì´ì–´ì“°ê¸°ë¥¼ ìœ„í•œ ë¹ˆ User íŠ¸ë¦¬ê±°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.")
                        api_prompt_for_gemini_client.append(
                            genai_types.Content(role="user", parts=[genai_types.Part.from_text(text=" ")])
                        )
                    # ë§ˆì§€ë§‰ì´ Userë¼ë©´ ê·¸ëŒ€ë¡œ ì „ì†¡ (User ë©”ì‹œì§€ê°€ Triggerê°€ ë¨)
            else:
                # ìŠ¬ë¡¯ì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ: íˆìŠ¤í† ë¦¬ + (í”„ë¡¬í”„íŠ¸ + ì›ë¬¸)
                logger.info("íˆìŠ¤í† ë¦¬ ë‚´ë¶€ì— ìŠ¬ë¡¯ì´ ì—†ìŠµë‹ˆë‹¤. í‘œì¤€ í”„ë¦¬í•„ ë°©ì‹ìœ¼ë¡œ ì›ë¬¸ì„ ëì— ì¶”ê°€í•©ë‹ˆë‹¤.")
                api_prompt_for_gemini_client = injected_history # ì¹˜í™˜ëœê²Œ ì—†ìœ¼ë©´ ì›ë³¸ê³¼ ê°™ìŒ
                
                # ê¸°ì¡´ ë°©ì‹ì˜ í”„ë¡¬í”„íŠ¸ ìƒì„± (ì—¬ê¸°ì„œ {{slot}} ì²˜ë¦¬ë¨)
                user_prompt_str = self._construct_prompt(text_chunk)
                api_prompt_for_gemini_client.append(
                    genai_types.Content(role="user", parts=[genai_types.Part.from_text(text=user_prompt_str)])
                )
        else:
            # í‘œì¤€ ëª¨ë“œ (í”„ë¦¬í•„ ë”)
            logger.info("í‘œì¤€ ë²ˆì—­ ëª¨ë“œ (í”„ë¦¬í•„ Off).")
            user_prompt_str = self._construct_prompt(text_chunk)
            api_prompt_for_gemini_client = [
                genai_types.Content(role="user", parts=[genai_types.Part.from_text(text=user_prompt_str)])
            ]

        try:
            # Gemini Client í˜¸ì¶œ
            translated_text_from_api = self.gemini_client.generate_text(
                prompt=api_prompt_for_gemini_client, # ì´ì œ í•­ìƒ List[Content]
                model_name=self.config.get("model_name", "gemini-2.0-flash"),
                generation_config_dict={
                    "temperature": self.config.get("temperature", 0.7),
                    "top_p": self.config.get("top_p", 0.9),
                    "thinking_level": self.config.get("thinking_level", "high")
                },
                thinking_budget=self.config.get("thinking_budget", None),
                system_instruction_text=api_system_instruction,
                stream=stream
            )

            if translated_text_from_api is None:
                logger.error("GeminiClient.generate_textê°€ Noneì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
                raise GeminiContentSafetyException("APIë¡œë¶€í„° ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (None ë°˜í™˜).")

            # ë¹ˆ ë¬¸ìì—´ ì‘ë‹µë„ ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œë¡œ ê°„ì£¼í•˜ì—¬ ì¬ì‹œë„ ë¡œì§ì„ íƒ€ë„ë¡ ìˆ˜ì •
            if not translated_text_from_api.strip() and text_chunk.strip():
                logger.warning(f"APIê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ì…ë ¥ì— ëŒ€í•´ ë¹ˆ ë¬¸ìì—´ì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. ì›ë³¸: '{text_chunk[:100]}...'")
                raise GeminiContentSafetyException("APIê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ì…ë ¥ì— ëŒ€í•´ ë¹ˆ ë²ˆì—­ ê²°ê³¼ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")

            logger.debug(f"Gemini API í˜¸ì¶œ ì„±ê³µ. ë²ˆì—­ëœ í…ìŠ¤íŠ¸ (ì¼ë¶€): {translated_text_from_api[:100]}...")
            return translated_text_from_api.strip()

        except GeminiContentSafetyException as e_safety:
            logger.warning(f"ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œë¡œ ë²ˆì—­ ì‹¤íŒ¨: {e_safety}")
            raise BtgTranslationException(f"ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œë¡œ ë²ˆì—­í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({e_safety})", original_exception=e_safety) from e_safety
        except GeminiAllApiKeysExhaustedException as e_keys:
            logger.error(f"API í‚¤ íšŒì „ ì‹¤íŒ¨: ëª¨ë“  API í‚¤ ì†Œì§„ ë˜ëŠ” ìœ íš¨í•˜ì§€ ì•ŠìŒ. ì›ë³¸ ì˜¤ë¥˜: {e_keys}")
            raise BtgApiClientException(f"ëª¨ë“  API í‚¤ë¥¼ ì‚¬ìš©í–ˆìœ¼ë‚˜ ìš”ì²­ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API í‚¤ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”. ({e_keys})", original_exception=e_keys) from e_keys
        except GeminiRateLimitException as e_rate:
            logger.error(f"API ì‚¬ìš©ëŸ‰ ì œí•œ ì´ˆê³¼ (í‚¤ íšŒì „ í›„ì—ë„ ë°œìƒ): {e_rate}")
            raise BtgApiClientException(f"API ì‚¬ìš©ëŸ‰ ì œí•œì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. ({e_rate})", original_exception=e_rate) from e_rate
        except GeminiInvalidRequestException as e_invalid:
            logger.error(f"ì˜ëª»ëœ API ìš”ì²­: {e_invalid}")
            raise BtgApiClientException(f"ì˜ëª»ëœ API ìš”ì²­ì…ë‹ˆë‹¤: {e_invalid}", original_exception=e_invalid) from e_invalid
        except GeminiApiException as e_api:
            logger.error(f"Gemini API í˜¸ì¶œ ì¤‘ ì¼ë°˜ ì˜¤ë¥˜ ë°œìƒ: {e_api}")
            raise BtgApiClientException(f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e_api}", original_exception=e_api) from e_api
        except Exception as e:
            logger.error(f"ë²ˆì—­ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            raise BtgTranslationException(f"ë²ˆì—­ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}", original_exception=e) from e

    def translate_text_with_content_safety_retry(
        self, 
        text_chunk: str, 
        max_split_attempts: int = 3,
        min_chunk_size: int = 100
    ) -> str:
        """
        ì½˜í…ì¸  ì•ˆì „ ì˜¤ë¥˜ ë°œìƒì‹œ ì²­í¬ë¥¼ ë¶„í• í•˜ì—¬ ì¬ì‹œë„í•˜ëŠ” ë²ˆì—­ ë©”ì„œë“œ
        
        Args:
            text_chunk: ë²ˆì—­í•  í…ìŠ¤íŠ¸
            max_split_attempts: ìµœëŒ€ ë¶„í•  ì‹œë„ íšŸìˆ˜
            min_chunk_size: ìµœì†Œ ì²­í¬ í¬ê¸°
            
        Returns:
            ë²ˆì—­ëœ í…ìŠ¤íŠ¸ (ì‹¤íŒ¨í•œ ë¶€ë¶„ì€ ì˜¤ë¥˜ ë©”ì‹œì§€ë¡œ ëŒ€ì²´)
        """
        try:
            # 1ì°¨ ì‹œë„: ì „ì²´ ì²­í¬ ë²ˆì—­
            return self.translate_text(text_chunk)
        except BtgTranslationException as e:
            # ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œê°€ ì•„ë‹Œ ê²½ìš°, ê·¸ëŒ€ë¡œ ì˜ˆì™¸ ë°œìƒ
            if not ("ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œ" in str(e)):
                raise e
            
            error_type_for_log = "ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œ"
            logger.warning(f"{error_type_for_log} ê°ì§€. ì²­í¬ ë¶„í•  ì¬ì‹œë„ ì‹œì‘: {str(e)}")
            return self._translate_with_recursive_splitting(
                text_chunk, max_split_attempts, min_chunk_size, current_attempt=1
            )

    def _translate_with_recursive_splitting(
        self,
        text_chunk: str,
        max_split_attempts: int,
        min_chunk_size: int,
        current_attempt: int = 1
    ) -> str:
    
        if current_attempt > max_split_attempts:
            logger.error(f"ìµœëŒ€ ë¶„í•  ì‹œë„ íšŸìˆ˜({max_split_attempts})ì— ë„ë‹¬. ë²ˆì—­ ì‹¤íŒ¨.")
            return f"[ë²ˆì—­ ì˜¤ë¥˜ë¡œ ì¸í•œ ì‹¤íŒ¨: ìµœëŒ€ ë¶„í•  ì‹œë„ ì´ˆê³¼]" # ë©”ì‹œì§€ ì¼ë°˜í™”

        if len(text_chunk.strip()) <= min_chunk_size:
            logger.warning(f"ìµœì†Œ ì²­í¬ í¬ê¸°ì— ë„ë‹¬í–ˆì§€ë§Œ ì—¬ì „íˆ ì˜¤ë¥˜ ë°œìƒ: {text_chunk[:50]}...")
            return f"[ë²ˆì—­ ì˜¤ë¥˜ë¡œ ì¸í•œ ì‹¤íŒ¨: {text_chunk[:30]}...]" # ë©”ì‹œì§€ ì¼ë°˜í™”

        logger.info(f"ğŸ“Š ì²­í¬ ë¶„í•  ì‹œë„ #{current_attempt} (ê¹Šì´: {current_attempt-1})")
        logger.info(f"   ğŸ“ ì›ë³¸ í¬ê¸°: {len(text_chunk)} ê¸€ì")
        logger.info(f"   ğŸ¯ ëª©í‘œ: ì •í™•íˆ 2ê°œ ì²­í¬ë¡œ ë¶„í•  (ì´ì§„ ë¶„í• )")

        
        # Strict ì´ì§„ ë¶„í•  (ì •í™•íˆ 2ê°œ ì²­í¬)
        sub_chunks = self.chunk_service.split_chunk_into_two_halves(
            text_chunk,
            target_size=len(text_chunk) // 2,
            min_chunk_ratio=0.3  # ë§ˆì§€ë§‰ ì²­í¬ê°€ 30% ë¯¸ë§Œì´ë©´ ë³‘í•©
        )
        
        # ë¶„í• ì´ ì•ˆëœ ê²½ìš° ë¬¸ì¥ ê¸°ë°˜ ë¶„í•  ì‹œë„
        if len(sub_chunks) <= 1:
            logger.info("í¬ê¸° ê¸°ë°˜ ë¶„í•  ì‹¤íŒ¨. ë¬¸ì¥ ê¸°ë°˜ ë¶„í•  ì‹œë„.")
            sub_chunks = self.chunk_service.split_chunk_by_sentences(
                text_chunk, max_sentences_per_chunk=1
            )
        
        if len(sub_chunks) <= 1:
            logger.error("ì²­í¬ ë¶„í•  ì‹¤íŒ¨. ë²ˆì—­ í¬ê¸°.")
            return f"[ë¶„í•  ë¶ˆê°€ëŠ¥í•œ ì˜¤ë¥˜ ë°œìƒ ì½˜í…ì¸ : {text_chunk[:30]}...]" # ë©”ì‹œì§€ ì¼ë°˜í™”
        
        # ê° ì„œë¸Œ ì²­í¬ ê°œë³„ ë²ˆì—­ ì‹œë„
        translated_parts = []
        total_sub_chunks = len(sub_chunks)
        successful_sub_chunks = 0
        failed_sub_chunks = 0
        
        logger.info(f"ğŸ”„ ë¶„í•  ì™„ë£Œ: {total_sub_chunks}ê°œ ì„œë¸Œ ì²­í¬ ìƒì„±")
        
        for i, sub_chunk in enumerate(sub_chunks):
            # ë¹ˆ ì²­í¬ ìŠ¤í‚µ (ê³µë°±ë§Œ ìˆëŠ” ê²½ìš° í¬í•¨)
            if not sub_chunk.strip():
                logger.warning(f"   âš ï¸ ì„œë¸Œ ì²­í¬ {i+1}/{total_sub_chunks} ë¹ˆ ì²­í¬ ê°ì§€. ìŠ¤í‚µ.")
                translated_parts.append("")  # ë¹ˆ ë¬¸ìì—´ ìœ ì§€
                continue
            
            sub_chunk_info = f"ì„œë¸Œ ì²­í¬ {i+1}/{total_sub_chunks}"
            sub_chunk_size = len(sub_chunk.strip())
            sub_chunk_preview = sub_chunk.strip()[:50].replace('\n', ' ') + '...'
            
            logger.info(f"   ğŸš€ {sub_chunk_info} ë²ˆì—­ ì‹œì‘")
            logger.debug(f"      ğŸ“ í¬ê¸°: {sub_chunk_size} ê¸€ì")
            logger.debug(f"      ğŸ“ ë‚´ìš©: {sub_chunk_preview}")
            
            start_time = time.time()
            
            try:
                if self.stop_check_callback and self.stop_check_callback():
                    logger.info(f"ì¤‘ë‹¨ ìš”ì²­ ê°ì§€ë¨. ì„œë¸Œ ì²­í¬ {i+1}/{total_sub_chunks} ë²ˆì—­ ì¤‘ë‹¨.")
                    raise BtgTranslationException("ë²ˆì—­ ì¤‘ë‹¨ ìš”ì²­ë¨.")
                # ì¬ê·€ ë¶„í•  ì‹œ ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš©
                translated_part = self.translate_text(sub_chunk.strip(), stream=True)
                processing_time = time.time() - start_time
                
                translated_parts.append(translated_part)
                successful_sub_chunks += 1
                
                logger.info(f"   âœ… {sub_chunk_info} ë²ˆì—­ ì„±ê³µ (ì†Œìš”: {processing_time:.2f}ì´ˆ, ê¹Šì´: {current_attempt-1})")
                logger.debug(f"      ğŸ“Š ê²°ê³¼ ê¸¸ì´: {len(translated_part)} ê¸€ì")
                logger.debug(f"      ğŸ“ˆ ì§„í–‰ë¥ : {(i+1)/total_sub_chunks*100:.1f}% ({i+1}/{total_sub_chunks})")
                logger.debug(f"      ğŸ“ ë²ˆì—­ëœ ë‚´ìš© (ì¼ë¶€): {translated_part[:50].replace(chr(10), ' ')}...")
                
            except BtgTranslationException as sub_e:
                processing_time = time.time() - start_time
                  # ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œì¸ ê²½ìš° ì¬ê·€ ì‹œë„
                if "ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œ" in str(sub_e):
                    error_type_for_log_sub = "ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œ"
                    logger.warning(f"   ğŸ›¡ï¸ {sub_chunk_info} {error_type_for_log_sub} ë°œìƒ (ì†Œìš”: {processing_time:.2f}ì´ˆ)")
                    logger.info(f"   ğŸ”„ ì¬ê·€ ë¶„í•  ì‹œë„ (ê¹Šì´: {current_attempt} â†’ {current_attempt+1})")
                    
                    # ì¬ê·€ì ìœ¼ë¡œ ë” ì‘ê²Œ ë¶„í•  ì‹œë„
                    recursive_result = self._translate_with_recursive_splitting(
                        sub_chunk, max_split_attempts, min_chunk_size, current_attempt + 1
                    )
                    translated_parts.append(recursive_result)
                    if "[ë²ˆì—­ ì˜¤ë¥˜ë¡œ ì¸í•œ ì‹¤íŒ¨" in recursive_result or "[ë¶„í•  ë¶ˆê°€ëŠ¥í•œ ì˜¤ë¥˜ ë°œìƒ ì½˜í…ì¸ " in recursive_result: # ì˜¤ë¥˜ ë©”ì‹œì§€ í™•ì¸ ê°•í™”
                        failed_sub_chunks += 1
                        logger.warning(f"   âŒ {sub_chunk_info} ìµœì¢… ì‹¤íŒ¨ (ì¬ê·€ ë¶„í•  í›„ì—ë„ ê²€ì—´ë¨)")
                    else:
                        successful_sub_chunks += 1
                        logger.info(f"   âœ… {sub_chunk_info} ì¬ê·€ ë¶„í•  í›„ ì„±ê³µ")
                else:
                    # ë‹¤ë¥¸ ë²ˆì—­ ì˜¤ë¥˜ì¸ ê²½ìš°
                    failed_sub_chunks += 1
                    
                    # APIë¡œë¶€í„° ë°›ì€ ì‹¤ì œ ì˜¤ë¥˜ ë©”ì‹œì§€ì— ê°€ê¹Œìš´ ë‚´ìš©ì„ ì¶”ì¶œ ì‹œë„
                    actual_api_error_str = str(sub_e) # ê¸°ë³¸ê°’: ì¡íŒ ì˜ˆì™¸ì˜ ì „ì²´ ë©”ì‹œì§€
                    if hasattr(sub_e, 'original_exception') and sub_e.original_exception:
                        orig_exc = sub_e.original_exception
                        # BtgApiClientException -> Gemini*Exception ì²´ì¸ í™•ì¸
                        if isinstance(orig_exc, BtgApiClientException) and \
                           hasattr(orig_exc, 'original_exception') and orig_exc.original_exception:
                            # orig_exc.original_exceptionì´ Gemini*Exception ê°ì²´ì„
                            actual_api_error_str = str(orig_exc.original_exception)
                        else:
                            # ì§ì ‘ì ì¸ ì›ì¸ ì˜ˆì™¸ì˜ ë©”ì‹œì§€ ì‚¬ìš©
                            actual_api_error_str = str(orig_exc)
                    
                    logger.error(f"   âŒ {sub_chunk_info} ë²ˆì—­ ì‹¤íŒ¨ (ì†Œìš”: {processing_time:.2f}ì´ˆ, ì˜ˆì™¸: {type(sub_e).__name__})")
                    logger.error(f"     API ì‹¤ì œ ì˜¤ë¥˜ ì‘ë‹µ (ì¶”ì •): {actual_api_error_str}") # ìƒì„¸ ì˜¤ë¥˜ ë¡œê¹…
                    translated_parts.append(f"[ë²ˆì—­ ì‹¤íŒ¨: {str(sub_e)[:100]}]") # ë²ˆì—­ ê²°ê³¼ì—ëŠ” ê°„ëµí•œ ì˜¤ë¥˜ ë©”ì‹œì§€ ìœ ì§€
                
                logger.debug(f"      ğŸ“ˆ ì§„í–‰ë¥ : {(i+1)/total_sub_chunks*100:.1f}% ({i+1}/{total_sub_chunks})")

        
        # ë²ˆì—­ëœ ë¶€ë¶„ë“¤ì„ ê²°í•©
        final_result = " ".join(translated_parts)
        
        # ë¶„í•  ë²ˆì—­ ì™„ë£Œ ìš”ì•½
        logger.info(f"ğŸ“‹ ë¶„í•  ë²ˆì—­ ì™„ë£Œ ìš”ì•½ (ê¹Šì´: {current_attempt-1})")
        logger.info(f"   ğŸ“Š ì´ ì„œë¸Œ ì²­í¬: {total_sub_chunks}ê°œ")
        logger.info(f"   âœ… ì„±ê³µ: {successful_sub_chunks}ê°œ")
        logger.info(f"   âŒ ì‹¤íŒ¨: {failed_sub_chunks}ê°œ")
        logger.info(f"   ğŸ“ ìµœì¢… ê²°ê³¼ ê¸¸ì´: {len(final_result)} ê¸€ì")
        
        if successful_sub_chunks > 0:
            success_rate = (successful_sub_chunks / total_sub_chunks) * 100
            logger.info(f"   ğŸ“ˆ ì„±ê³µë¥ : {success_rate:.1f}%")
        
        return final_result
    
    def set_stop_check_callback(self, callback: Optional[Callable[[], bool]]) -> None:
        """
        ì¤‘ë‹¨ ìš”ì²­ì„ í™•ì¸í•˜ëŠ” ì½œë°± í•¨ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
        
        Args:
            callback: ì¤‘ë‹¨ ìš”ì²­ ì—¬ë¶€ë¥¼ ë°˜í™˜í•˜ëŠ” ì½œë°± í•¨ìˆ˜
        """
        self.stop_check_callback = callback

    # ============================================================================
    # ë¹„ë™ê¸° ë©”ì„œë“œ (Phase 2: asyncio ë§ˆì´ê·¸ë ˆì´ì…˜)
    # ============================================================================

    async def translate_chunk_async(
        self,
        chunk_text: str,
        stream: bool = False,
        timeout: Optional[float] = None
    ) -> str:
        """
        ë¹„ë™ê¸° ì²­í¬ ë²ˆì—­ ë©”ì„œë“œ (ì§„ì •í•œ ë¹„ë™ê¸° êµ¬í˜„)
        
        Args:
            chunk_text: ë²ˆì—­í•  í…ìŠ¤íŠ¸
            stream: ìŠ¤íŠ¸ë¦¬ë° ì—¬ë¶€
            timeout: íƒ€ì„ì•„ì›ƒ ì‹œê°„(ì´ˆ)
            
        Returns:
            ë²ˆì—­ëœ í…ìŠ¤íŠ¸
            
        Raises:
            asyncio.TimeoutError: íƒ€ì„ì•„ì›ƒ ì´ˆê³¼
            asyncio.CancelledError: ì‘ì—… ì·¨ì†Œë¨
            BtgTranslationException: ë²ˆì—­ ì‹¤íŒ¨
        """
        # ğŸ“ ì¤‘ë‹¨ ì²´í¬: ì‘ì—… ì‹œì‘ ì „
        if self.stop_check_callback and self.stop_check_callback():
            logger.info("translate_chunk_async: ì¤‘ë‹¨ ìš”ì²­ ê°ì§€ë¨ (ì‘ì—… ì‹œì‘ ì „)")
            raise asyncio.CancelledError("ë²ˆì—­ ì¤‘ë‹¨ ìš”ì²­ë¨")
        
        if not chunk_text.strip():
            logger.debug("translate_chunk_async: ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆì–´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜.")
            return ""
        
        # ì†Œì„¤ ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸° ë¡œê¹…
        text_preview = chunk_text[:100].replace('\n', ' ')
        logger.info(f"ë¹„ë™ê¸° ì²­í¬ ë²ˆì—­ ìš”ì²­: \"{text_preview}{'...' if len(chunk_text) > 100 else ''}\"")
        
        try:
            # ì§„ì •í•œ ë¹„ë™ê¸° ë©”ì„œë“œ í˜¸ì¶œ (run_in_executor ì œê±°)
            if timeout:
                result = await asyncio.wait_for(
                    self.translate_text_with_content_safety_retry_async(chunk_text),
                    timeout=timeout
                )
            else:
                result = await self.translate_text_with_content_safety_retry_async(chunk_text)
            
            # ğŸ“ ì¤‘ë‹¨ ì²´í¬: API ì‘ë‹µ í›„
            if self.stop_check_callback and self.stop_check_callback():
                logger.info("translate_chunk_async: ì¤‘ë‹¨ ìš”ì²­ ê°ì§€ë¨ (ì‘ë‹µ í›„)")
                raise asyncio.CancelledError("ë²ˆì—­ ì¤‘ë‹¨ ìš”ì²­ë¨")
            
            return result
        except asyncio.TimeoutError:
            logger.error(f"ë¹„ë™ê¸° ë²ˆì—­ íƒ€ì„ì•„ì›ƒ ({timeout}ì´ˆ)")
            raise
        except asyncio.CancelledError:
            logger.info("ë¹„ë™ê¸° ë²ˆì—­ì´ ì·¨ì†Œë¨")
            raise
        except Exception as e:
            logger.error(f"ë¹„ë™ê¸° ë²ˆì—­ ì¤‘ ì˜¤ë¥˜: {type(e).__name__} - {e}", exc_info=True)
            if isinstance(e, BtgTranslationException):
                raise
            raise BtgTranslationException(f"ë¹„ë™ê¸° ë²ˆì—­ ì¤‘ ì˜¤ë¥˜: {e}", original_exception=e) from e

    async def translate_text_with_content_safety_retry_async(
        self,
        chunk_text: str,
        max_split_attempts: int = 3,
        min_chunk_size: int = 100,
        timeout: Optional[float] = None
    ) -> str:
        """
        ë¹„ë™ê¸° ì½˜í…ì¸  ì•ˆì „ì„± ì¬ì‹œë„ì™€ í•¨ê»˜ ì²­í¬ ë²ˆì—­
        
        Args:
            chunk_text: ë²ˆì—­í•  í…ìŠ¤íŠ¸
            max_split_attempts: ìµœëŒ€ ë¶„í•  ì‹œë„ íšŸìˆ˜
            min_chunk_size: ìµœì†Œ ì²­í¬ í¬ê¸°
            timeout: íƒ€ì„ì•„ì›ƒ ì‹œê°„(ì´ˆ)
            
        Returns:
            ë²ˆì—­ëœ í…ìŠ¤íŠ¸
            
        Raises:
            asyncio.TimeoutError: íƒ€ì„ì•„ì›ƒ ì´ˆê³¼
            BtgTranslationException: ë²ˆì—­ ì‹¤íŒ¨
        """
        try:
            loop = asyncio.get_event_loop()
            
            def _sync_translate_with_retry():
                return self.translate_text_with_content_safety_retry(
                    chunk_text,
                    max_split_attempts,
                    min_chunk_size
                )
            
            if timeout:
                result = await asyncio.wait_for(
                    loop.run_in_executor(None, _sync_translate_with_retry),
                    timeout=timeout
                )
            else:
                result = await loop.run_in_executor(None, _sync_translate_with_retry)
            
            return result
        except asyncio.TimeoutError:
            logger.error(f"ë¹„ë™ê¸° ë²ˆì—­(ì¬ì‹œë„) íƒ€ì„ì•„ì›ƒ ({timeout}ì´ˆ)")
            raise
        except asyncio.CancelledError:
            logger.info("ë¹„ë™ê¸° ë²ˆì—­(ì¬ì‹œë„)ì´ ì·¨ì†Œë¨")
            raise
        except Exception as e:
            logger.error(f"ë¹„ë™ê¸° ë²ˆì—­(ì¬ì‹œë„) ì¤‘ ì˜¤ë¥˜: {type(e).__name__} - {e}", exc_info=True)
            if isinstance(e, BtgTranslationException):
                raise
            raise BtgTranslationException(f"ë¹„ë™ê¸° ë²ˆì—­(ì¬ì‹œë„) ì¤‘ ì˜¤ë¥˜: {e}", original_exception=e) from e

if __name__ == '__main__':
    # MockGeminiClientì—ì„œ typesë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ, ì´ ë¸”ë¡ ë‚´ì—ì„œ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
    from google.genai import types as genai_types # Ensure types is imported for hints

    print("--- TranslationService í…ŒìŠ¤íŠ¸ ---")
    class MockGeminiClient(GeminiClient):
        def __init__(self, auth_credentials, project=None, location=None, requests_per_minute: Optional[int] = None):
            try:
                super().__init__(auth_credentials=auth_credentials, project=project, location=location, requests_per_minute=requests_per_minute)
            except Exception as e:
                print(f"Warning: MockGeminiClient super().__init__ failed: {e}. This might be okay for some mock scenarios.")
                # If super init fails (e.g. dummy API key validation),
                # the mock might still function if it overrides all necessary methods
                # and doesn't rely on base class state initialized by __init__.
                # For Pylance, inheritance is the main fix.

            self.mock_auth_credentials = auth_credentials
            self.current_model_name_for_test: Optional[str] = None
            self.mock_api_keys_list: List[str] = []
            self.mock_current_api_key: Optional[str] = None

            if isinstance(auth_credentials, list):
                self.mock_api_keys_list = auth_credentials
                if self.mock_api_keys_list: self.mock_current_api_key = self.mock_api_keys_list[0]
            elif isinstance(auth_credentials, str) and not auth_credentials.startswith('{'): # Assuming API key string
                self.mock_api_keys_list = [auth_credentials]
                self.mock_current_api_key = auth_credentials
            print(f"MockGeminiClient initialized. Mock API Keys: {self.mock_api_keys_list}, Mock Current Key: {self.mock_current_api_key}")

        def generate_text(
            self,
            prompt: Union[str, List[Union[str, genai_types.Part]]],
            model_name: str,
            generation_config_dict: Optional[Dict[str, Any]] = None,
            safety_settings_list_of_dicts: Optional[List[Dict[str, Any]]] = None,
            system_instruction_text: Optional[str] = None,
            max_retries: int = 5,
            initial_backoff: float = 2.0,
            max_backoff: float = 60.0,
            stream: bool = False
        ) -> Optional[Union[str, Any]]:
            self.current_model_name_for_test = model_name

            prompt_text_for_mock = ""
            if isinstance(prompt, str):
                prompt_text_for_mock = prompt
            elif isinstance(prompt, list):
                temp_parts = []
                for item in prompt:
                    if isinstance(item, str):
                        temp_parts.append(item)
                    elif hasattr(item, 'text'): # Duck typing for Part-like objects
                        temp_parts.append(item.text)
                    else:
                        temp_parts.append(str(item))
                prompt_text_for_mock = "".join(temp_parts)

            print(f"  MockGeminiClient.generate_text í˜¸ì¶œë¨ (ëª¨ë¸: {model_name}). Mock í˜„ì¬ í‚¤: {self.mock_current_api_key[:5] if self.mock_current_api_key else 'N/A'}")

            if "ì•ˆì „ ë¬¸ì œ" in prompt_text_for_mock:
                raise GeminiContentSafetyException("Mock ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œ")
            if "ì‚¬ìš©ëŸ‰ ì œí•œ" in prompt_text_for_mock: # Simplified logic for mock
                raise GeminiRateLimitException("Mock API ì‚¬ìš©ëŸ‰ ì œí•œ")
            if "ì˜ëª»ëœ ìš”ì²­" in prompt_text_for_mock:
                raise GeminiInvalidRequestException("Mock ì˜ëª»ëœ ìš”ì²­")

            text_to_be_translated = prompt_text_for_mock
            if "ë²ˆì—­í•  í…ìŠ¤íŠ¸:\n" in prompt_text_for_mock:
                text_to_be_translated = prompt_text_for_mock.split("ë²ˆì—­í•  í…ìŠ¤íŠ¸:\n")[-1].strip()
            elif "Translate to Korean:" in prompt_text_for_mock:
                 text_to_be_translated = prompt_text_for_mock.split("Translate to Korean:")[-1].strip()

            mock_translation = f"[ë²ˆì—­ë¨] {text_to_be_translated[:50]}..."

            is_json_response_expected = generation_config_dict and \
                                        generation_config_dict.get("response_mime_type") == "application/json"

            if is_json_response_expected:
                return {"translated_text": mock_translation, "mock_json": True}
            else:
                return mock_translation

        def list_models(self) -> List[Dict[str, Any]]:
            print("  MockGeminiClient.list_models í˜¸ì¶œë¨")
            # Return a structure similar to what GeminiClient.list_models would return
            return [
                {"name": "models/mock-gemini-flash", "short_name": "mock-gemini-flash", "display_name": "Mock Gemini Flash", "description": "A mock flash model.", "input_token_limit": 1000, "output_token_limit": 1000},
                {"name": "models/mock-gemini-pro", "short_name": "mock-gemini-pro", "display_name": "Mock Gemini Pro", "description": "A mock pro model.", "input_token_limit": 2000, "output_token_limit": 2000},
            ]

    sample_config_base = {
        "model_name": "gemini-1.5-flash", "temperature": 0.7, "top_p": 0.9,
        "prompts": "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ìš©ì–´ì§‘ ì»¨í…ìŠ¤íŠ¸: {{glossary_context}}\n\në²ˆì—­í•  í…ìŠ¤íŠ¸:\n{{slot}}",
        "enable_dynamic_glossary_injection": True, # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ í™œì„±í™”
        "glossary_json_path": "test_glossary.json", # í†µí•©ëœ ìš©ì–´ì§‘ ê²½ë¡œ
        "max_glossary_entries_per_chunk_injection": 3,
        "max_glossary_chars_per_chunk_injection": 200,
    }

    # 1. ìš©ì–´ì§‘ ì£¼ì… í…ŒìŠ¤íŠ¸
    print("\n--- 1. ìš©ì–´ì§‘ ì£¼ì… ë²ˆì—­ í…ŒìŠ¤íŠ¸ ---")
    config1 = sample_config_base.copy()
    
    test_glossary_data = [
    {"keyword": "Alice", "translated_keyword": "ì•¨ë¦¬ìŠ¤", "target_language": "ko", "occurrence_count": 10},
    {"keyword": "Bob", "translated_keyword": "ë°¥", "target_language": "ko", "occurrence_count": 8}
]
    from infrastructure.file_handler import write_json_file, delete_file
    test_glossary_file = Path(config1["glossary_json_path"]) # Use path from config
    if test_glossary_file.exists(): delete_file(test_glossary_file)
    write_json_file(test_glossary_file, test_glossary_data)

    gemini_client_instance = MockGeminiClient(auth_credentials="dummy_api_key")
    translation_service1 = TranslationService(gemini_client_instance, config1)
    text_to_translate1 = "Hello Alice, how are you Bob?"
    try:
        translated1 = translation_service1.translate_text(text_to_translate1)
        print(f"  ì›ë³¸: {text_to_translate1}")
        print(f"  ë²ˆì—­ ê²°ê³¼: {translated1}")
    except Exception as e:
        print(f"  í…ŒìŠ¤íŠ¸ 1 ì˜¤ë¥˜: {e}")
    finally:
        if test_glossary_file.exists(): delete_file(test_glossary_file)

    # 2. ë¡œì–´ë¶ ë¹„í™œì„±í™” í…ŒìŠ¤íŠ¸
    print("\n--- 2. ë¡œì–´ë¶ ë¹„í™œì„±í™” í…ŒìŠ¤íŠ¸ ---")
    config2 = sample_config_base.copy()
    config2["enable_dynamic_glossary_injection"] = False
    translation_service2 = TranslationService(gemini_client_instance, config2)
    text_to_translate2 = "This is a test sentence."
    try:
        translated2 = translation_service2.translate_text(text_to_translate2)
        print(f"ì›ë³¸: {text_to_translate2}")
        print(f"ë²ˆì—­ ê²°ê³¼: {translated2}")
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ 2 ì˜¤ë¥˜: {e}")

    # 3. ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œ í…ŒìŠ¤íŠ¸
    print("\n--- 3. ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œ í…ŒìŠ¤íŠ¸ ---")
    config3 = sample_config_base.copy()
    translation_service3 = TranslationService(gemini_client_instance, config3)
    text_unsafe = "ì•ˆì „ ë¬¸ì œ í…ŒìŠ¤íŠ¸ìš© í…ìŠ¤íŠ¸"
    try:
        translation_service3.translate_text(text_unsafe)
    except BtgTranslationException as e:
        print(f"ì˜ˆìƒëœ ì˜ˆì™¸ ë°œìƒ (ì½˜í…ì¸  ì•ˆì „): {e}")
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ 3 ì˜¤ë¥˜: {type(e).__name__} - {e}")

    print("\n--- TranslationService í…ŒìŠ¤íŠ¸ ì¢…ë£Œ ---")

    # ============================================================================
    # ë¹„ë™ê¸° ë©”ì„œë“œ (Phase 2: asyncio ë§ˆì´ê·¸ë ˆì´ì…˜)
    # ============================================================================

    async def translate_text_async(self, text_chunk: str, stream: bool = False) -> str:
        """
        ë¹„ë™ê¸° í…ìŠ¤íŠ¸ ë²ˆì—­ ë©”ì„œë“œ (translate_textì˜ ë¹„ë™ê¸° ë²„ì „)
        
        Args:
            text_chunk: ë²ˆì—­í•  í…ìŠ¤íŠ¸
            stream: ìŠ¤íŠ¸ë¦¬ë° ì—¬ë¶€
            
        Returns:
            ë²ˆì—­ëœ í…ìŠ¤íŠ¸
            
        Raises:
            asyncio.CancelledError: ì‘ì—…ì´ ì·¨ì†Œëœ ê²½ìš°
            BtgTranslationException: ë²ˆì—­ ì‹¤íŒ¨
        """
        if not text_chunk.strip():
            logger.debug("translate_text_async: ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆì–´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜.")
            return ""
        
        # ğŸ“ ì¤‘ë‹¨ ì²´í¬: ì‘ì—… ì‹œì‘ ì „ (asyncio.CancelledError ë°œìƒ)
        if self.stop_check_callback and self.stop_check_callback():
            logger.info("translate_text_async: ì¤‘ë‹¨ ìš”ì²­ ê°ì§€ë¨ (ì‘ì—… ì‹œì‘ ì „)")
            raise asyncio.CancelledError("ë²ˆì—­ ì¤‘ë‹¨ ìš”ì²­ë¨")
        
        text_preview = text_chunk[:100].replace('\n', ' ')
        logger.info(f"ë¹„ë™ê¸° ë²ˆì—­ ìš”ì²­: \"{text_preview}{'...' if len(text_chunk) > 100 else ''}\"")
        
        # ìš©ì–´ì§‘ ë° í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ (ë™ê¸° ë©”ì„œë“œì™€ ë™ì¼)
        glossary_context_str = "ìš©ì–´ì§‘ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ"
        
        if self.config.get("enable_dynamic_glossary_injection", False) and self.glossary_entries_for_injection:
            logger.info("ìš©ì–´ì§‘ ì»¨í…ìŠ¤íŠ¸ ì£¼ì… í™œì„±í™”ë¨ (ì²­í¬ ë‚´ ê´€ë ¨ í‚¤ì›Œë“œ ì²´í¬).")
            chunk_text_lower = text_chunk.lower()
            final_target_lang = self.config.get("target_translation_language", "ko").lower()
            relevant_entries = []
            
            for entry in self.glossary_entries_for_injection:
                if entry.target_language.lower() == final_target_lang and entry.keyword.lower() in chunk_text_lower:
                    relevant_entries.append(entry)
            
            max_entries = self.config.get("max_glossary_entries_per_chunk_injection", 3)
            max_chars = self.config.get("max_glossary_chars_per_chunk_injection", 500)
            glossary_context_str = _format_glossary_for_prompt(relevant_entries, max_entries, max_chars)
            
            if relevant_entries:
                logger.info(f"API ìš”ì²­ì— ì£¼ì…í•  ìš©ì–´ì§‘ ì»¨í…ìŠ¤íŠ¸ ìƒì„±ë¨. ë‚´ìš© ì¼ë¶€: {glossary_context_str[:100]}...")
        
        replacements = {
            "{{slot}}": text_chunk,
            "{{glossary_context}}": glossary_context_str
        }

        api_prompt_for_gemini_client: List[genai_types.Content] = []
        api_system_instruction: Optional[str] = None

        if self.config.get("enable_prefill_translation", False):
            logger.info("í”„ë¦¬í•„ ë²ˆì—­ ëª¨ë“œ í™œì„±í™”ë¨ (Slot Injection ì²´í¬).")
            api_system_instruction = self.config.get("prefill_system_instruction", "")
            prefill_cached_history_raw = self.config.get("prefill_cached_history", [])
            base_history: List[genai_types.Content] = []
            
            if isinstance(prefill_cached_history_raw, list):
                for item in prefill_cached_history_raw:
                    if isinstance(item, dict) and "role" in item and "parts" in item:
                        sdk_parts = []
                        for part_item in item.get("parts", []):
                            if isinstance(part_item, str):
                                sdk_parts.append(genai_types.Part.from_text(text=part_item))
                        if sdk_parts:
                            base_history.append(genai_types.Content(role=item["role"], parts=sdk_parts))

            injected_history, injected = _inject_slots_into_history(base_history, replacements)

            if injected:
                logger.info("íˆìŠ¤í† ë¦¬ ë‚´ë¶€ì—ì„œ '{{slot}}'ì´ ê°ì§€ë˜ì–´ ì›ë¬¸ì„ ì£¼ì…í–ˆìŠµë‹ˆë‹¤ (Jailbreak ëª¨ë“œ).")
                api_prompt_for_gemini_client = injected_history
                if api_prompt_for_gemini_client and api_prompt_for_gemini_client[-1].role == "model":
                    api_prompt_for_gemini_client.append(
                        genai_types.Content(role="user", parts=[genai_types.Part.from_text(text=" ")])
                    )
            else:
                api_prompt_for_gemini_client = injected_history
                user_prompt_str = self._construct_prompt(text_chunk)
                api_prompt_for_gemini_client.append(
                    genai_types.Content(role="user", parts=[genai_types.Part.from_text(text=user_prompt_str)])
                )
        else:
            user_prompt_str = self._construct_prompt(text_chunk)
            api_prompt_for_gemini_client = [
                genai_types.Content(role="user", parts=[genai_types.Part.from_text(text=user_prompt_str)])
            ]

        try:
            translated_text_from_api = await self.gemini_client.generate_text_async(
                prompt=api_prompt_for_gemini_client,
                model_name=self.config.get("model_name", "gemini-2.0-flash"),
                generation_config_dict={
                    "temperature": self.config.get("temperature", 0.7),
                    "top_p": self.config.get("top_p", 0.9),
                    "thinking_level": self.config.get("thinking_level", "high")
                },
                thinking_budget=self.config.get("thinking_budget", None),
                system_instruction_text=api_system_instruction,
                stream=stream
            )

            if translated_text_from_api is None:
                raise GeminiContentSafetyException("APIë¡œë¶€í„° ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (None ë°˜í™˜).")

            if not translated_text_from_api.strip() and text_chunk.strip():
                raise GeminiContentSafetyException("APIê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ì…ë ¥ì— ëŒ€í•´ ë¹ˆ ë²ˆì—­ ê²°ê³¼ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")

            return translated_text_from_api.strip()

        except asyncio.CancelledError:
            logger.info("ë¹„ë™ê¸° ë²ˆì—­ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤")
            raise
        except GeminiContentSafetyException as e_safety:
            raise BtgTranslationException(f"ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œë¡œ ë²ˆì—­í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({e_safety})", original_exception=e_safety) from e_safety
        except GeminiAllApiKeysExhaustedException as e_keys:
            raise BtgApiClientException(f"ëª¨ë“  API í‚¤ë¥¼ ì‚¬ìš©í–ˆìœ¼ë‚˜ ìš”ì²­ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ({e_keys})", original_exception=e_keys) from e_keys
        except GeminiRateLimitException as e_rate:
            raise BtgApiClientException(f"API ì‚¬ìš©ëŸ‰ ì œí•œì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ({e_rate})", original_exception=e_rate) from e_rate
        except GeminiInvalidRequestException as e_invalid:
            raise BtgApiClientException(f"ì˜ëª»ëœ API ìš”ì²­ì…ë‹ˆë‹¤: {e_invalid}", original_exception=e_invalid) from e_invalid
        except GeminiApiException as e_api:
            raise BtgApiClientException(f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e_api}", original_exception=e_api) from e_api
        except Exception as e:
            raise BtgTranslationException(f"ë²ˆì—­ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}", original_exception=e) from e

    async def translate_text_with_content_safety_retry_async(
        self, 
        text_chunk: str, 
        max_split_attempts: int = 3,
        min_chunk_size: int = 100
    ) -> str:
        """
        ë¹„ë™ê¸° ë²„ì „: ì½˜í…ì¸  ì•ˆì „ ì˜¤ë¥˜ ë°œìƒì‹œ ì²­í¬ë¥¼ ë¶„í• í•˜ì—¬ ì¬ì‹œë„í•˜ëŠ” ë²ˆì—­ ë©”ì„œë“œ
        
        Args:
            text_chunk: ë²ˆì—­í•  í…ìŠ¤íŠ¸
            max_split_attempts: ìµœëŒ€ ë¶„í•  ì‹œë„ íšŸìˆ˜
            min_chunk_size: ìµœì†Œ ì²­í¬ í¬ê¸°
            
        Returns:
            ë²ˆì—­ëœ í…ìŠ¤íŠ¸ (ì‹¤íŒ¨í•œ ë¶€ë¶„ì€ ì˜¤ë¥˜ ë©”ì‹œì§€ë¡œ ëŒ€ì²´)
        """
        try:
            return await self.translate_text_async(text_chunk)
        except BtgTranslationException as e:
            if not ("ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œ" in str(e)):
                raise e
            
            logger.warning(f"ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œ ê°ì§€. ë¹„ë™ê¸° ì²­í¬ ë¶„í•  ì¬ì‹œë„ ì‹œì‘: {str(e)}")
            return await self._translate_with_recursive_splitting_async(
                text_chunk, max_split_attempts, min_chunk_size, current_attempt=1
            )

    async def _translate_with_recursive_splitting_async(
        self,
        text_chunk: str,
        max_split_attempts: int,
        min_chunk_size: int,
        current_attempt: int = 1
    ) -> str:
        if current_attempt > max_split_attempts:
            logger.error(f"ìµœëŒ€ ë¶„í•  ì‹œë„ íšŸìˆ˜({max_split_attempts})ì— ë„ë‹¬. ë²ˆì—­ ì‹¤íŒ¨.")
            return f"[ë²ˆì—­ ì˜¤ë¥˜ë¡œ ì¸í•œ ì‹¤íŒ¨: ìµœëŒ€ ë¶„í•  ì‹œë„ ì´ˆê³¼]"

        if len(text_chunk.strip()) <= min_chunk_size:
            logger.warning(f"ìµœì†Œ ì²­í¬ í¬ê¸°ì— ë„ë‹¬í–ˆì§€ë§Œ ì—¬ì „íˆ ì˜¤ë¥˜ ë°œìƒ: {text_chunk[:50]}...")
            return f"[ë²ˆì—­ ì˜¤ë¥˜ë¡œ ì¸í•œ ì‹¤íŒ¨: {text_chunk[:30]}...]"

        logger.info(f"ğŸ“Š ì²­í¬ ë¶„í•  ì‹œë„ #{current_attempt} (ê¹Šì´: {current_attempt-1})")
        logger.info(f"   ğŸ“ ì›ë³¸ í¬ê¸°: {len(text_chunk)} ê¸€ì")
        logger.info(f"   ğŸ¯ ëª©í‘œ: ì •í™•íˆ 2ê°œ ì²­í¬ë¡œ ë¶„í•  (ì´ì§„ ë¶„í• )")

        # Strict ì´ì§„ ë¶„í•  (ì •í™•íˆ 2ê°œ ì²­í¬)
        sub_chunks = self.chunk_service.split_chunk_into_two_halves(
            text_chunk,
            target_size=len(text_chunk) // 2,
            min_chunk_ratio=0.3  # ë§ˆì§€ë§‰ ì²­í¬ê°€ 30% ë¯¸ë§Œì´ë©´ ë³‘í•©
        )
        
        if len(sub_chunks) <= 1:
            sub_chunks = self.chunk_service.split_chunk_by_sentences(
                text_chunk, max_sentences_per_chunk=1
            )
        
        if len(sub_chunks) <= 1:
            logger.error("ì²­í¬ ë¶„í•  ì‹¤íŒ¨. ë²ˆì—­ í¬ê¸°.")
            return f"[ë¶„í•  ë¶ˆê°€ëŠ¥í•œ ì˜¤ë¥˜ ë°œìƒ ì½˜í…ì¸ : {text_chunk[:30]}...]"
        
        logger.info(f"   ğŸ”„ {len(sub_chunks)}ê°œ ì„œë¸Œ ì²­í¬ë¥¼ ë³‘ë ¬ ì²˜ë¦¬í•©ë‹ˆë‹¤ (ë¹„ë™ê¸°).")
        
        # ë¹„ë™ê¸° ì‘ì—… ë˜í¼ í•¨ìˆ˜
        async def translate_sub_chunk_with_check(sub_chunk: str, idx: int) -> tuple[int, str]:
            """ê°œë³„ ì„œë¸Œ ì²­í¬ ë²ˆì—­ (ì·¨ì†Œ í™•ì¸ í¬í•¨)"""
            # ğŸ“ ì·¨ì†Œ í™•ì¸ 1: ì‘ì—… ì‹œì‘ ì „
            if self.stop_check_callback and self.stop_check_callback():
                raise asyncio.CancelledError(f"ì„œë¸Œ ì²­í¬ {idx+1} ë²ˆì—­ ì¤‘ë‹¨ ìš”ì²­ë¨ (ì‘ì—… ì‹œì‘ ì „)")
            
            if not sub_chunk.strip():
                logger.warning(f"   âš ï¸ ì„œë¸Œ ì²­í¬ {idx+1}/{len(sub_chunks)} ë¹ˆ ì²­í¬ ê°ì§€. ìŠ¤í‚µ.")
                return (idx, "")
            
            try:
                # ğŸ“ ì·¨ì†Œ í™•ì¸ 2: API í˜¸ì¶œ ì§ì „
                if self.stop_check_callback and self.stop_check_callback():
                    raise asyncio.CancelledError(f"ì„œë¸Œ ì²­í¬ {idx+1} ë²ˆì—­ ì¤‘ë‹¨ ìš”ì²­ë¨ (API í˜¸ì¶œ ì§ì „)")
                
                translated = await self.translate_text_async(sub_chunk)
                logger.info(f"   âœ… ì„œë¸Œ ì²­í¬ {idx+1}/{len(sub_chunks)} ë²ˆì—­ ì™„ë£Œ")
                return (idx, translated)
                
            except asyncio.CancelledError:
                logger.info(f"   ğŸ›‘ ì„œë¸Œ ì²­í¬ {idx+1} ì·¨ì†Œë¨")
                raise
            except BtgTranslationException as e_sub:
                if "ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œ" in str(e_sub) and current_attempt < max_split_attempts:
                    logger.warning(f"   ğŸ›¡ï¸ ì„œë¸Œ ì²­í¬ {idx+1} ì½˜í…ì¸  ì•ˆì „ ì˜¤ë¥˜. ì¬ê·€ ë¶„í•  ì‹œë„.")
                    recursive_result = await self._translate_with_recursive_splitting_async(
                        sub_chunk, max_split_attempts, min_chunk_size, current_attempt + 1
                    )
                    return (idx, recursive_result)
                else:
                    error_marker = f"[ì„œë¸Œ ì²­í¬ {idx+1} ë²ˆì—­ ì‹¤íŒ¨: {str(e_sub)[:50]}]"
                    logger.error(f"   âŒ ì„œë¸Œ ì²­í¬ {idx+1} ë²ˆì—­ ì‹¤íŒ¨: {str(e_sub)[:100]}")
                    return (idx, error_marker)
            except Exception as e_general:
                logger.error(f"   âŒ ì„œë¸Œ ì²­í¬ {idx+1} ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e_general}")
                return (idx, f"[ì„œë¸Œ ì²­í¬ {idx+1} ë²ˆì—­ ì˜¤ë¥˜]")
        
        # ì‘ì—… ìƒì„± (ìˆœì°¨ì ìœ¼ë¡œ ì·¨ì†Œ í™•ì¸í•˜ë©° ìƒì„±)
        tasks = []
        for i, sub_chunk in enumerate(sub_chunks):
            # ğŸ“ ì·¨ì†Œ í™•ì¸: ì‘ì—… ìƒì„± ì „
            if self.stop_check_callback and self.stop_check_callback():
                logger.warning(f"ì¤‘ë‹¨ ìš”ì²­ ê°ì§€ë¨. {i}/{len(sub_chunks)}ê°œ ì„œë¸Œ ì²­í¬ ì‘ì—… ìƒì„± ì¤‘ ì¤‘ë‹¨.")
                break
            
            task = asyncio.create_task(translate_sub_chunk_with_check(sub_chunk, i))
            tasks.append(task)
        
        # ìƒì„±ëœ ì‘ì—…ë“¤ì„ ë³‘ë ¬ ì²˜ë¦¬
        results = []
        for task in tasks:
            try:
                idx, translated = await task
                results.append((idx, translated))
            except asyncio.CancelledError:
                logger.info("ì„œë¸Œ ì²­í¬ ë²ˆì—­ ì·¨ì†Œë¨. ë‚˜ë¨¸ì§€ ì‘ì—… ì·¨ì†Œ ì¤‘...")
                # ë‚˜ë¨¸ì§€ ì‘ì—…ë“¤ë„ ì·¨ì†Œ
                for remaining_task in tasks:
                    if not remaining_task.done():
                        remaining_task.cancel()
                raise BtgTranslationException("ì„œë¸Œ ì²­í¬ ë²ˆì—­ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ê²°ê³¼ë¥¼ ì›ë˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì—¬ ê²°í•©
        results.sort(key=lambda x: x[0])
        translated_parts = [text for _, text in results]
        
        logger.info(f"   ğŸ“Š ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ: {len(results)}/{len(sub_chunks)}ê°œ ì„œë¸Œ ì²­í¬ ì²˜ë¦¬ë¨")
        
        return "\n\n".join(translated_parts)

