# c:\Users\Hyunwoo_Room\Downloads\Neo_Batch_Translator\glossary_service.py
import json
import random
import re
import time
import os
import asyncio
import copy # Added for deepcopy
from pathlib import Path
from pydantic import BaseModel, Field as PydanticField # Field ì´ë¦„ ì¶©ëŒ ë°©ì§€
from typing import Dict, Any, Optional, List, Union, Tuple, Callable

try:
    from infrastructure.gemini_client import GeminiClient, GeminiContentSafetyException, GeminiRateLimitException, GeminiApiException, GeminiAllApiKeysExhaustedException
    from infrastructure.file_handler import write_json_file, ensure_dir_exists, delete_file, read_json_file
    from infrastructure.logger_config import setup_logger
    from utils.chunk_service import ChunkService
    from core.exceptions import BtgBusinessLogicException, BtgApiClientException, BtgFileHandlerException
    from core.dtos import GlossaryExtractionProgressDTO, GlossaryEntryDTO
    # genai types ì„í¬íŠ¸ ì¶”ê°€ (TranslationServiceì™€ ë™ì¼)
    from google.genai import types as genai_types
except ImportError:
    # ë‹¨ë… ì‹¤í–‰ ë˜ëŠ” ë‹¤ë¥¸ ê²½ë¡œì—ì„œì˜ importë¥¼ ìœ„í•œ fallback
    from infrastructure.gemini_client import GeminiClient, GeminiContentSafetyException, GeminiRateLimitException, GeminiApiException, GeminiAllApiKeysExhaustedException # type: ignore
    from infrastructure.file_handler import write_json_file, ensure_dir_exists, delete_file, read_json_file # type: ignore
    from utils.chunk_service import ChunkService # type: ignore
    from infrastructure.logger_config import setup_logger # type: ignore
    from core.exceptions import BtgBusinessLogicException, BtgApiClientException, BtgFileHandlerException # type: ignore
    from core.dtos import GlossaryExtractionProgressDTO, GlossaryEntryDTO # type: ignore
    from google.genai import types as genai_types # type: ignore

logger = setup_logger(__name__)

class ApiGlossaryTerm(BaseModel):
    """Pydantic ëª¨ë¸: APIë¡œë¶€í„° ì§ì ‘ ë°›ì„ ìš©ì–´ì§‘ í•­ëª©ì˜ ìŠ¤í‚¤ë§ˆ"""
    keyword: str = PydanticField(description="The original term found in the text.")
    translated_keyword: str = PydanticField(description="The translation of the keyword.")
    target_language: str = PydanticField(description="The BCP-47 language code of the translated_keyword.")
    occurrence_count: int = PydanticField(description="Estimated number of times the keyword appears in the segment.")

def _inject_slots_into_history(
    history: List[genai_types.Content], 
    replacements: Dict[str, str]
) -> tuple[List[genai_types.Content], bool]:
    """
    íˆìŠ¤í† ë¦¬ ë‚´ì˜ Content ê°ì²´ë“¤ì„ ìˆœíšŒí•˜ë©° ìŠ¬ë¡¯({{slot}} ë“±)ì„ ì‹¤ì œ ê°’ìœ¼ë¡œ ì¹˜í™˜í•©ë‹ˆë‹¤.
    ë°˜í™˜ê°’: (ìˆ˜ì •ëœ íˆìŠ¤í† ë¦¬, ì¹˜í™˜ ë°œìƒ ì—¬ë¶€)
    """
    new_history = copy.deepcopy(history)
    replacement_occurred = False

    for content in new_history:
        if not hasattr(content, 'parts'):
            continue
            
        for part in content.parts:
            if hasattr(part, 'text') and part.text:
                original_text = part.text
                modified_text = original_text
                
                for key, value in replacements.items():
                    if key in modified_text:
                        modified_text = modified_text.replace(key, value)
                        replacement_occurred = True
                
                if original_text != modified_text:
                    part.text = modified_text
    
    return new_history, replacement_occurred

class SimpleGlossaryService:
    """
    í…ìŠ¤íŠ¸ì—ì„œ ê°„ë‹¨í•œ ìš©ì–´ì§‘ í•­ëª©(ì›ë³¸ ìš©ì–´, ë²ˆì—­ëœ ìš©ì–´, ì¶œë°œ/ë„ì°© ì–¸ì–´, ë“±ì¥ íšŸìˆ˜)ì„
    ì¶”ì¶œí•˜ê³  ê´€ë¦¬í•˜ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤. (ê²½ëŸ‰í™” ë²„ì „)
    """
    def __init__(self, gemini_client: GeminiClient, config: Dict[str, Any]):
        """
        SimpleGlossaryServiceë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            gemini_client (GeminiClient): Gemini APIì™€ í†µì‹ í•˜ê¸° ìœ„í•œ í´ë¼ì´ì–¸íŠ¸.
            config (Dict[str, Any]): ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • (ì£¼ë¡œ íŒŒì¼ëª… ì ‘ë¯¸ì‚¬ ë“±).
        """
        self.gemini_client = gemini_client
        self.config = config
        self.chunk_service = ChunkService() # ChunkService ì¸ìŠ¤í„´ìŠ¤í™”
    
    def _get_glossary_extraction_prompt(self, segment_text: str, user_override_glossary_prompt: Optional[str] = None) -> str:
        """ìš©ì–´ì§‘ í•­ëª© ì¶”ì¶œì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if user_override_glossary_prompt and user_override_glossary_prompt.strip():
            base_template = user_override_glossary_prompt
            logger.info("ì‚¬ìš©ì ì¬ì •ì˜ ìš©ì–´ì§‘ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            base_template = self.config.get("simple_glossary_extraction_prompt_template") or \
                ("Analyze the following text. Identify key terms, focusing specifically on "
                 "**people (characters), proper nouns (e.g., unique items, titles, artifacts), "
                 "place names (locations, cities, countries, specific buildings), and organization names (e.g., companies, groups, factions, schools)**. "
                 "For each identified term, provide its translation into {target_lang_name} (BCP-47: {target_lang_code}), "
                 "and estimate their occurrence count in this segment.\n"
                 "The response should be a list of these term objects, conforming to the provided schema.\n"
                 "Text: ```\n{novelText}\n```\n"
                 "Ensure your response is a list of objects, where each object has 'keyword', 'translated_keyword', 'target_language', and 'occurrence_count' fields.")

        # ê²½ëŸ‰í™”ëœ ì„œë¹„ìŠ¤ì—ì„œëŠ” ì‚¬ìš©ìê°€ ë²ˆì—­ ëª©í‘œ ì–¸ì–´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì œê³µí•œë‹¤ê³  ê°€ì •
        # ë˜ëŠ” ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ. ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ "ko" (í•œêµ­ì–´)ë¥¼ ì‚¬ìš©.
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì´ ë¶€ë¶„ì„ ë™ì ìœ¼ë¡œ ì„¤ì •í•´ì•¼ í•¨.
        target_lang_code = self.config.get("glossary_target_language_code", "ko")
        target_lang_name = self.config.get("glossary_target_language_name", "Korean")

        prompt = base_template.replace("{target_lang_code}", target_lang_code)
        prompt = prompt.replace("{target_lang_name}", target_lang_name)
        
        # [Strict Mode] í•„ìˆ˜ í”Œë ˆì´ìŠ¤í™€ë” ê²€ì¦
        if "{novelText}" not in prompt:
            raise BtgBusinessLogicException("ìš©ì–´ì§‘ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ì— í•„ìˆ˜ í”Œë ˆì´ìŠ¤í™€ë” '{novelText}'ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")

        prompt = prompt.replace("{novelText}", segment_text) # ìˆ˜ì •: base_template ëŒ€ì‹  prompt ì‚¬ìš©
        return prompt

    def _parse_api_glossary_terms_to_dto(
        self,
        api_terms: List[ApiGlossaryTerm]
    ) -> List[GlossaryEntryDTO]: # ë°˜í™˜ íƒ€ì… ë³€ê²½
        """
        API ì‘ë‹µ ë“±ìœ¼ë¡œ ë°›ì€ ì›ì‹œ ìš©ì–´ì§‘ í•­ëª© ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¥¼ GlossaryEntryDTO ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        """
        glossary_entries: List[GlossaryEntryDTO] = [] # ë³€ìˆ˜ëª… ë³€ê²½
        if not isinstance(api_terms, list): # raw_item_list -> api_terms
            logger.warning(f"API ìš©ì–´ì§‘ í•­ëª© ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤: {type(api_terms)}. ì›ë³¸: {str(api_terms)[:200]}")
            return glossary_entries

        for term in api_terms:
            if isinstance(term, ApiGlossaryTerm):
                try:
                    entry = GlossaryEntryDTO(
                        keyword=term.keyword,
                        translated_keyword=term.translated_keyword,
                        target_language=term.target_language,
                        occurrence_count=term.occurrence_count
                    )
                    glossary_entries.append(entry)
                except Exception as e: # Catch potential errors during DTO creation
                    logger.warning(f"ApiGlossaryTermì„ GlossaryEntryDTOë¡œ ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {term}, ì˜¤ë¥˜: {e}")
                    continue
            else:
                logger.warning(f"ì˜ëª»ëœ API ìš©ì–´ì§‘ í•­ëª© í˜•ì‹ ê±´ë„ˆëœ€: {term}")
        return glossary_entries

    def _parse_dict_list_to_dto(self, raw_item_list: List[Dict[str, Any]]) -> List[GlossaryEntryDTO]:
        glossary_entries: List[GlossaryEntryDTO] = []
        for item_dict in raw_item_list:
            try:
                # GlossaryEntryDTO expects specific fields.
                # Ensure item_dict has them or handle missing keys gracefully.
                glossary_entries.append(GlossaryEntryDTO(**item_dict))
            except TypeError as e:
                logger.warning(f"ë”•ì…”ë„ˆë¦¬ë¥¼ GlossaryEntryDTOë¡œ ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {item_dict}, ì˜¤ë¥˜: {e}")
        return glossary_entries

    # _get_conflict_resolution_prompt, _group_similar_keywords_via_api ë©”ì„œë“œëŠ” ê²½ëŸ‰í™”ë¡œ ì¸í•´ ì œê±° ë˜ëŠ” ëŒ€í­ ë‹¨ìˆœí™”.
    # ì—¬ê¸°ì„œëŠ” ì œê±°í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°€ì •. í•„ìš”í•˜ë‹¤ë©´ ë§¤ìš° ë‹¨ìˆœí•œ í˜•íƒœë¡œ ì¬êµ¬í˜„.

    # _extract_glossary_entries_from_segment_via_api (ë™ê¸° ë²„ì „) ì œê±°ë¨.
    # _extract_glossary_entries_from_segment_via_api_async ì‚¬ìš© ê¶Œì¥.

    def _select_sample_segments(self, all_segments: List[str]) -> List[str]:
        """ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ì—ì„œ í‘œë³¸ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì„ íƒí•©ë‹ˆë‹¤."""
        # ìƒ˜í”Œë§ ë°©ì‹ ì„¤ì • (uniform, random, importance-based ë“±)
        sampling_method = self.config.get("glossary_sampling_method", "uniform") # ì„¤ì • í‚¤ ë³€ê²½
        sample_ratio = self.config.get("glossary_sampling_ratio", 10.0) / 100.0 # ê¸°ë³¸ ìƒ˜í”Œë§ ë¹„ìœ¨ ë‚®ì¶¤ (ê²½ëŸ‰í™”)
        
        if not (0 < sample_ratio <= 1.0):
            logger.warning(f"ì˜ëª»ëœ lorebook_sampling_ratio ê°’: {sample_ratio*100}%. 25%ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.")
            sample_ratio = 0.25
        
        total_segments = len(all_segments)
        if total_segments == 0:
            return []
        
        sample_size = max(1, int(total_segments * sample_ratio))
        
        if sample_size >= total_segments: 
            return all_segments
        
        if sampling_method == "random":
            selected_indices = sorted(random.sample(range(total_segments), sample_size))
        elif sampling_method == "uniform": # ê· ë“± ìƒ˜í”Œë§
            step = total_segments / sample_size
            selected_indices = sorted(list(set(int(i * step) for i in range(sample_size)))) # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            # sample_sizeë³´ë‹¤ ì ê²Œ ì„ íƒë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë¶€ì¡±ë¶„ì€ ëœë¤ìœ¼ë¡œ ì±„ìš°ê±°ë‚˜ ì•ë¶€ë¶„ì—ì„œ ì±„ì›€
            if len(selected_indices) < sample_size:
                additional_needed = sample_size - len(selected_indices)
                remaining_indices = [i for i in range(total_segments) if i not in selected_indices]
                if len(remaining_indices) >= additional_needed:
                    selected_indices.extend(random.sample(remaining_indices, additional_needed))
                else: # ë‚¨ì€ ì¸ë±ìŠ¤ê°€ ë¶€ì¡±í•˜ë©´ ëª¨ë‘ ì¶”ê°€
                    selected_indices.extend(remaining_indices)
                selected_indices = sorted(list(set(selected_indices)))

        # TODO: "importance-based" ìƒ˜í”Œë§ êµ¬í˜„ (ì˜ˆ: íŠ¹ì • í‚¤ì›Œë“œ í¬í•¨ ì„¸ê·¸ë¨¼íŠ¸ ìš°ì„ )
        else: # ê¸°ë³¸ì€ ëœë¤
            selected_indices = sorted(random.sample(range(total_segments), sample_size))
            
        return [all_segments[i] for i in selected_indices]

    def _get_lorebook_output_path(self, input_file_path: Union[str, Path]) -> Path:
        """ì…ë ¥ íŒŒì¼ ê²½ë¡œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¡œì–´ë¶ JSON íŒŒì¼ ê²½ë¡œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        p_input = Path(input_file_path)
        base_name = p_input.stem
        output_dir = p_input.parent
        suffix = self.config.get("glossary_output_json_filename_suffix", "_glossary.json") # ì„¤ì • í‚¤ ë³€ê²½
        return output_dir / f"{base_name}{suffix}" # íŒŒì¼ëª… ë³€ê²½

    def _save_glossary_to_json(self, glossary_entries: List[GlossaryEntryDTO], output_path: Path): # í•¨ìˆ˜ëª… ë° DTO ë³€ê²½
        """ìš©ì–´ì§‘ í•­ëª© ë¦¬ìŠ¤íŠ¸ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        # dataclass ê°ì²´ë¥¼ dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        data_to_save = [entry.__dict__ for entry in glossary_entries]
        try:
            write_json_file(output_path, data_to_save, indent=4) # file_handler ì‚¬ìš©
            logger.info(f"ìš©ì–´ì§‘ì´ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ {len(glossary_entries)}ê°œ í•­ëª©.")
        except Exception as e:
            logger.error(f"ìš©ì–´ì§‘ JSON íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ({output_path}): {e}")
            raise BtgFileHandlerException(f"ìš©ì–´ì§‘ JSON íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {output_path}", original_exception=e) from e

    def _resolve_glossary_conflicts(self, all_extracted_entries: List[GlossaryEntryDTO]) -> List[GlossaryEntryDTO]: # í•¨ìˆ˜ëª… ë° DTO ë³€ê²½
        """
        ì¶”ì¶œëœ ìš©ì–´ì§‘ í•­ëª©ë“¤ì˜ ì¶©ëŒì„ í•´ê²°í•©ë‹ˆë‹¤. (ê²½ëŸ‰í™” ë²„ì „: ì¤‘ë³µ ì œê±° ë° ë“±ì¥ íšŸìˆ˜ í•©ì‚°)
        
        ê°™ì€ ì›ë³¸ ìš©ì–´(keyword)ì— ëŒ€í•´ ì—¬ëŸ¬ ë²ˆì—­ì´ ìˆì„ ê²½ìš°:
        - ë¦¬ìŠ¤íŠ¸ì—ì„œ ë¨¼ì € ë“±ì¥í•œ ë²ˆì—­(translated_keyword)ì„ ìœ ì§€
        - ë“±ì¥ íšŸìˆ˜(occurrence_count)ëŠ” ëª¨ë‘ í•©ì‚°
        
        ë”°ë¼ì„œ ì‹œë“œ ìš©ì–´ì§‘ì˜ ë²ˆì—­ì„ ìš°ì„ í•˜ë ¤ë©´ ì‹œë“œ í•­ëª©ì„ ë¦¬ìŠ¤íŠ¸ ì•ì— ë°°ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
        """
        if not all_extracted_entries:
            return []

        logger.info(f"ìš©ì–´ì§‘ ì¶©ëŒ í•´ê²° ì‹œì‘. ì´ {len(all_extracted_entries)}ê°œ í•­ëª© ê²€í†  ì¤‘...")
        
        # (keyword, target_language)ë¥¼ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ ê·¸ë£¹í™” ë° ë“±ì¥ íšŸìˆ˜ í•©ì‚°       
        # translated_keywordëŠ” ì²« ë²ˆì§¸ ë“±ì¥í•œ ê²ƒì„ ì‚¬ìš© (ì‹œë“œ ìš°ì„ ì„ ìœ„í•´ ì‹œë“œë¥¼ ë¨¼ì € ë„£ì–´ì•¼ í•¨)
        final_entries_map: Dict[Tuple[str, str], GlossaryEntryDTO] = {} # í‚¤ì—ì„œ source_language ì œê±°

        for entry in all_extracted_entries:
            key_tuple = (entry.keyword.lower(), entry.target_language.lower().split('-')[0]) # í‚¤ì—ì„œ source_language ì œê±°
            if key_tuple not in final_entries_map:
                # ì²« ë²ˆì§¸ ë“±ì¥: ì´ ë²ˆì—­ì„ ìµœì¢… ë²ˆì—­ìœ¼ë¡œ ì‚¬ìš©
                final_entries_map[key_tuple] = entry
            else:
                # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” í‚¤: ë²ˆì—­ì€ ìœ ì§€í•˜ê³  ë“±ì¥ íšŸìˆ˜ë§Œ í•©ì‚°
                final_entries_map[key_tuple].occurrence_count += entry.occurrence_count
        
        final_glossary = list(final_entries_map.values())
        # ìµœì¢… ìš©ì–´ì§‘ ì •ë ¬ (ì˜ˆ: í‚¤ì›Œë“œ, ë„ì°©ì–¸ì–´ ìˆœ)
        final_glossary.sort(key=lambda x: (x.keyword.lower(), x.target_language.lower())) # ì •ë ¬ í‚¤ì—ì„œ source_language ì œê±°
              
        logger.info(f"ìš©ì–´ì§‘ ì¶©ëŒ í•´ê²° ì™„ë£Œ. ìµœì¢… {len(final_glossary)}ê°œ í•­ëª©.")
        return final_glossary

    def _select_best_entry_from_group(self, entry_group: List[GlossaryEntryDTO]) -> GlossaryEntryDTO: # DTO ë³€ê²½
        """ì£¼ì–´ì§„ ìš©ì–´ì§‘ í•­ëª© ê·¸ë£¹ì—ì„œ ê°€ì¥ ì¢‹ì€ í•­ëª©ì„ ì„ íƒí•©ë‹ˆë‹¤ (ì˜ˆ: ê°€ì¥ ê¸´ ì„¤ëª…, ê°€ì¥ ë†’ì€ ì¤‘ìš”ë„)."""
        if not entry_group:
            raise ValueError("ë¹ˆ ìš©ì–´ì§‘ í•­ëª© ê·¸ë£¹ì—ì„œ ìµœì„  í•­ëª©ì„ ì„ íƒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        # ê²½ëŸ‰í™” ë²„ì „ì—ì„œëŠ” ë³µì¡í•œ ì„ íƒ ë¡œì§ ëŒ€ì‹  ì²« ë²ˆì§¸ í•­ëª© ë°˜í™˜ ë˜ëŠ” ë“±ì¥ íšŸìˆ˜ ë§ì€ ê²ƒ ì„ íƒ ë“±
        entry_group.sort(key=lambda e: (-e.occurrence_count, e.keyword.lower())) # ë“±ì¥ íšŸìˆ˜ ë§ì€ ìˆœ, ê°™ìœ¼ë©´ í‚¤ì›Œë“œ ìˆœ
        return entry_group[0]

    # =====================================================================
    # ë¹„ë™ê¸° ë©”ì„œë“œ (Async Methods)
    # =====================================================================

    async def _extract_glossary_entries_from_segment_via_api_async(
        self,
        segment_text: str,
        user_override_glossary_prompt: Optional[str] = None,
        stop_check: Optional[Callable[[], bool]] = None
    ) -> List[GlossaryEntryDTO]:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìš©ì–´ì§‘ í•­ëª©ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. (ë¹„ë™ê¸° ë²„ì „)
        í”„ë¦¬í•„(Prefill) ë° êµ¬ì¡°í™”ëœ ì¶œë ¥(Structured Output)ì„ ì§€ì›í•©ë‹ˆë‹¤.
        
        Args:
            segment_text: ë¶„ì„í•  í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸
            user_override_glossary_prompt: ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ (ì˜µì…˜)
            stop_check: ì¤‘ë‹¨ ìš”ì²­ í™•ì¸ ì½œë°±
            
        Returns:
            ì¶”ì¶œëœ ìš©ì–´ì§‘ í•­ëª© ë¦¬ìŠ¤íŠ¸
            
        Raises:
            BtgApiClientException: API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
            BtgBusinessLogicException: ë‚´ë¶€ ì˜¤ë¥˜ ì‹œ
            asyncio.CancelledError: ì‘ì—… ì·¨ì†Œ ì‹œ
        """
        # ğŸ“ ì¤‘ë‹¨ ì²´í¬ 1: ì‘ì—… ì‹œì‘ ì „
        if stop_check and stop_check():
            logger.info("ìš©ì–´ì§‘ ì¶”ì¶œì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤ (ì‘ì—… ì‹œì‘ ì „)")
            raise asyncio.CancelledError("ìš©ì–´ì§‘ ì¶”ì¶œ ì¤‘ë‹¨ ìš”ì²­ë¨")
        
        model_name = self.config.get("model_name", "gemini-2.0-flash")
        generation_config_params = { 
            "temperature": self.config.get("glossary_extraction_temperature", 0.3),
            "response_mime_type": "application/json",
            "response_schema": list[ApiGlossaryTerm],
            "thinking_level": self.config.get("thinking_level", "high")
        }

        api_prompt_for_gemini_client: Union[str, List[genai_types.Content]]
        api_system_instruction: Optional[str] = None

        # --- í”„ë¦¬í•„(Prefill) ëª¨ë“œ í™•ì¸ ---
        if self.config.get("enable_glossary_prefill", False):
            logger.info("ìš©ì–´ì§‘ ì¶”ì¶œ í”„ë¦¬í•„ ëª¨ë“œ í™œì„±í™”ë¨.")
            
            # 1. ì‹œìŠ¤í…œ ì§€ì¹¨ ì„¤ì •
            api_system_instruction = self.config.get("glossary_prefill_system_instruction", "")
            
            # 2. ìºì‹œëœ íˆìŠ¤í† ë¦¬ ë¡œë“œ ë° ë³€í™˜
            prefill_history_raw = self.config.get("glossary_prefill_cached_history", [])
            base_history: List[genai_types.Content] = []
            
            if isinstance(prefill_history_raw, list):
                for item in prefill_history_raw:
                    if isinstance(item, dict) and "role" in item and "parts" in item:
                        sdk_parts = []
                        for part_item in item.get("parts", []):
                            if isinstance(part_item, str):
                                sdk_parts.append(genai_types.Part.from_text(text=part_item))
                        if sdk_parts:
                            base_history.append(genai_types.Content(role=item["role"], parts=sdk_parts))
            
            # 3. ìŠ¬ë¡¯ ì£¼ì… ({novelText})
            replacements = {
                "{novelText}": segment_text
            }
            
            injected_history, injected = _inject_slots_into_history(base_history, replacements)
            
            if injected:
                logger.debug("íˆìŠ¤í† ë¦¬ ë‚´ ìŠ¬ë¡¯ì´ ê°ì§€ë˜ì–´ ì„¸ê·¸ë¨¼íŠ¸ í…ìŠ¤íŠ¸ë¥¼ ì£¼ì…í–ˆìŠµë‹ˆë‹¤.")
                api_prompt_for_gemini_client = injected_history
                
                # [Trigger Logic] ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ Modelì´ë©´ ì´ì–´ì“°ê¸°ë¥¼ ìœ„í•œ ë¹ˆ User ë©”ì‹œì§€ ì¶”ê°€
                if api_prompt_for_gemini_client and api_prompt_for_gemini_client[-1].role == "model":
                    logger.debug("ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ Modelì´ë¯€ë¡œ ì´ì–´ì“°ê¸°ë¥¼ ìœ„í•œ ë¹ˆ User íŠ¸ë¦¬ê±°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.")
                    api_prompt_for_gemini_client.append(
                        genai_types.Content(role="user", parts=[genai_types.Part.from_text(text=" ")])
                    )
            else:
                # ìŠ¬ë¡¯ì´ ì—†ìœ¼ë©´ ê¸°ì¡´ í…œí”Œë¦¿ ë°©ì‹ í”„ë¡¬í”„íŠ¸ë¥¼ ìœ ì € ë©”ì‹œì§€ë¡œ ì¶”ê°€
                logger.info("íˆìŠ¤í† ë¦¬ ë‚´ë¶€ì— ìŠ¬ë¡¯ì´ ì—†ìŠµë‹ˆë‹¤. í‘œì¤€ í”„ë¡¬í”„íŠ¸ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.")
                prompt_str = self._get_glossary_extraction_prompt(segment_text, user_override_glossary_prompt)
                api_prompt_for_gemini_client = injected_history
                api_prompt_for_gemini_client.append(
                    genai_types.Content(role="user", parts=[genai_types.Part.from_text(text=prompt_str)])
                )
        else:
            # --- í‘œì¤€ ëª¨ë“œ ---
            api_prompt_for_gemini_client = self._get_glossary_extraction_prompt(segment_text, user_override_glossary_prompt)

        # ğŸ“ ì¤‘ë‹¨ ì²´í¬ 2: API í˜¸ì¶œ ì§ì „
        if stop_check and stop_check():
            logger.info("ìš©ì–´ì§‘ ì¶”ì¶œì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤ (API í˜¸ì¶œ ì§ì „)")
            raise asyncio.CancelledError("ìš©ì–´ì§‘ ì¶”ì¶œ ì¤‘ë‹¨ ìš”ì²­ë¨")

        try:
            # ë¹„ë™ê¸° API í˜¸ì¶œ
            response_data = await self.gemini_client.generate_text_async(
                prompt=api_prompt_for_gemini_client,
                model_name=model_name,
                generation_config_dict=generation_config_params,
                thinking_budget=self.config.get("thinking_budget", None),
                system_instruction_text=api_system_instruction
            )

            # ğŸ“ ì¤‘ë‹¨ ì²´í¬ 3: API ì‘ë‹µ í›„
            if stop_check and stop_check():
                logger.info("ìš©ì–´ì§‘ ì¶”ì¶œì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤ (API ì‘ë‹µ í›„)")
                raise asyncio.CancelledError("ìš©ì–´ì§‘ ì¶”ì¶œ ì¤‘ë‹¨ ìš”ì²­ë¨")

            # --- ì‘ë‹µ ì²˜ë¦¬ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼) ---
            if isinstance(response_data, list) and all(isinstance(item, ApiGlossaryTerm) for item in response_data):
                logger.debug("GeminiClientê°€ ApiGlossaryTerm ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
                return self._parse_api_glossary_terms_to_dto(response_data)
            elif isinstance(response_data, list) and all(isinstance(item, dict) for item in response_data):
                logger.warning("GeminiClientê°€ dict ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
                return self._parse_dict_list_to_dto(response_data)
            elif isinstance(response_data, dict):
                logger.warning(f"GeminiClientê°€ ì˜ˆìƒì¹˜ ëª»í•œ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤: {str(response_data)[:200]}")
                raw_terms_fallback = response_data.get("terms")
                if isinstance(raw_terms_fallback, list):
                    return self._parse_dict_list_to_dto(raw_terms_fallback)
                else:
                    logger.error(f"API ì‘ë‹µì´ ìœ íš¨í•œ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤: {response_data}")
                    return []
            elif response_data is None:
                logger.warning(f"ìš©ì–´ì§‘ ì¶”ì¶œ APIë¡œë¶€í„° ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return []
            elif isinstance(response_data, str):
                logger.warning(f"GeminiClientê°€ ë¬¸ìì—´ì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤ (JSON íŒŒì‹± ì‹¤íŒ¨ ì¶”ì •): {response_data[:200]}...")
                return []
            else:
                logger.warning(f"GeminiClientë¡œë¶€í„° ì˜ˆìƒì¹˜ ì•Šì€ íƒ€ì…ì˜ ì‘ë‹µ ({type(response_data)})ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.")
                return []
            
        except asyncio.CancelledError:
            logger.info("ìš©ì–´ì§‘ ì¶”ì¶œì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            raise
        except GeminiAllApiKeysExhaustedException as e_keys:
            logger.critical(f"ëª¨ë“  API í‚¤ ì†Œì§„ìœ¼ë¡œ ìš©ì–´ì§‘ ì¶”ì¶œ ì¤‘ë‹¨: {e_keys}")
            raise BtgApiClientException(f"ëª¨ë“  API í‚¤ ì†Œì§„: {e_keys}", original_exception=e_keys) from e_keys
        except GeminiApiException as e_api:
            logger.error(f"ìš©ì–´ì§‘ ì¶”ì¶œ API í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨: {e_api}. ì„¸ê·¸ë¨¼íŠ¸: {segment_text[:50]}...")
            raise BtgApiClientException(f"ìš©ì–´ì§‘ ì¶”ì¶œ API í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨: {e_api}", original_exception=e_api) from e_api       
        except Exception as e:
            logger.error(f"ìš©ì–´ì§‘ ì¶”ì¶œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ë‚´ë¶€ ì˜¤ë¥˜: {e}.", exc_info=True)
            raise BtgBusinessLogicException(f"ìš©ì–´ì§‘ ì¶”ì¶œ ì¤‘ ë‚´ë¶€ ì˜¤ë¥˜: {e}", original_exception=e) from e

    async def extract_and_save_glossary_async(
        self,
        novel_text_content: str,
        input_file_path_for_naming: Union[str, Path],
        progress_callback: Optional[Callable[[GlossaryExtractionProgressDTO], None]] = None,
        seed_glossary_path: Optional[Union[str, Path]] = None,
        user_override_glossary_extraction_prompt: Optional[str] = None,
        stop_check: Optional[Callable[[], bool]] = None,
        max_workers: int = 4,
        rpm: int = 60
    ) -> Path:
        """
        ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ë‚´ìš©ì—ì„œ ë¡œì–´ë¶ì„ ì¶”ì¶œí•˜ê³  JSON íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤. (ë¹„ë™ê¸° ë²„ì „)
        
        Args:
            novel_text_content: ë¶„ì„í•  ì „ì²´ í…ìŠ¤íŠ¸ ë‚´ìš©
            input_file_path_for_naming: ì¶œë ¥ JSON íŒŒì¼ ì´ë¦„ ìƒì„±ì— ì‚¬ìš©ë  ì›ë³¸ ì…ë ¥ íŒŒì¼ ê²½ë¡œ
            progress_callback: ì§„í–‰ ìƒí™©ì„ ì•Œë¦¬ê¸° ìœ„í•œ ì½œë°± í•¨ìˆ˜
            seed_glossary_path: ì°¸ê³ í•  ê¸°ì¡´ ìš©ì–´ì§‘ JSON íŒŒì¼ ê²½ë¡œ
            user_override_glossary_extraction_prompt: ìš©ì–´ì§‘ ì¶”ì¶œ ì‹œ ì‚¬ìš©í•  ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸
            stop_check: ì¤‘ì§€ ìš”ì²­ì„ í™•ì¸í•˜ëŠ” ì½œë°± í•¨ìˆ˜
            
        Returns:
            ìƒì„±ëœ ë¡œì–´ë¶ JSON íŒŒì¼ì˜ ê²½ë¡œ
            
        Raises:
            BtgBusinessLogicException: ìš©ì–´ì§‘ ì¶”ì¶œ ë˜ëŠ” ì €ì¥ ê³¼ì •ì—ì„œ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ ì‹œ
            asyncio.CancelledError: ì‘ì—… ì·¨ì†Œ ì‹œ
        """
        all_extracted_entries_from_segments: List[GlossaryEntryDTO] = []
        seed_entries: List[GlossaryEntryDTO] = []

        # ì‹œë“œ ìš©ì–´ì§‘ ë¡œë“œ
        if seed_glossary_path:
            seed_path_obj = Path(seed_glossary_path)
            if seed_path_obj.exists() and seed_path_obj.is_file():
                try:
                    logger.info(f"ì‹œë“œ ìš©ì–´ì§‘ íŒŒì¼ ë¡œë“œ ì¤‘: {seed_path_obj}")
                    raw_seed_data = read_json_file(seed_path_obj)
                    if isinstance(raw_seed_data, list):
                        for item_dict in raw_seed_data:
                            if isinstance(item_dict, dict) and "keyword" in item_dict and \
                               "translated_keyword" in item_dict and \
                               "target_language" in item_dict:
                                try:
                                    entry = GlossaryEntryDTO(
                                        keyword=item_dict.get("keyword", ""),
                                        translated_keyword=item_dict.get("translated_keyword", ""),
                                        target_language=item_dict.get("target_language", ""),
                                        occurrence_count=int(item_dict.get("occurrence_count", 0))
                                    )
                                    if entry.keyword and entry.translated_keyword:
                                        seed_entries.append(entry)
                                except (TypeError, ValueError) as e_dto:
                                    logger.warning(f"ì‹œë“œ ìš©ì–´ì§‘ í•­ëª© DTO ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {item_dict}, ì˜¤ë¥˜: {e_dto}")
                        logger.info(f"{len(seed_entries)}ê°œì˜ ì‹œë“œ ìš©ì–´ì§‘ í•­ëª© ë¡œë“œ ì™„ë£Œ.")
                except Exception as e_seed:
                    logger.error(f"ì‹œë“œ ìš©ì–´ì§‘ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ({seed_path_obj}): {e_seed}", exc_info=True)
            else:
                logger.warning(f"ì œê³µëœ ì‹œë“œ ìš©ì–´ì§‘ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤: {seed_glossary_path}")
        
        # ChunkServiceë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë¶„í• 
        glossary_segment_size = self.config.get("glossary_chunk_size", self.config.get("chunk_size", 8000))
        all_text_segments = self.chunk_service.create_chunks_from_file_content(novel_text_content, glossary_segment_size)

        sample_segments = self._select_sample_segments(all_text_segments)
        num_sample_segments = len(sample_segments)

        # ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ ìœ íš¨ ì´ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ ê³„ì‚°
        effective_total_segments_for_progress = num_sample_segments
        if num_sample_segments == 0 and seed_entries:
            effective_total_segments_for_progress = 1
        elif num_sample_segments == 0 and not novel_text_content.strip() and not seed_entries:
            effective_total_segments_for_progress = 0

        # ë¹ˆ ì…ë ¥ ì²˜ë¦¬
        if not novel_text_content.strip() and not sample_segments and not seed_entries:
            logger.info("ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆê³ , í‘œë³¸ ì„¸ê·¸ë¨¼íŠ¸ ë° ì‹œë“œ ìš©ì–´ì§‘ë„ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ìš©ì–´ì§‘ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            lorebook_output_path = self._get_lorebook_output_path(input_file_path_for_naming)
            self._save_glossary_to_json([], lorebook_output_path)
            if progress_callback:
                progress_callback(GlossaryExtractionProgressDTO(
                    total_segments=effective_total_segments_for_progress,
                    processed_segments=0,
                    current_status_message="ì…ë ¥ í…ìŠ¤íŠ¸ ë° ì‹œë“œ ì—†ìŒ",
                    extracted_entries_count=0
                ))
            return lorebook_output_path
        elif not novel_text_content.strip() and not sample_segments and seed_entries:
            logger.info("ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆê³  í‘œë³¸ ì„¸ê·¸ë¨¼íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì‹œë“œ ìš©ì–´ì§‘ë§Œìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            all_extracted_entries_from_segments.extend(seed_entries)
            if progress_callback:
                progress_callback(GlossaryExtractionProgressDTO(
                    total_segments=effective_total_segments_for_progress,
                    processed_segments=0,
                    current_status_message="ì‹œë“œ ìš©ì–´ì§‘ ì²˜ë¦¬ ì¤‘...", 
                    extracted_entries_count=len(seed_entries)
                ))
        elif sample_segments:
            logger.info(f"ì´ {len(all_text_segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸ ì¤‘ {num_sample_segments}ê°œì˜ í‘œë³¸ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ìš©ì–´ì§‘ ì¶”ì¶œ ì‹œì‘...")
            logger.info(f"ë™ì‹œ ì‘ì—… ìˆ˜: {max_workers}, RPM ì œí•œ: {rpm}/ë¶„")
        
            processed_segments_count = 0
            if progress_callback:
                progress_callback(GlossaryExtractionProgressDTO(
                    total_segments=effective_total_segments_for_progress,
                    processed_segments=processed_segments_count,
                    current_status_message="ì¶”ì¶œ ì‹œì‘ ì¤‘...",
                    extracted_entries_count=len(seed_entries)
                ))

            # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ
            semaphore = asyncio.Semaphore(max_workers)
            
            # RPM ì œí•œì„ ìœ„í•œ ì†ë„ ì œí•œ ì¥ì¹˜ (ì‹œê°„ ê°„ê²© ê³„ì‚°)
            request_interval = 60.0 / rpm if rpm > 0 else 0
            last_request_time = 0
            
            async def rate_limited_extract(segment_text: str) -> List[GlossaryEntryDTO]:
                """RPM ì œí•œì„ ê³ ë ¤í•œ ì¶”ì¶œ í•¨ìˆ˜"""
                nonlocal last_request_time
                
                # ğŸ“ ì·¨ì†Œ í™•ì¸ 1: API í˜¸ì¶œ ì „
                if stop_check and stop_check():
                    raise asyncio.CancelledError("ìš©ì–´ì§‘ ì¶”ì¶œì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤ (ì‘ì—… ì‹œì‘ ì „)")
                
                # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì‹¤í–‰ ì œí•œ
                async with semaphore:
                    # ğŸ“ ì·¨ì†Œ í™•ì¸ 2: ì„¸ë§ˆí¬ì–´ íšë“ í›„
                    if stop_check and stop_check():
                        raise asyncio.CancelledError("ìš©ì–´ì§‘ ì¶”ì¶œì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤ (ì„¸ë§ˆí¬ì–´ íšë“ í›„)")
                    
                    # RPM ì†ë„ ì œí•œ ì ìš©
                    elapsed = asyncio.get_event_loop().time() - last_request_time
                    if elapsed < request_interval:
                        await asyncio.sleep(request_interval - elapsed)
                    
                    # ğŸ“ ì·¨ì†Œ í™•ì¸ 3: RPM ëŒ€ê¸° í›„
                    if stop_check and stop_check():
                        raise asyncio.CancelledError("ìš©ì–´ì§‘ ì¶”ì¶œì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤ (RPM ëŒ€ê¸° í›„)")
                    
                    last_request_time = asyncio.get_event_loop().time()
                    
                    return await self._extract_glossary_entries_from_segment_via_api_async(
                        segment_text,
                        user_override_glossary_extraction_prompt,
                        stop_check  # stop_check ì „ë‹¬
                    )

            # ì‘ì—…ì„ ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±í•˜ê³  ì²˜ë¦¬ (ë™ì‹œì„±ì€ semaphoreë¡œ ì œì–´)
            tasks = []
            for segment_idx, segment in enumerate(sample_segments):
                # ğŸ“ ì·¨ì†Œ í™•ì¸: ì‘ì—… ìƒì„± ì „
                if stop_check and stop_check():
                    logger.warning(f"ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ìš©ì–´ì§‘ ì¶”ì¶œì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤. {segment_idx}/{num_sample_segments}ê°œ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ì¤‘ ì¤‘ë‹¨.")
                    break
                
                task = asyncio.create_task(rate_limited_extract(segment))
                tasks.append((task, segment))
            
            # ìƒì„±ëœ ì‘ì—…ë“¤ì„ ì™„ë£Œ ì²˜ë¦¬
            for task, segment in tasks:
                try:
                    # GeminiClientì˜ http_options timeoutì— ì˜ì¡´
                    # (ê¸°ë³¸ê°’: _TIMEOUT_SECONDS = 500ì´ˆ)
                    extracted_entries_for_segment = await task
                    if extracted_entries_for_segment:
                        all_extracted_entries_from_segments.extend(extracted_entries_for_segment)
                except asyncio.CancelledError:
                    logger.info("ìš©ì–´ì§‘ ì¶”ì¶œì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    # ë‚˜ë¨¸ì§€ ì§„í–‰ ì¤‘ì¸ ì‘ì—…ë“¤ë„ ì·¨ì†Œ
                    for remaining_task, _ in tasks:
                        if not remaining_task.done():
                            remaining_task.cancel()
                    raise  # ìƒìœ„ë¡œ ì „íŒŒí•˜ì—¬ ì¦‰ì‹œ ì¢…ë£Œ
                except BtgApiClientException as e_api:
                    if isinstance(e_api.original_exception, GeminiAllApiKeysExhaustedException):
                        logger.critical("ëª¨ë“  API í‚¤ê°€ ì†Œì§„ë˜ì–´ ìš©ì–´ì§‘ ì¶”ì¶œ ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                        for remaining_task, _ in tasks:
                            if not remaining_task.done():
                                remaining_task.cancel()
                        raise e_api
                    logger.error(f"í‘œë³¸ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ì¤‘ API ì˜¤ë¥˜ ë°œìƒ (ì„¸ê·¸ë¨¼íŠ¸: {segment[:50]}...): {e_api}")
                except Exception as exc:
                    logger.error(f"í‘œë³¸ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ (ì„¸ê·¸ë¨¼íŠ¸: {segment[:50]}...): {exc}")
                finally:
                    processed_segments_count += 1
                    if progress_callback:
                        status_msg = f"í‘œë³¸ ì„¸ê·¸ë¨¼íŠ¸ {processed_segments_count}/{len(tasks)} ì²˜ë¦¬ ì™„ë£Œ"
                        if processed_segments_count == len(tasks):
                            status_msg = "ëª¨ë“  í‘œë³¸ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ì™„ë£Œ, ì¶©ëŒ í•´ê²° ë° ì €ì¥ ì¤‘..."
                        progress_callback(GlossaryExtractionProgressDTO(
                            total_segments=effective_total_segments_for_progress,
                            processed_segments=processed_segments_count,
                            current_status_message=status_msg,
                            extracted_entries_count=len(all_extracted_entries_from_segments) + len(seed_entries)
                        ))

        # ì‹œë“œ í•­ëª©ì´ ìˆê³ , ìƒˆë¡œìš´ ì¶”ì¶œë„ ìˆì—ˆë‹¤ë©´ ë³‘í•©
        if seed_entries and (novel_text_content.strip() and sample_segments):
            logger.info(f"{len(seed_entries)}ê°œì˜ ì‹œë“œ í•­ëª©ì„ ìƒˆë¡œ ì¶”ì¶œëœ í•­ëª©ê³¼ ë³‘í•©í•©ë‹ˆë‹¤. (ì‹œë“œ í•­ëª© ìš°ì„ )")
            all_extracted_entries_from_segments = seed_entries + all_extracted_entries_from_segments

        # ì¶©ëŒ í•´ê²°
        final_glossary = self._resolve_glossary_conflicts(all_extracted_entries_from_segments)
        
        # ì¤‘ìš”ë„(ë“±ì¥ íšŸìˆ˜)ì— ë”°ë¼ ì •ë ¬
        final_glossary.sort(key=lambda x: (-x.occurrence_count, x.keyword.lower()))
        logger.info(f"ìµœì¢… ìš©ì–´ì§‘ì„ ë“±ì¥ íšŸìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í–ˆìŠµë‹ˆë‹¤. (ìƒìœ„ 3ê°œ: {[e.keyword for e in final_glossary[:3]]})")

        # ë¡œì–´ë¶ ìµœëŒ€ í•­ëª© ìˆ˜ ì œí•œ
        max_total_glossary_entries = self.config.get("glossary_max_total_entries", 500)
        if len(final_glossary) > max_total_glossary_entries:
            logger.info(f"ì •ë ¬ëœ ìš©ì–´ì§‘ í•­ëª©({len(final_glossary)}ê°œ)ì´ ìµœëŒ€ ì œí•œ({max_total_glossary_entries}ê°œ)ì„ ì´ˆê³¼í•˜ì—¬ ìƒìœ„ í•­ëª©ë§Œ ì €ì¥í•©ë‹ˆë‹¤.")
            final_glossary = final_glossary[:max_total_glossary_entries]

        # ìµœì¢… ë¡œì–´ë¶ ì €ì¥
        glossary_output_path = self._get_lorebook_output_path(input_file_path_for_naming)
        self._save_glossary_to_json(final_glossary, glossary_output_path)
        
        logger.info(f"ìš©ì–´ì§‘ ì¶”ì¶œ ë° ì €ì¥ ì™„ë£Œ. ê²°ê³¼: {glossary_output_path}")

        # ìµœì¢… ì§„í–‰ë¥  ì½œë°±
        if progress_callback:
            final_processed_segments = processed_segments_count if sample_segments else (1 if seed_entries else 0)
            progress_callback(GlossaryExtractionProgressDTO(
                total_segments=effective_total_segments_for_progress,
                processed_segments=final_processed_segments,
                current_status_message=f"ì¶”ì¶œ ì™„ë£Œ: {glossary_output_path.name}",
                extracted_entries_count=len(final_glossary)
            ))
        return glossary_output_path

